{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5cfd882-4374-4168-a474-4dac40c03d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (23.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m136.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: werkzeug, typing-extensions, triton, tensorboard-data-server, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markdown, grpcio, absl-py, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.1.0 grpcio-1.67.1 markdown-3.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 protobuf-5.28.3 tensorboard-2.18.0 tensorboard-data-server-0.7.2 torch-2.4.0 triton-3.0.0 typing-extensions-4.12.2 werkzeug-3.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==3.0.1\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate==0.4.3\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting bitsandbytes==0.44.0\n",
      "  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting trl==0.11.1\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting qwen-vl-utils\n",
      "  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.1)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.1)\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets==3.0.1)\n",
      "  Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.0.1)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers==4.45.1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets==3.0.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2023.4.0)\n",
      "Collecting aiohttp (from datasets==3.0.1)\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.1)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting av (from qwen-vl-utils)\n",
      "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (9.3.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets==3.0.1)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m134.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m163.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m141.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m203.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, shtab, safetensors, requests, regex, pyarrow, propcache, multidict, mdurl, fsspec, frozenlist, docstring-parser, dill, av, async-timeout, aiohappyeyeballs, yarl, qwen-vl-utils, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, bitsandbytes, accelerate, peft, datasets, trl, evaluate\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 av-13.1.0 bitsandbytes-0.44.0 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 evaluate-0.4.3 frozenlist-1.5.0 fsspec-2024.6.1 huggingface-hub-0.26.2 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 peft-0.13.0 propcache-0.2.0 pyarrow-18.0.0 pytz-2024.2 qwen-vl-utils-0.0.8 regex-2024.9.11 requests-2.32.3 rich-13.9.4 safetensors-0.4.5 shtab-1.7.1 tokenizers-0.20.1 tqdm-4.66.6 transformers-4.45.1 trl-0.11.1 tyro-0.8.14 tzdata-2024.2 xxhash-3.5.0 yarl-1.17.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch==2.4.0\" tensorboard pillow\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d66aff-bc9a-4bc3-9c37-be2b143ddf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ninja\n",
      "Successfully installed ninja-1.11.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.4.0)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187309225 sha256=237ef9c6157db394e1ddde4ba609a21ebb98382377a27041edc09318801a6f24\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.6.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU가 Flash Attention을 지원하는지 확인\n",
    "assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "\n",
    "# Flash Attention 설치\n",
    "!pip install ninja packaging\n",
    "!pip install flash-attn --no-build-isolation --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "186dd2d0-0f13-41aa-9c52-44b728f17e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
    "\n",
    "다음의 지시사항을 따르십시오.\n",
    "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
    "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
    "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
    "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
    "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
    "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
    "\n",
    "검색 결과:\n",
    "-------------------------\n",
    "{search_result}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b87daaf2-e61d-4548-aaa3-64a4733929af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-------------------------\\n문서1: 새롭게 창출되어야 할 시장 중 중요한 부분이 있거나 급한 경우 또는 필요하게 되는 경우가 있으면 자유 진입이 중요시되어야 하나, 신규 진입을 할 때 장애물이 생기게 되면서 사업할 때 막히는 경우가 있다. 단적인 예로 보면 지역 난방을 개별 난방으로 전환되는 사업이라던가 24시간 자유자재 생활하게 되어 있어야 할 조건이 불리한 경우, 2019년 한일 무역 분쟁 이후 일본으로부터 신규 원자재 도입 시 특정된 업체들 에 직접 납품하게 될 재료가 필요하게 되면 당연히 넣어야 하는 조건이 있어야 한다. 그래도 안된다면 제3국으로 수출시킬 때 일본의 동의 없이 일본이 아닌 업체에 조달해야 하는 대체재를 바꿀 수밖에 없다. 하지만 이와 같은 원자재와 같이 긴요하게 공급되어야 할 필요 재료들을 직접 반입시킬 때 막히는 형태를 가진 나비 효과와도 이와 비슷한 연관성이 있다. 그 외에도 같은 조건만 제약된 형태로 생활하게 되어 있는 식으로만 사용하여도 진입 장벽이 적용될 수도 있게 된다. 끝으로 게임에서도 진입 장벽을 적용받는 경우가 있는 경우도 간혹 있지만 전 세계적으로 성공하게 되어 있는 사례는 포켓몬스터가 유일하다.\\n-------------------------\\n문서2: 개털과 머리카락, 흙, 규사, 목재가루에 이어 옷핀과 팝콘, 종이조각까지…. 16일 찾은 삼성전자 광주공장의 청소기 성능평가 시험실에서는 ‘더럽히기’ 작업이 한창이었다. 청소기 흡입력을 검증하기 위해 생활 쓰레기를 롤러로 카펫에 촘촘히 박았다. 실험 대상인 카펫 종류만 40~50종. 0.3㎛의 미세먼지 입자는 4㎏ 한 통에 200만원을 주고 사온다고 했다. 옆방에서는 애써 개발한 청소기에 대한 ‘고문’작업이 한창이었다. 로봇청소기는 감시 카메라가 지켜보는 가운데 하루 24시간 고장날 때까지 작동한다. 청소기 부품은 영하 40도에서 영상 120도를 오가며 500시간 동안 얼렸다 구워진다. 코드선은 거친 사포로 무자비하게 문지르고 호스는 좌우로 잡아당기길 반복한다. 온·오프 작동 버튼은 2만회를 눌러본 후 이상이 없어야 통과다. 지난 6월 삼성전자가 출시한 ‘모션싱크’는 이 지난한 평가 과정 외에도 한 단계를 더 거쳤다. ‘뒤집어지지 않는 청소기’라는 것을 입증하기 위한 주행성능 평가다. 이를 위해 2억원이 넘는 전용장비를 들여왔다. 호스를 빠르게 잡아당기고 갑자기 방향을 바꾸면서 기우는지를 시험한다. 직진성 시험은 개발자들끼리 커피내기 시합을 하며 진행했다. 긴 복도에서 청소기 본체를 볼링공처럼 굴리면 한번에 25~30ｍ는 직선으로 쭉 나갔다. 여기서 얻은 아이디어로 볼링장에서의 ‘모션싱크’ 유튜브 홍보영상도 완성됐다. 모션싱크 개발팀을 이끈 이준화 상무는 “청소기가 넘어지면 불편할 뿐만 아니라 먼지를 분리하는 부분이 막혀 쓰레기가 역류할 수도 있다”며 “납작하게 만들어보고 무게도 실어보면서 수많은 시행착오를 거쳤다”고 말했다. 문제 해결의 실마리는 본체와 바퀴가 따로 움직이는 ‘본체 회전’ 구조에서 찾았다. 바퀴의 모양과 크기도 관건이었다.이 상무는 “문턱을 넘기 위해 본체 높이 만큼 바퀴를 키웠다”며 “동시에 무게중심을 잡기 위해 경주용 휠체어처럼 바퀴 윗부분이 안쪽으로 경사진 캠버드 휠을 채용했다”고 소개했다. 그러자 또다른 과제가 생겼다. 바퀴가 너무 커 본체의 공간 활용도가 줄은 것. 이에 바퀴의 테두리만 남긴 채 중간을 뚫고 그 사이로 배기구멍을 냈다. 이 상무는 “이런 구조와 기술관련 특허가 수천 건”이라며 “국내에서 가장 많은 청소기 특허를 갖고 있다”고 말했다.모션싱크 개발엔 1년간 10여 명의 개발팀뿐 아니라 디자인과 양산팀 등 전 부서가 매달렸다. 악역은 이 상무가 맡았다. 수백 번째 다시 그린 설계도면을 받아들고서도 “청소기가 아니라 탱크”라는 독설을 서슴지 않았다. 몇 주간 밤을 새운 후 시제품을 완성했을 때도 “여기서 크기를 5%만 더 줄이자”고 다그쳤다. 새로운 벽에 부딪힐 때마다 “우리의 먼지 같은 지식으로 감히 안 된다고 단정짓지 말자”는 윤부근 소비자가전(CE)부문 사장의 말을 전하며 부하 직원들을 자극했다. 팀원들과 1998년 나가노 동계올림픽에서 김동성 선수가 마지막 순간 한 발을 쑥 내밀어 쇼트트랙 금메달을 딴 영상을 즐겨 봤다. 이 상무는 “지금 우리는 결승선 앞”이라며 “한 발만 더 뻗어 보자”고 독려했다. 그는 “그렇게 내민 한 발이 메달의 색을 결정한다”며 “끈질긴 혁신없이 비슷한 제품으로는 시장을 이끌어갈 수가 없다”고 말했다.\\n-------------------------\\n문서3: 영국의 진공청소기 전문업체 다이슨이 삼성전자의 신제품 청소기에 대해 특허 소송을 제기했다. 이 소송이 프리미엄 생활가전 마케팅을 강화하고 있는 삼성전자의 글로벌 영업 전략에 어떤 영향을 미칠지 주목된다. 최근 독일 베를린 IFA2013에서 막스 콘체 다이슨 최고경영자(CEO)는 기자와 만나 “최근 삼성전자가 출시한 모션싱크 청소기가 다이슨의 특허를 침해했다”며 “영국 고등법원에 특허 소송을 냈다”고 말했다. 그는 “아이디어와 혁신을 통해 공정하게 경쟁하길 원하고 있다”며 “하지만 경쟁사의 제품을 베끼는 기업들 때문에 정당한 경쟁이 되지 않아 힘들다”고 삼성을 비판했다. 다이슨은 ‘영국의 스티브 잡스’로 불리는 제임스 다이슨경이 1993년 창업한 회사다. 짧은 역사에도 불구하고 밀레, 보쉬, 일렉트로룩스 등 유럽 전통 강호들을 제치고 세계 청소기 시장 점유율 15%로 1위를 차지하고 있다. 다이슨 측은 삼성이 일반적으로 쓰이는 두 바퀴가 아닌 볼(ball) 기술을 이용한 다이슨의 실린더 청소기 메커니즘을 그대로 차용했다고 판단하고 있다. 삼성의 모션싱크에 대해 ‘모조품(rip-off)’이라고 표현했다. 그러나 구체적인 소송 내용을 비롯해 삼성 제품의 판매 금지나 라이선스 비용을 청구할지에 대해서는 언급하지 않았다. 하지만 디자인을 둘러싸고 진행된 삼성전자와 애플 간 소송과 달리 이번 특허 소송은 기술 특허를 침해한 것인 만큼 이길 확률이 높다는 게 다이슨 측 설명이다. 소송 대상이 된 삼성전자의 모션싱크는 올 6월 출시한 프리미엄 청소기다. 냉장고 세탁기 등 삼성전자 프리미엄 이미지를 청소기 등 소형가전으로까지 확대하기 위해 내놓은 야심작이다. A15면에 계속\\n-------------------------\\n문서4: 이 때문에 수천명의 관중들이 입석으로 향하는 좁은 터널에 몰리게 되었다. 하지만 경기장 중앙부에는 이미 많은 수의 관중들이 있었다. 하지만 터널 뒤쪽에 있던 사람들은 앞의 상황을 몰랐기 때문에 계속해서 앞 사람을 밀었고, 이는 입석 앞쪽에서 발생한 사고의 원인이 되었다. 평소에는 수용 한계 인원에 다다르면 경찰 또는 직원이 터널의 입구에 서서 진입을 막고 다른 곳으로 입장하도록 안내했지만, 이 날은 그리 하지 않았고, 그 이유는 여전히 제대로 해명되지 않았다.\\n\\n입석 앞쪽에서 발생한 이 문제를 다른 곳에 있는 사람들은 경기에 빠져들어 눈치채지 못하고 있었다. 3시 6분, 일부 팬들이 지나치게 붐비는 곳에서 빠져나오기 위해 펜스를 타고 오르기 시작하자, 경찰이 권고하여 심판은 경기를 잠시 중단시켰다. 이 때, 관중들은 펜스에 있는 작은 문을 억지로 열었고, 이를 통해 그 곳을 빠져나왔다. 리핑 레인 바로 위에 있던 웨스트 스탠드의 관중들은 뒷사람이 당겨주어 무사하였다.\\n\\n팬들은 펜스에 매우 빽빽하게 몰려 있었고, 이로 인해 많은 이들이 압력으로 질식사하고 만다. 관중석은 순식간에 아수라장이 되었다. 경찰과 직원, 구급 대원만으로는 사태를 처리할 수 없었다. 부상당하지 않은 일부 관중들은 심폐소생술을 실시하며 부상당한 관중들을 도왔고, 일부는 경찰, 직원들의 거짓 변명을 가차없이 반박했다. 사건이 발생한 후에도 일부 경찰관들은 리버풀의 관중들이 노팅엄 포리스트 관중들 쪽으로 가는 것을 막기 위해, 사고가 난 곳의 4분의 3에 저지선을 쳤다. 일부 팬들은 부상당한 관중들을 직접 구급차로 데려가기 위해 경찰의 저지선을 넘어가려 했으나 저지당했다.\\n-------------------------\\n문서5: 일 년전쟁 직전, 지구 연방군이 지온공국의 모빌 슈트 개발 계획을 입수한 후 급히 차기주력전차계획 RX계획을 변경하여 독자적으로 개발한 지구연방군 최초의 \\'모빌 슈트\\'이다.\\n\\n복잡한 2족 보행기구를 생략하고 전차포와 부무장을 \\'인간형\\'의 상반신에 탑재하여서, 이에 실망한 레빌 장군의 말을 빌면 \\'그냥 탱크\\'였다. 지구 중력하에서는 신뢰도가 높은 캐터필러 주행방식과 70km에 달하는 야지주행속도와 함께 장사정 실탄병기인 120mm 2연장 저반동포로 상당한 위력을 발휘했다. 우주공간에서의 사용을 위해 추진시스템을 탑재하기는 했으나, 하반신이 없으므로 간략화된 AMBAC기능으로 인해 운동성이 극히 낮았다.\\n\\n미노프스키-이요네스크형 핵융합로 개발 이전에 개발된 기체로 가스 터빈 엔진과 구식 핵융합로의 이중방식 제네레이터를 탑재하고 있다.(코어 블록 시스템이 추가되는 시점에서 핵융합로로 일원화되었다는 설도 있다.) 기본적으로 원거리 지원형 장비로 생산되었으므로 일 년전쟁 기간 동안 모빌 슈트라기보다는 이동포대로써의 운용이 많았고, 근접전이 되는 경우 상반신의 가동 범위가 낮고 부무장인 40mm 발사기의 위력이 낮아 매우 불리했다.\\n\\n개전 초기 사이드 7에서 테스트중이던 시제기 4기 중 1기(4호기)를 회수하여 화이트베이스에서 운용하였다. 복좌식 - 조종수와 사격수 - 으로 초기 운용되다 류 호세이 소위 전사 이후 개조하여 단좌식으로 운용하게 된다.'}, {'role': 'user', 'content': '모션싱크의 내부 공간을 이용하는 데 방해가 된 것은?'}, {'role': 'assistant', 'content': '모션싱크 청소기의 내부 공간을 이용하는 데 방해가 된 것은 바퀴의 크기였습니다. 모션싱크 청소기의 개발 과정에서 본체의 높이를 넘기 위해 바퀴를 키웠지만, 이로 인해 본체의 공간 활용도가 줄어드는 문제가 발생했습니다. 이를 해결하기 위해 바퀴의 테두리만 남기고 중간을 뚫어 그 사이로 배기구멍을 내는 방식으로 설계를 변경했습니다 [[ref2]].'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# sample[\"search_result\"] 리스트를 원하는 형식의 문자열로 변환하는 함수\n",
    "def format_search_result(search_results):\n",
    "    return \"\\n-------------------------\\n\".join([f\"문서{idx + 1}: {result}\" for idx, result in enumerate(search_results)])\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환하는 함수      \n",
    "def format_data(sample):\n",
    "    formatted_search_result = format_search_result(sample[\"search_result\"])\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message.format(search_result=formatted_search_result),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"question\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"answer\"]\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"iamjoon/klue-mrc-ko-rag-dataset\", split=\"train\")\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환\n",
    "dataset = [format_data(sample) for sample in dataset]\n",
    "\n",
    "print(dataset[345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "31a19b55-8441-4b24-9b9b-5c638c898eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 전체 데이터 개수: 1884\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191254b3c8a14fc1a8a1f0048f56145c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 개수: 942\n"
     ]
    }
   ],
   "source": [
    "# 현재 전체 데이터 개수\n",
    "print(f\"현재 전체 데이터 개수: {len(dataset)}\")\n",
    "\n",
    "# 테스트용으로 50% 따로 저장 \n",
    "test_size = int(len(dataset) * 0.5)\n",
    "test_dataset = dataset[-test_size:]  # 뒤에서 50%\n",
    "\n",
    "# 테스트셋 저장\n",
    "from datasets import Dataset\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "test_dataset.save_to_disk('test_dataset')\n",
    "print(f\"테스트 데이터 개수: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7e7de5a8-7355-45ad-97ae-da0363450332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수: 376\n"
     ]
    }
   ],
   "source": [
    "# 앞의 데이터 중 20%를 학습셋으로 사용\n",
    "dataset = dataset[:int(len(dataset) * 0.2)]\n",
    "\n",
    "print(f\"데이터 개수: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e1d9107a-5ff2-42b3-846a-2466b3933155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 형태의 dataset을 Dataset 객체로 변환\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "43136b31-164c-48d3-bc81-71e337177f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c536efd13944119590c8f1e111ef5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-7B-Instruct\" \n",
    "\n",
    "# BitsAndBytes 4비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,                             # 4비트 양자화 사용\n",
    "   bnb_4bit_use_double_quant=True,               # 이중 양자화 사용으로 메모리 추가 절약\n",
    "   bnb_4bit_quant_type=\"nf4\",                    # 4비트 양자화 타입 설정(normalized float 4)\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16         # 연산 시 bfloat16 타입 사용\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4b7f2fd6-8a17-4ff0-bc7c-33d406861ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-------------------------\n",
      "문서1: 열대우림 '장마전선'은 벵골만과 서북태평양에서 동아시아 몬순의 하위시스템으로 조성된다. '장마전선'의 북진 움직임은 아열대 능선이 발달한 데 영향을 받는다. 이 북쪽으로 이동하는 준정전선은 남한에서 '장마'라고 불리며, 주요 강수 기간을 나타낸다. 창마전선'은 한반도를 통과하는 데 약 4~5주가 걸린다. 이러한 느린 움직임은 매년 6월말과 7월에 한반도 전체에 많은 양의 여름 강우량을 발생시킨다. 최근 들어 '창마전선'은 7월 말부터 8월 초까지 다양한 규모의 폭풍우와 함께 폭우가 쏟아지면서 한반도를 통과하는 데 3주도 채 걸리지 않는 등 빠르게 움직이는 경향을 보였다. '창마' 이후 더 극한의 날씨와 국지적인 폭우가 발생하고 있다는 뜻이다. 잠열 방출에 의해 강하게 변형된 바로크린 교란에서 비롯된 초여름의 '창마' 비의 역학관계는 여전히 제대로 파악되지 않고 있다. 가을 창마로 부를 수 있는 또 다른 '창마' 유형도 있다. 이는 물론 기상청의 공식 용어는 아니다. 그러나 최근의 기후 변화로 인해 '낙하 창마'라는 용어가 생겨났다. '낙하 창마'는 보통 8월 말에서 9월 초에 시작한다. 한반도에서 북태평양고기압이 완전히 끝난 뒤 '폭포창마'도 끝났다. 최근의 이 '폭포창마'는 보통의 '창마'보다 훨씬 더 큰 피해를 가져오는데, 왜냐하면 '폭포창마'는 단기간에 극도의 폭우가 집중적으로 쏟아지기 때문이다. 장마 순환 변화가 없을 경우 강수량이 증가할 것으로 예상되지만 비교적 완만한 이동이나 시기 변화는 동중국인, 한국, 일본 기후에 큰 영향을 미칠 수 있다.\n",
      "-------------------------\n",
      "문서2: 올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.\n",
      "-------------------------\n",
      "문서3: 궤도물리학은 계절의 지속 기간이 지구의 궤도가 지점과 분점 사이의 공간을 휩쓸고 지나가는 면적이 클수록 길어지며, 따라서 만약 이심률이 극단적으로 커진다면 원일점 쪽에서 나타나는 계절이 오래 지속될 것이다고 예측한다. 현재 지구에서는, 지구가 근일점에 접근할수록(태양에 가까워질수록) 북반구는 가을을 지나 겨울로 향하고, 한편 남반구에서는 반대되는 계절이 나타나고 있다. 결과적으로, 북반구에서는 가을과 겨울이 봄과 여름보다 살짝 짧다. 하지만, 전 지구적으로 보았을 때는 남반구는 오히려 봄과 여름이 살짝 짧음으로서 균형이 맞는다. 2006년에는 밀란코비치 주기에 따라 북반구의 여름이 겨울보다 4.66일 더 길었고, 봄은 가을보다 2.9일 더 길었다. \n",
      "\n",
      "장축단선의 세차운동 또한 지구의 지점과 분점의 위치를 느리게 바꾸고 있다. 참고로, 이 움직임은 지구의 \"궤도\"를 바꾸는 것이지 지구의 자전축을 바꾸는 것이 아니다(자전축 변화는 자전축의 세차운동 참조). 다음 1만 년 동안, 북반구의 겨울은 조금씩 길어질 것이고 여름은 조금씩 짧아질 것이다. 하지만, 한 쪽이 차가워짐에 따라 반대쪽은 따뜻해지는 것처럼 어떠한 영향도 반대의 영향을 받을 것이다.\n",
      "-------------------------\n",
      "문서4: “물빛만 봐도 동해는 한눈에 알아볼 수 있습니다.” 한국 국적선 사상 최초로 북극항로 운항에 나선 스테나폴라리스호(號)가 베링해협을 지나 동해에 들어선 19일. 젊은 시절 10년간 항해 경험이 있는 남청도 해양대 교수는 유난히 푸른 바다를 가리키며 이렇게 말했다. 이제 목적지인 광양항까지는 이틀 남았다. 스테나폴라리스는 광양항에 도착한다. 지난 9월16일 밤 러시아 우스트루가를 떠난 지 35일 만이다. 바스코 알렉산더 선장은 “올해는 작년보다 해빙(海氷) 양이 늘어난 데다 쇄빙선 일정에 차질이 생겨 예상보다 약간 늦어졌다”며 “그래도 수에즈 운하를 통과하는 (북유럽~한국) 노선에 비해 8일 정도 운항일수를 줄였다”고 했다. 남 교수는 이번 북극항로 시범운항의 성과에 대해 “전체적으로 수에즈 운하를 지나는 항로에 비해 20만달러 정도 적게 들어간 것으로 파악하고 있다. 이 정도면 경제성이 충분하다”는 견해를 피력했다. 이승헌 현대글로비스 해기사는 “북극항로는 1만5500㎞ 정도로 수에즈 운하를 지나는 항로보다 7000㎞나 짧아 연료비가 30만달러 정도 덜 들어간다”며 “하지만 쇄빙선료와 아이스파일럿비 등을 통행료로 내야 하고 (사고 위험에 대비한) 보험료도 훨씬 높기 때문에 이 부분을 얼마나 줄이느냐가 북극항로의 경제성을 좌우할 것”이라고 말했다. 북극항로 통행료에 대해 패트릭 스반 스테나해운 매니저는 “스테나해운의 북극항로 운항 실적이 많아 이번 통행료는 t당 5달러를 적용해 총 20만달러 정도 들었다”며 “하지만 운항실적이 없는 선사는 훨씬 비싼 비용을 내야 한다”고 했다. 남 교수는 “선사의 운항실적 등 복잡한 규정에 따라 통행료가 달라지는 현재의 관행이 바뀌어야 한다”고 지적했다.북극항로의 안전성에 대해서는 긍정적인 평가가 나왔다. 하지만 쇄빙선 운항시스템은 개선해야 할 점으로 지적됐다. 남 교수는 “쇄빙선이 당초 만나기로 한 시간에 나타나지 않아 사나흘씩 기다려야 하는 현실은 북극항로 활성화의 장애 요인”이라고 꼬집었다. 스반 매니저는 “여름철 서너 달은 항해가 안전하고 경제성도 있다고 본다”면서도 “해빙 등의 변수가 있기 때문에 당분간 (컨테이너선보다) 벌크선 위주로 운항이 많아질 것”이라고 내다봤다. 9월23일 북극권에 들어선 스테나폴라리스는 9월30일 처음 해빙구간(얼음바다)에 들어섰다. 쇄빙선을 기다리느라 북극해 한가운데에서 발이 묶이기도 했다.총 390㎞에 달하는 얼음바다에선 항해 속도를 평소의 절반 수준으로 줄여야 했다. 가끔 얼음이 선체에 부닥치는 소리가 들렸지만 위협적으로 느껴지지는 않았다. 11일 베링해로 들어선 뒤부터 다시 속도를 높였다. 그동안 제대로 누리지 못한 햇빛이 갑판 위에 쏟아졌다. 남 교수가 일렁이는 물결을 내려다보며 읊조렸다. “북극항로를 단순히 바닷길 하나가 새로 열리는 정도로 생각하면 안 됩니다. 북극권이 열리며 새로 생겨나는 세계 질서 속으로 가는 ‘21세기의 비단길’이 아닐까요.”\n",
      "-------------------------\n",
      "문서5: 부산시와 부산관광공사는 “2021 세계지구과학총회 유치위원회”(위원장 서울대 이상묵 교수)가 지난달 31일 “2021년 세계지구과학총회(MOCA 2021)”를 부산으로 유치했다고 밝혔다. 세계지구과학총회는 1969년부터 4년마다 60개국 1,500명이 참가하는 국제행사이다. 국제측지 및 지구물리연맹(IUGG) 산하 독립협회인 국제기상대기과학연합회, 국제해양물리과학연합회, 국제빙권과학협회의 통합 학술대회로 구성된 지구과학분야 학술올림픽으로 한국에서는 처음으로 개최된다. 이번 세계지구과학총회의 부산 개최는 지난달 31일 남아프리카공화국 케이프타운에서 개최된 세계지구과학총회에서 확정되었다. 부산관광공사․IUGG한국지부‧한국기상학회‧한국해양학회‧대한지질학회‧벡스코는 유치단을 구성해 미래창조과학부‧부산시‧한국관광공사의 후원으로 공동 유치 활동을 벌여온 결과, 영국(맨체스터)과 치열한 경쟁을 벌여 유치에 성공했다. 한국유치단은 이번 유치를 위해 남아공 케이프타운을 직접 방문해 두 차례에 걸쳐 유치제안 PT발표를 진행했다. 또한 부산 유치 홍보관을 운영하며, 국제회의 도시 부산의 매력을 알리는데 총력을 기울였다. 유치단은 제안발표에서 한국의 지구 환경 문제에 대한 국가적 관심, 빙권 과학에 대한 투자 등 동북아 중심국가로서의 선도적 역할에 대해 강조했다. 특히 APEC기후센터, 한국해양과학기술연구원, 국립수산과학연구원 등 지구과학 관련 대학과 연구기관이 부산에 소재하고 있는 점을 집중 어필하며 부산 개최의 당위성에 대해서도 강조했다. 한편 국제측지 및 지구물리연맹(IUGG)은 지구과학 및 우주환경 관련 홍보와 국제 협력촉진을 목적으로 1919년 설립되었으며, UN, UNESCO 등의 국제기구와 협력관계를 가지고 있다. 2021년 세계지구과학총회는 7월 18일부터 23일까지 6일간 부산 벡스코에서 개최될 계획이며 각국의 지구과학관련 전문기관, 업체들이 참여하는 전시회를 비롯해 세계적 석학들의 강연과 전문가 토론, 영화도시 부산의 특성을 살린 지구과학영화제, 에세이콘테스트, 젊은 과학자의 밤 등으로 구성될 예정이다.<|im_end|>\n",
      "<|im_start|>user\n",
      "북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "북태평양 기단과 오호츠크해 기단이 만나 형성되는 장마전선은 한반도에 약 한 달가량 머무릅니다. 문서2에 따르면, 장마전선은 한반도 중남부를 오르내리며 약 한 달 동안 비를 뿌린다고 합니다. 이는 최근 30년간의 평균치와도 일치하며, 중부지방의 장마기간은 평균 32일로 나타났습니다 [[ref2]].<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "30e267a6-5dcc-4cba-aaba-d85bb75d07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    " \n",
    "# QLoRA 논문 및 Sebastian Raschka 실험에 기반한 LoRA Conifg\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        r=6,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce608537-ab12-4239-81e5-146b31dda2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-rag-ko\",     # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=3,                      # 학습할 총 에포크 수\n",
    "    per_device_train_batch_size=2,           # 장치당 학습 배치 크기\n",
    "    gradient_accumulation_steps=2,           # 역전파/가중치 업데이트 전 누적할 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 그래디언트 체크포인팅 사용\n",
    "    optim=\"adamw_torch_fused\",               # 퓨즈드 AdamW 옵티마이저 사용\n",
    "    logging_steps=10,                        # 10스텝마다 로그 기록\n",
    "    save_strategy=\"steps\",                   # 특정 스텝마다 체크포인트 저장\n",
    "    save_steps=50,\n",
    "    bf16=True,                              # bfloat16 정밀도 사용\n",
    "    tf32=True,                              # tf32 정밀도 사용\n",
    "    learning_rate=1e-4,                     # 학습률 (QLoRA 논문 기반)\n",
    "    max_grad_norm=0.3,                      # 최대 그래디언트 노름 (QLoRA 논문 기반)\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율 (QLoRA 논문 기반)\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률 스케줄러 사용\n",
    "    push_to_hub=False,                      # 허브에 모델 업로드 안 함\n",
    "    report_to=\"tensorboard\",                # 텐서보드에 메트릭 기록\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b84572ba-4421-4b06-9399-e4bb12c7a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        # messages의 각 내용에서 개행문자 제거\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        # 깨끗해진 메시지로 템플릿 적용\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        # 텍스트를 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # 레이블 초기화\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # assistant 응답 부분 찾기\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        # 토큰 ID 가져오기\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # <|im_start|>assistant 찾기\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                # assistant 토큰 찾기\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    # assistant 응답의 시작 위치로 이동\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    # <|im_end|>를 찾을 때까지 레이블 설정\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "                            # <|im_end|> 토큰도 레이블에 포함\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    # 패딩 적용\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch\n",
    "\n",
    "def print_tokens_and_labels(batch):\n",
    "    input_ids = batch[\"input_ids\"][0].tolist()\n",
    "    labels = batch[\"labels\"][0].tolist()\n",
    "    \n",
    "    print(\"\\n토큰과 레이블 비교:\")\n",
    "    print(f\"{'Token ID':<10} {'Token':<30} {'Label':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for token_id, label in zip(input_ids, labels):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        label_str = str(label) if label != -100 else \"-100\"\n",
    "        print(f\"{token_id:<10} {token:<30} {label_str:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f87dc9a9-72fd-4985-9d84-85ec1c4a42fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 4757])\n",
      "어텐션 마스크 형태: torch.Size([1, 4757])\n",
      "레이블 형태: torch.Size([1, 4757])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2800c3e8-2108-4f4c-abac-218c6902c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[151644, 8948, 198, 64795, 82528, 33704, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 138520, 19391, 143604, 129264, 130650, 382, 13146, 48431, 20401, 66790, 29326, 131193, 17877, 125686, 125548, 139713, 624, 16, 13, 138520, 53680, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 143604, 16186, 139713, 624, 17, 13, 85322, 77226, 98801, 19391, 130768, 130213, 17877, 143604, 16186, 125476, 34395, 53900, 21329, 95577, 139713, 624, 18, 13, 138520, 19391, 128605, 143603, 12802, 85322, 77226, 98801, 19391, 130671, 32290, 85322, 77226, 98801, 126377, 330, 33883, 64795, 138520, 93, 19391, 128605, 130213, 12802, 136673, 1189, 5140, 45881, 34395, 143604, 16186, 139713, 624, 19, 13, 143604, 47836, 53618, 142976, 139236, 18411, 142616, 82190, 53435, 40853, 129549, 53435, 125068, 17877, 140174, 128836, 32290, 5140, 240, 97, 19391, 36330, 250, 125746, 16560, 23084, 126402, 83634, 17380, 94613, 139236, 84621, 47324, 18411, 129624, 20487, 139713, 13, 95617, 18411, 129901, 26698, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 56475, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 5053, 130939, 54116, 57132, 16186, 139713, 624, 20, 13, 95617, 18411, 129901, 26698, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 80573, 220, 20, 42044, 139236, 56475, 143409, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 20492, 4318, 1097, 20, 5053, 130939, 54116, 57132, 16186, 139713, 624, 21, 13, 81173, 66845, 23573, 49367, 23259, 20401, 139236, 18411, 58677, 26699, 82190, 143604, 16186, 139713, 382, 129845, 77226, 98801, 510, 771, 28447, 51588, 26698, 16, 25, 2587, 23573, 144286, 481, 32129, 129616, 129439, 129283, 129392, 29326, 92817, 198, 9, 74361, 222, 129150, 56475, 20401, 58034, 130803, 549, 10764, 245, 249, 24897, 145008, 11, 30520, 120, 11, 10764, 245, 249, 24897, 145008, 11, 54969, 126893, 11, 30520, 120, 11, 5140, 96, 101, 144120, 127165, 120, 198, 9, 220, 17, 53189, 135628, 81133, 220, 17, 130454, 56475, 5140, 96, 101, 144120, 127165, 120, 85251, 17877, 125834, 128836, 13, 32129, 129616, 129439, 129283, 16560, 1036, 55054, 125086, 33704, 84621, 28626, 47985, 128646, 128836, 13, 130331, 129882, 125544, 126615, 124685, 29281, 145263, 23573, 47911, 241, 17877, 95002, 28733, 16560, 130768, 127042, 144357, 12802, 125590, 20428, 141767, 28626, 135152, 28626, 72553, 42039, 125834, 128836, 854, 129254, 126254, 128836, 13, 4710, 19391, 60315, 131767, 16560, 1036, 60315, 16560, 73077, 59698, 52959, 12802, 93182, 129885, 32129, 129616, 129439, 129283, 129381, 126310, 128909, 12802, 62071, 32077, 125149, 28002, 129254, 129238, 13146, 13, 74361, 222, 129150, 56475, 5140, 255, 246, 128646, 126204, 64521, 21329, 143552, 136105, 127816, 854, 129254, 22042, 251, 144472, 13146, 13, 125714, 134133, 129439, 129283, 85997, 29326, 125544, 136294, 90486, 19969, 80573, 141767, 88259, 126746, 19969, 220, 16, 24, 23, 19, 126216, 38523, 105, 141274, 65865, 56475, 220, 23, 125568, 126299, 74361, 230, 134088, 85251, 17877, 54116, 49664, 129330, 19969, 140887, 74361, 222, 25715, 31328, 73077, 12802, 29326, 49367, 12802, 21329, 17380, 126327, 5140, 243, 227, 130454, 17877, 90711, 246, 57801, 134108, 60960, 20487, 49664, 17877, 125674, 40281, 21329, 129293, 23573, 71108, 53680, 129676, 1036, 25715, 144793, 129007, 36978, 116, 126886, 12802, 143416, 124889, 19391, 136009, 3369, 66845, 31079, 527, 16560, 5140, 228, 241, 133847, 79004, 135379, 71108, 854, 130939, 126254, 128836, 271, 26, 23573, 144286, 481, 73077, 129616, 125054, 50340, 126429, 21329, 198, 9, 74361, 222, 129150, 56475, 20401, 58034, 130803, 549, 5140, 228, 241, 130436, 11, 54969, 126893, 11, 10764, 245, 249, 24897, 145008, 127165, 120, 198, 9, 220, 17, 53189, 135628, 81133, 127042, 30520, 120, 56475, 10764, 245, 249, 24897, 145008, 127165, 120, 85251, 17877, 125834, 128836, 13, 73077, 129616, 125054, 129615, 1036, 134246, 125548, 13146, 0, 133267, 5140, 117, 254, 125548, 124905, 11, 23084, 124873, 77596, 235, 47985, 45710, 38177, 62618, 13146, 13, 138665, 144229, 88781, 19391, 5140, 228, 240, 33704, 125466, 19391, 77596, 238, 17877, 66136, 134372, 34395, 89659, 126254, 137750, 854, 129254, 126254, 128836, 198, 26, 23573, 144286, 481, 35509, 129283, 10764, 252, 230, 124419, 21329, 198, 9, 74361, 222, 129150, 56475, 20401, 58034, 130803, 549, 10764, 245, 249, 24897, 145008, 11, 30520, 120, 11, 54969, 126893, 11, 10764, 245, 249, 24897, 145008, 127165, 120, 198, 9, 220, 17, 53189, 135628, 81133, 220, 16, 130454, 56475, 10764, 245, 249, 24897, 145008, 127165, 120, 85251, 17877, 125834, 128836, 13, 35509, 129283, 16560, 1036, 95218, 19969, 128753, 20487, 49664, 60960, 55902, 12802, 130723, 34395, 53900, 130105, 53442, 130108, 60716, 26698, 81676, 74808, 130472, 12802, 19391, 131417, 144162, 63332, 125476, 34395, 44518, 47985, 125580, 125590, 20428, 127166, 144183, 141767, 59698, 133692, 133995, 13146, 13, 133267, 5140, 117, 101, 144110, 13146, 17839, 129254, 126254, 128836, 271, 18, 61741, 95577, 93672, 29346, 19391, 38523, 105, 144110, 17877, 53618, 90486, 60315, 131767, 16560, 58777, 40853, 66136, 19391, 54116, 79716, 12802, 73518, 145292, 131219, 20401, 126429, 35711, 77953, 12802, 129423, 133485, 34395, 135511, 129337, 143862, 124245, 19946, 34395, 129112, 220, 24, 137104, 74361, 222, 25715, 31328, 35509, 129283, 19969, 220, 16, 53189, 135628, 81133, 220, 16, 130454, 19391, 23084, 33704, 220, 18, 88259, 83666, 11, 220, 16, 126746, 132612, 22042, 109, 126730, 28626, 74808, 129321, 42039, 54969, 126893, 130454, 17877, 37195, 254, 134828, 13146, 13, 49367, 63089, 59698, 19969, 98869, 23259, 95577, 140532, 18411, 66136, 125615, 134497, 23084, 74361, 222, 88259, 18411, 3315, 104, 241, 33509, 125476, 53900, 25715, 11, 90486, 60315, 131767, 16560, 3369, 134277, 50340, 19969, 21329, 125544, 71334, 129254, 74884, 116, 144172, 13146, 13, 49367, 63089, 59698, 47985, 132377, 126317, 3315, 104, 241, 52959, 35509, 125476, 125615, 126322, 67511, 17877, 48108, 230, 145063, 13146, 13, 10764, 249, 245, 129378, 90486, 60315, 131767, 16560, 1036, 126793, 126893, 130454, 129381, 130270, 3315, 104, 241, 52959, 19969, 21329, 50696, 52959, 47985, 97143, 126551, 129242, 143703, 28002, 23084, 40771, 112, 129471, 129567, 56475, 47665, 245, 31079, 60315, 34395, 28927, 114, 33704, 28927, 105, 65306, 12802, 125761, 854, 129254, 126254, 128836, 90486, 60315, 131767, 16560, 54825, 126844, 10764, 230, 105, 88259, 56475, 141767, 28626, 135628, 81133, 134014, 61298, 19969, 93672, 124419, 17380, 8620, 121, 224, 125511, 16560, 77596, 235, 88259, 18411, 126365, 246, 83277, 10764, 240, 222, 141767, 145008, 42039, 10764, 245, 249, 24897, 145008, 17877, 61298, 35509, 129283, 16560, 127165, 120, 85251, 17877, 125834, 128836, 13, 54116, 49664, 12802, 34143, 105, 32831, 128841, 143803, 24485, 226, 125118, 126588, 46832, 246, 137032, 61298, 144286, 34143, 230, 29326, 127121, 56039, 89659, 58777, 40853, 137138, 18411, 5140, 240, 97, 144487, 125761, 13, 90486, 60315, 131767, 16560, 79302, 239, 130640, 17877, 5140, 228, 240, 12802, 129900, 131611, 62107, 32290, 20401, 125714, 43590, 18411, 5140, 251, 226, 57801, 72344, 238, 13146, 198, 771, 28447, 51588, 26698, 17, 25, 44518, 130765, 43115, 20487, 129889, 220, 16, 21, 12802, 144262, 17877, 126365, 246, 83277, 220, 16, 20, 125086, 138913, 54116, 49664, 42905, 77002, 128677, 133864, 125953, 21329, 133995, 125590, 129238, 31328, 42039, 26698, 16560, 136065, 42039, 73523, 127559, 65865, 129296, 126835, 10764, 230, 105, 23259, 17380, 66790, 79632, 133381, 13, 85997, 129695, 16186, 125544, 126429, 129510, 127194, 20401, 94315, 130609, 12802, 130898, 61298, 136541, 125166, 125544, 125674, 12802, 63089, 5140, 45881, 12802, 130000, 24897, 20401, 74884, 222, 141571, 12802, 10764, 252, 230, 124419, 52959, 126896, 47985, 73523, 127559, 65865, 129296, 126835, 17380, 77002, 129382, 33883, 220, 20, 12802, 144262, 128878, 127042, 125511, 28626, 17380, 81058, 126063, 125590, 220, 22, 61741, 19391, 220, 17, 126333, 142786, 125120, 17877, 10764, 245, 230, 26699, 132537, 127864, 65865, 64795, 128836, 13, 74361, 222, 125519, 136448, 220, 16, 138913, 125686, 126551, 72553, 54825, 133847, 220, 18, 126246, 125166, 220, 17, 125086, 138913, 54116, 49664, 82190, 45104, 101, 65865, 10764, 230, 105, 135444, 97143, 125761, 13, 54825, 74884, 116, 130612, 142512, 144626, 144018, 126310, 11, 23084, 80573, 141571, 125544, 10764, 252, 230, 55054, 29326, 47985, 73523, 127559, 65865, 129296, 126835, 10764, 230, 105, 23259, 135979, 77002, 129382, 126063, 125590, 130593, 45104, 101, 65865, 10764, 230, 105, 135444, 97143, 125199, 20487, 129147, 73523, 127559, 65865, 10764, 230, 105, 135444, 220, 18, 79632, 130593, 74884, 226, 129924, 133087, 18411, 125834, 23573, 128584, 136108, 220, 20, 15, 126216, 62107, 19391, 136293, 84255, 17380, 23872, 120, 88259, 56475, 20401, 126616, 20487, 49664, 12802, 73518, 141438, 382, 29281, 131005, 44518, 144393, 129889, 10764, 39834, 95218, 81173, 13146, 31328, 220, 16, 18, 130766, 17877, 38523, 105, 125476, 220, 17, 15, 18, 13, 17, 23084, 144262, 20401, 74884, 226, 129924, 220, 22, 59761, 11, 125149, 130454, 128013, 43115, 20487, 220, 19, 125625, 131000, 11, 74361, 230, 134088, 85251, 220, 16, 24, 17, 59761, 11, 8494, 3298, 33704, 220, 16, 13, 15, 17, 20401, 132968, 128677, 80968, 17877, 129624, 144141, 13146, 13, 130549, 142510, 144109, 125120, 73523, 135444, 132376, 251, 19969, 64119, 131611, 220, 17, 24, 59761, 19969, 73518, 80573, 220, 16, 16, 133087, 18411, 54116, 49664, 128836, 13, 73077, 144041, 24897, 86831, 144950, 17380, 24897, 56419, 129889, 220, 22, 65865, 220, 22, 130766, 131411, 54116, 66845, 130408, 20401, 128677, 80968, 17877, 135513, 144107, 34395, 81173, 126337, 65865, 136448, 79207, 117, 28002, 33883, 10764, 39834, 17877, 62938, 135628, 144026, 24897, 44518, 28002, 134133, 126616, 69923, 19391, 54116, 57026, 128836, 382, 134644, 135628, 144026, 24897, 44518, 28002, 134133, 54969, 12802, 139287, 79207, 44680, 72509, 12802, 21329, 129889, 94315, 141571, 57268, 129616, 126291, 126445, 28626, 144539, 81133, 91043, 81133, 24897, 80573, 58034, 66845, 132537, 44518, 144393, 129889, 69441, 231, 139746, 25715, 126712, 126333, 12802, 220, 18, 13, 18, 17, 124982, 125590, 220, 15, 130766, 220, 19, 133087, 129615, 81173, 131893, 20401, 128677, 80968, 17877, 129624, 144141, 13146, 13, 130549, 10764, 235, 120, 53189, 79207, 44680, 72509, 12802, 21329, 220, 16, 125625, 65865, 7, 41429, 12802, 63089, 65865, 11, 220, 22, 53680, 220, 15, 14, 18, 12802, 144262, 11, 220, 17, 125086, 126333, 8, 56475, 220, 16, 125086, 126333, 74884, 226, 129924, 79207, 117, 133886, 126352, 145102, 34395, 81173, 126337, 65865, 31328, 220, 21, 125625, 65865, 136448, 220, 19, 59761, 20401, 142510, 126246, 125166, 80573, 74884, 226, 135402, 79207, 117, 133886, 126352, 126923, 16560, 77002, 62938, 135628, 144026, 24897, 44518, 28002, 134133, 32129, 55902, 44518, 144393, 220, 18, 80901, 124982, 125615, 10764, 39834, 12802, 136065, 42039, 136293, 44518, 28002, 134133, 126616, 69923, 19391, 54116, 57026, 82190, 127218, 47985, 54969, 12802, 139287, 79207, 44680, 72509, 12802, 21329, 31866, 19391, 129296, 29281, 133381, 13, 136293, 44518, 28002, 134133, 136448, 220, 16, 125625, 65865, 53680, 220, 21, 125625, 65865, 19391, 129296, 126835, 77002, 129382, 33883, 220, 16, 125625, 65865, 129889, 220, 17, 125086, 126333, 91043, 129924, 82190, 79207, 117, 28002, 33883, 127218, 42039, 26698, 16560, 136065, 42039, 136293, 44518, 28002, 134133, 20401, 79207, 117, 28002, 10764, 230, 105, 135444, 97143, 125761, 624, 771, 28447, 51588, 26698, 18, 25, 220, 16, 24, 21, 15, 126216, 19391, 55673, 83036, 59698, 19391, 38150, 125068, 82190, 129381, 60716, 220, 16, 24, 21, 15, 126216, 220, 20, 128514, 220, 22, 32077, 49367, 12802, 35711, 85413, 101, 32077, 24897, 65865, 56475, 84255, 17380, 5140, 41902, 144687, 48364, 104, 142786, 125120, 17877, 53618, 125476, 144107, 34395, 220, 16, 24, 21, 18, 126216, 126377, 220, 17, 126746, 23259, 20401, 55673, 65865, 64577, 133886, 129882, 21329, 128836, 13, 54825, 60716, 44518, 144393, 126377, 220, 20, 15, 59761, 20401, 129392, 126746, 18411, 54116, 49664, 82190, 129392, 126746, 137078, 74361, 222, 12802, 139982, 17877, 27767, 251, 128739, 125580, 34395, 68232, 126216, 19391, 74808, 69923, 52300, 54070, 28002, 129392, 126746, 18411, 60960, 82528, 96137, 90486, 129283, 128753, 12802, 59698, 80573, 129676, 55673, 83036, 59698, 20401, 134585, 79207, 44680, 225, 222, 129296, 135444, 72344, 238, 13146, 382, 12802, 127033, 220, 16, 24, 21, 20, 126216, 53680, 220, 16, 24, 22, 18, 126216, 126377, 125206, 85057, 125674, 84621, 83666, 20401, 129392, 126746, 137078, 17877, 27767, 251, 128739, 132537, 138267, 129704, 19391, 35509, 41429, 126204, 74361, 222, 126614, 129889, 220, 16, 24, 21, 24, 126216, 19391, 220, 17, 19, 59761, 20401, 142786, 125120, 17877, 54116, 49664, 42905, 77002, 125206, 85057, 220, 17, 18, 21, 144109, 125120, 17877, 54116, 49664, 33883, 28733, 70582, 129889, 22042, 109, 10764, 228, 254, 24897, 11, 131522, 60294, 131196, 10764, 228, 254, 24897, 134454, 98358, 126337, 63332, 57026, 54330, 131611, 220, 16, 24, 22, 19, 126216, 126377, 10764, 39834, 20401, 220, 17, 15, 126216, 62107, 20401, 56983, 48606, 124657, 130766, 19391, 54116, 57026, 128836, 13, 48364, 104, 74361, 222, 129150, 56475, 48364, 104, 142786, 125120, 17877, 54116, 49664, 23573, 129296, 23259, 135979, 125206, 85057, 220, 17, 15, 15, 144109, 125120, 11, 125206, 85057, 220, 17, 15, 15, 15, 126246, 125166, 18411, 54116, 49664, 23573, 129296, 23259, 16560, 49367, 129616, 136125, 136065, 12802, 125199, 34395, 125206, 85057, 220, 17, 15, 15, 144109, 125120, 53680, 220, 17, 15, 15, 59761, 20401, 10764, 251, 105, 76435, 125166, 18411, 20136, 102, 133847, 26698, 54116, 49664, 23573, 129296, 23259, 47985, 49367, 129616, 136125, 136065, 125489, 13, 4710, 16, 24, 22, 19, 126216, 19391, 56983, 48606, 124657, 130766, 12802, 135763, 52300, 126844, 129378, 220, 16, 15, 128514, 220, 16, 18, 32077, 17380, 95617, 29281, 128993, 35339, 125615, 126429, 50340, 141571, 136233, 58777, 40853, 56475, 20401, 85997, 56039, 40281, 28002, 80573, 20401, 44518, 144393, 81173, 126337, 65865, 129889, 73518, 19969, 29326, 125544, 44518, 57801, 57268, 20401, 16751, 222, 141008, 43115, 20487, 124982, 125590, 124657, 129034, 42039, 58677, 33883, 220, 16, 19, 32077, 17380, 77353, 20487, 64119, 131611, 55673, 83036, 59698, 20401, 124657, 130766, 10764, 235, 120, 135152, 29346, 80573, 83556, 29281, 12802, 23894, 117, 144172, 13146, 13, 23084, 129147, 10764, 39834, 33704, 73077, 29326, 125544, 23872, 120, 24897, 127121, 28002, 11, 94315, 21329, 60315, 56039, 37064, 57160, 92120, 57268, 136357, 18585, 232, 33704, 129296, 23259, 126253, 60315, 16751, 222, 141008, 137351, 65865, 12802, 139325, 10764, 252, 230, 17380, 127121, 23084, 55054, 57268, 18411, 133970, 23573, 138267, 54330, 65865, 128911, 129296, 23259, 64850, 72553, 17877, 36330, 250, 65865, 135298, 34395, 55673, 83036, 59698, 20401, 55673, 65865, 129296, 23259, 16560, 129381, 37195, 254, 73518, 34395, 89659, 56475, 20401, 124657, 130766, 10764, 235, 120, 135152, 29346, 18411, 133083, 82619, 40853, 16186, 129615, 58777, 125068, 20401, 66790, 29326, 19969, 66136, 125476, 141798, 13, 23084, 19391, 131978, 49367, 129616, 135854, 1036, 80901, 66845, 23573, 129296, 23259, 126327, 16560, 126423, 131000, 128841, 83556, 125489, 854, 129254, 143231, 128618, 59761, 132537, 127218, 72553, 125866, 47985, 36330, 250, 65865, 29326, 136387, 129062, 129254, 58777, 125068, 126327, 142654, 20401, 125580, 125590, 58777, 125068, 3315, 116, 94, 33704, 49367, 129616, 20487, 20401, 135797, 18411, 135839, 126253, 21329, 133995, 34395, 49367, 129616, 135854, 124657, 130766, 10764, 235, 120, 135152, 29346, 56475, 44518, 126337, 48408, 125568, 126423, 77226, 23573, 139052, 29281, 17877, 47911, 241, 34395, 137046, 34395, 129112, 13, 49367, 129616, 135854, 54825, 94315, 73518, 19969, 29326, 125544, 126327, 56419, 56290, 96137, 32129, 53680, 23573, 132091, 138630, 141798, 7, 14009, 128514, 62275, 48009, 527, 129889, 140730, 91043, 29326, 127121, 27767, 120, 12802, 59698, 47985, 73518, 19969, 29326, 125544, 126327, 56419, 56290, 18411, 131961, 31079, 32129, 53680, 128836, 16560, 130213, 12802, 126423, 125476, 137046, 568, 129381, 60716, 70585, 107, 124419, 73077, 28002, 130000, 24897, 80573, 131417, 141142, 33704, 136293, 44518, 28002, 134133, 129889, 220, 16, 125625, 65865, 126558, 32985, 117, 125991, 125535, 17877, 10764, 236, 120, 144172, 125590, 126429, 50340, 141571, 136233, 58777, 40853, 56475, 130847, 129807, 220, 18, 125625, 65865, 129889, 127218, 12802, 90711, 250, 74361, 222, 88259, 18411, 74884, 120, 126835, 19391, 131417, 52959, 85403, 55902, 42039, 10764, 229, 112, 40853, 64795, 128836, 13, 85322, 55054, 98801, 45130, 101, 126550, 136854, 38150, 125199, 34395, 220, 16, 42044, 74361, 222, 25715, 124982, 125615, 49367, 129616, 136125, 85403, 55902, 42039, 5140, 117, 254, 85251, 55673, 83036, 59698, 16560, 70585, 107, 124419, 126327, 45104, 101, 132537, 136293, 44518, 28002, 134133, 124657, 130766, 34143, 105, 32831, 19391, 60985, 98642, 128836, 13, 4710, 16, 24, 22, 23, 126216, 126558, 220, 16, 24, 23, 15, 126216, 128878, 129296, 23259, 23894, 116, 3315, 65291, 59698, 17380, 140968, 57089, 125580, 34395, 220, 17, 126746, 23259, 135979, 47665, 254, 53189, 73518, 31328, 220, 22, 125625, 131000, 28733, 55902, 33704, 136293, 84255, 17380, 23872, 120, 88259, 32129, 55902, 81173, 13146, 136262, 220, 16, 24, 23, 15, 126216, 126377, 44518, 28754, 20401, 130408, 12802, 127353, 220, 17, 16, 126216, 62275, 20401, 141526, 126346, 47818, 125991, 17877, 16751, 222, 141008, 128836, 13, 44518, 144393, 98358, 63256, 94315, 73518, 34395, 89659, 58777, 40853, 56475, 130847, 129807, 27767, 120, 28626, 144036, 56983, 48606, 38523, 105, 141274, 126322, 26698, 60960, 126524, 65865, 56475, 220, 16, 136999, 45710, 31328, 127218, 129885, 220, 16, 126216, 137769, 84255, 17380, 19391, 38150, 125068, 23573, 73077, 32129, 13146, 16186, 126746, 80573, 129676, 16751, 222, 141008, 76337, 17877, 35509, 141798, 624, 771, 28447, 51588, 26698, 19, 25, 3315, 65291, 24897, 129262, 21329, 135444, 220, 19, 19, 13, 16, 20, 128808, 31328, 28626, 5140, 244, 101, 31079, 130974, 77002, 134583, 33883, 48364, 104, 126352, 53442, 32077, 130092, 132376, 251, 29326, 19969, 91043, 52300, 128753, 34395, 76337, 17877, 90711, 246, 144277, 13146, 13, 46832, 246, 132841, 3369, 78125, 128808, 527, 80573, 126423, 80968, 3369, 131347, 126614, 527, 12802, 23894, 117, 133847, 127165, 120, 32831, 134064, 80573, 141526, 66845, 143995, 77002, 132376, 251, 29326, 79302, 239, 66845, 53496, 66261, 3369, 65865, 13935, 125625, 7, 99866, 13935, 102107, 8, 129534, 125068, 527, 12802, 61298, 144124, 42044, 19391, 125149, 127085, 141798, 13, 14520, 114, 124780, 125747, 20487, 55054, 362, 18, 32290, 220, 17, 32077, 126310, 19969, 128844, 128739, 133627, 56475, 3315, 65291, 24897, 129262, 21329, 23259, 16560, 220, 17, 13, 17, 15, 4, 53900, 132772, 23573, 220, 16, 24, 21, 22, 13, 16, 24, 19391, 126352, 53442, 18411, 95577, 144172, 13146, 13, 74884, 116, 124785, 31328, 10764, 230, 105, 25715, 130427, 220, 18, 16, 18, 21, 127475, 54321, 31079, 59698, 18411, 143802, 129865, 47985, 132537, 143802, 76337, 62275, 19391, 3315, 65291, 24897, 129262, 21329, 135444, 220, 17, 15, 15, 15, 125519, 56475, 220, 16, 24, 22, 15, 125519, 22042, 239, 42039, 40771, 231, 132772, 128836, 13, 128792, 124785, 135227, 44518, 19969, 131120, 132652, 58034, 80901, 98358, 87608, 45710, 54330, 17380, 60960, 131837, 126932, 47985, 19391, 73518, 26698, 60960, 126414, 54116, 124517, 20401, 55673, 19969, 19969, 136115, 85403, 85251, 128836, 13, 77353, 132618, 142234, 131396, 55673, 35711, 54116, 124517, 20401, 68232, 126216, 220, 19, 79716, 20487, 126423, 133864, 54116, 66845, 19391, 129293, 125714, 142588, 71108, 131411, 124657, 125476, 19391, 53900, 132772, 135257, 12802, 89095, 97, 141798, 13, 127165, 120, 32831, 134064, 55673, 132184, 56419, 126352, 53442, 32077, 129885, 220, 19, 13, 20, 24, 4, 37195, 106, 33704, 220, 16, 18, 15, 72553, 24, 15, 15, 15, 54321, 128878, 5140, 244, 101, 31079, 141798, 13, 74884, 116, 124785, 124781, 132376, 251, 128739, 55054, 31328, 425, 26227, 126793, 28002, 130271, 19969, 1036, 21329, 126588, 33883, 220, 19, 79716, 20487, 126440, 124517, 12802, 131870, 12802, 125834, 132618, 95617, 55902, 129885, 220, 17, 92817, 54321, 53989, 226, 31079, 81676, 220, 23, 92817, 22, 23, 15, 15, 127475, 54321, 19391, 126488, 53680, 47836, 71108, 854, 125866, 124905, 134952, 126414, 54330, 19969, 18411, 220, 17, 18, 15, 72553, 54321, 56475, 220, 17, 15, 15, 72553, 54321, 42039, 220, 16, 18, 4, 37195, 106, 144162, 36330, 102, 126614, 17877, 53989, 105, 13146, 13, 23084, 129378, 37195, 247, 135257, 33704, 133146, 33883, 220, 21, 128514, 22, 32077, 48780, 129439, 62275, 12802, 141767, 125544, 28626, 140038, 126423, 80968, 5140, 239, 242, 56290, 19969, 124657, 125476, 24897, 134798, 13146, 124905, 126932, 47985, 56983, 128808, 28626, 18411, 66136, 220, 21, 13, 16, 23, 4, 40771, 231, 132772, 23573, 136331, 81173, 66845, 59698, 13146, 13, 54321, 13935, 129062, 60294, 46832, 246, 132841, 12802, 129359, 126402, 220, 20, 126216, 19, 59761, 128514, 62107, 19391, 81173, 126781, 59698, 31328, 220, 16, 15, 19, 23, 54321, 18, 15, 65865, 128878, 5140, 244, 101, 31079, 130974, 77002, 129093, 56290, 129413, 41429, 19969, 40771, 231, 129471, 128555, 132876, 130357, 24485, 242, 56290, 23872, 121, 41429, 54116, 92817, 19969, 23084, 31079, 21329, 131611, 64577, 57089, 125625, 54330, 47985, 10764, 252, 246, 17877, 3315, 241, 108, 21329, 129293, 128836, 13, 141526, 66845, 125625, 4080, 20, 13, 15, 22, 33871, 54116, 52959, 125625, 4080, 21, 13, 15, 21, 11334, 127871, 130651, 141526, 66845, 80901, 52959, 4080, 22, 13, 18, 22, 33871, 61298, 50340, 126981, 130229, 130000, 78125, 92817, 4080, 20, 13, 21, 23, 11334, 77002, 85403, 125678, 54330, 47985, 10764, 250, 246, 125118, 92192, 134828, 13146, 624, 771, 28447, 51588, 26698, 20, 25, 54825, 33883, 220, 23, 128514, 32985, 98, 48606, 17380, 16560, 48408, 84667, 28002, 144394, 124685, 43590, 29326, 19391, 12802, 92031, 20401, 30520, 120, 131131, 129439, 31079, 73077, 28002, 130137, 24897, 80573, 129676, 134748, 51391, 12802, 126781, 56983, 48606, 5140, 41902, 144687, 18411, 53900, 57026, 135392, 137351, 80901, 64850, 19391, 3315, 109, 226, 130109, 83277, 220, 18, 18, 59761, 20401, 43115, 20487, 64850, 56475, 659, 17, 22, 15, 17877, 95170, 125166, 134002, 13, 220, 16, 23, 24, 17, 126216, 48408, 84667, 28002, 144394, 124685, 43590, 29326, 19391, 12802, 70179, 62618, 60716, 85057, 130357, 73077, 28002, 130137, 24897, 16560, 220, 16, 17, 59761, 20401, 10764, 39834, 66136, 135607, 139287, 56983, 48606, 17380, 10764, 251, 94, 23259, 133381, 13, 32985, 98, 48606, 17380, 16560, 62107, 66019, 63332, 88781, 129296, 23259, 135979, 44518, 144393, 17877, 93721, 126063, 33509, 60315, 65553, 97, 29346, 20136, 116, 125120, 12802, 70943, 126641, 20401, 44518, 144393, 19391, 129423, 129502, 42039, 16235, 226, 79632, 52300, 94315, 36055, 131005, 80968, 220, 18, 126746, 23259, 135979, 129882, 21329, 134002, 13, 20136, 116, 125120, 20401, 129423, 129502, 136646, 32985, 98, 48606, 17380, 16560, 66136, 135607, 139287, 56983, 48606, 20401, 81173, 34395, 20401, 27767, 57160, 230, 105, 74361, 222, 130427, 97143, 31079, 77353, 126299, 128533, 220, 24, 126216, 126322, 126246, 659, 18, 17, 15, 130408, 17877, 74361, 222, 88259, 126204, 11, 55838, 251, 126333, 53680, 220, 19, 88259, 19391, 124970, 23573, 36330, 250, 126746, 56475, 220, 17, 42044, 129835, 56983, 48606, 18411, 23084, 142875, 34395, 11, 220, 19, 18, 21, 59761, 20401, 129392, 126746, 18411, 53900, 124982, 127378, 11, 659, 19, 21, 21, 20401, 134748, 43115, 28754, 36330, 250, 126746, 132841, 33704, 10764, 72509, 29346, 37064, 234, 28002, 134929, 24897, 80573, 47665, 254, 12802, 131196, 5140, 96, 101, 24897, 62107, 20401, 5140, 240, 97, 17380, 22042, 222, 125476, 5140, 252, 255, 144120, 19391, 129901, 141438, 13, 32985, 98, 48606, 17380, 16560, 74808, 130472, 12802, 18411, 47911, 100, 57801, 3315, 98, 238, 34395, 126310, 126614, 23259, 20401, 47665, 242, 80901, 80573, 129676, 141767, 145008, 82190, 134748, 10764, 252, 246, 17877, 129423, 43590, 29326, 144022, 126322, 67511, 17877, 81718, 136892, 125199, 33509, 60315, 54825, 16560, 127218, 12802, 129093, 16186, 125615, 134006, 45130, 111, 56475, 130037, 55902, 125466, 17877, 5140, 228, 241, 17877, 28733, 137046, 13, 54825, 16560, 130005, 77596, 235, 93701, 23259, 45710, 19391, 141231, 133995, 13146, 382, 37064, 234, 28002, 10764, 7492, 60294, 11, 65510, 3315, 120, 230, 28002, 11, 10764, 250, 112, 12802, 62071, 144262, 24897, 11, 37064, 234, 79004, 28626, 71015, 142793, 128187, 11, 79207, 44680, 233, 108, 131196, 5140, 116, 234, 17380, 89235, 11, 125674, 12802, 89235, 32985, 98, 125544, 132557, 53680, 143241, 226, 5140, 116, 234, 126746, 26698, 24897, 131050, 10764, 39834, 53680, 129676, 129624, 52959, 127284, 125199, 34395, 94315, 126641, 20401, 125674, 128514, 19391, 32985, 98, 48606, 17380, 19391, 124970, 82190, 35509, 144090, 126429, 26699, 52300, 140094, 20401, 129296, 23259, 134771, 129676, 20136, 116, 125120, 20401, 73077, 28002, 130137, 24897, 16560, 220, 18, 59761, 20401, 77353, 126299, 80968, 10764, 236, 246, 144413, 28626, 7, 16, 23, 24, 19, 93, 24, 21, 8, 17877, 124657, 130766, 126204, 220, 17, 42044, 129835, 220, 17, 80901, 7, 16, 23, 24, 22, 93, 24, 23, 8, 18411, 53900, 139836, 13, 30520, 120, 131131, 129439, 31079, 56475, 3315, 229, 57160, 229, 112, 23573, 92751, 127906, 64850, 19391, 135878, 134312, 120, 125512, 12802, 16778, 226, 73077, 28002, 130137, 24897, 20401, 126291, 125522, 25715, 10764, 245, 101, 28002, 129238, 125932, 47324, 53189, 16560, 220, 16, 23, 24, 24, 126216, 134748, 127840, 58677, 54321, 129125, 5140, 116, 234, 126746, 134644, 129807, 28733, 136275, 130271, 24897, 17380, 23084, 80968, 135298, 125476, 34395, 53900, 124982, 33509, 60315, 32985, 98, 48606, 17380, 80573, 54825, 20401, 90711, 250, 88259, 71015, 142793, 128187, 33704, 63332, 34395, 19391, 126352, 63089, 82190, 30520, 120, 131131, 129439, 31079, 56475, 54825, 129360, 55673, 20401, 18411, 135797, 23573, 73986, 21329, 83036, 24897, 20401, 23084, 132524, 17877, 48364, 255, 88259, 134002, 13, 129238, 125932, 47324, 53189, 16560, 140886, 128186, 33883, 54825, 129125, 137767, 116, 126251, 57801, 53900, 124982, 34395, 220, 17, 21, 41429, 20401, 32985, 98, 48606, 17380, 16560, 73077, 28002, 130137, 24897, 18411, 220, 23, 21, 79207, 117, 220, 21, 17, 45104, 101, 20401, 54116, 49664, 53680, 5140, 228, 222, 50340, 93672, 220, 19, 80901, 74884, 226, 63256, 17380, 129423, 129502, 126063, 34395, 11, 20136, 116, 125120, 20401, 220, 16, 80901, 18411, 61298, 28733, 136275, 130271, 24897, 19391, 220, 16, 20, 59761, 20401, 43115, 20487, 64850, 5140, 240, 97, 17380, 22042, 222, 134828, 13146, 382, 57268, 28002, 130137, 24897, 16560, 125640, 13146, 82965, 73986, 131529, 12802, 129900, 12802, 144140, 131344, 129423, 129502, 17877, 127154, 21329, 133995, 133099, 140084, 126720, 33883, 144107, 17877, 128900, 13, 220, 23, 128514, 94315, 130237, 19391, 32985, 98, 48606, 17380, 20401, 85403, 31328, 51391, 28002, 19969, 62099, 95, 31079, 85251, 32985, 117, 40853, 42039, 32129, 130472, 140026, 79207, 105, 136275, 16186, 125615, 32985, 98, 48606, 17380, 16560, 220, 24, 128514, 20401, 138313, 18411, 5140, 228, 241, 144172, 13146, 13, 220, 16, 24, 15, 15, 126216, 66136, 135607, 139287, 56983, 48606, 19969, 220, 23, 59761, 20401, 10764, 39834, 64850, 17380, 53989, 226, 31079, 64850, 53618, 73077, 28002, 130137, 24897, 16560, 60716, 85057, 133245, 34395, 5140, 116, 234, 126746, 134644, 129807, 42039, 63332, 34395, 18411, 131170, 126352, 63089, 23573, 94315, 32985, 98, 48606, 17380, 16560, 71015, 142793, 128187, 53680, 129676, 125674, 31328, 28626, 126746, 12802, 24897, 90711, 112, 89235, 139287, 24897, 17380, 45104, 242, 134828, 13146, 13, 54825, 20401, 94203, 125535, 42039, 126558, 95617, 70582, 20401, 65510, 126524, 12802, 5140, 244, 101, 31079, 128732, 53618, 72553, 35509, 16560, 5140, 41902, 126322, 20401, 52300, 54825, 16560, 23872, 120, 88259, 136608, 55902, 5140, 228, 240, 33704, 77353, 135402, 220, 16, 72553, 34143, 105, 60294, 18411, 133621, 94203, 125535, 17877, 32985, 118, 125761, 382, 16, 24, 15, 16, 126216, 32985, 98, 48606, 17380, 16560, 129423, 129502, 53680, 22042, 112, 134014, 128187, 20401, 134583, 48408, 84667, 28002, 144394, 56983, 48606, 56475, 30520, 120, 131131, 129439, 31079, 20401, 84255, 136499, 125625, 12802, 134133, 20401, 136605, 126291, 125522, 25715, 135979, 73077, 28002, 130137, 24897, 19391, 137844, 141438, 13, 54825, 44518, 144393, 53680, 126844, 44518, 144393, 17877, 142196, 54825, 80573, 134014, 128187, 33704, 5140, 223, 232, 93701, 131702, 126488, 56290, 18411, 83556, 33509, 136387, 94315, 20487, 20401, 79207, 113, 124780, 128552, 32985, 98, 48606, 17380, 80573, 127218, 129360, 64577, 54330, 83556, 31079, 126588, 127041, 120, 131833, 64850, 56475, 54825, 20401, 28927, 105, 129382, 129125, 128677, 54321, 126063, 34395, 11, 40771, 112, 40853, 33704, 130005, 10764, 39834, 20401, 126291, 125522, 56475, 32985, 98, 48606, 17380, 20401, 23084, 132524, 19391, 134015, 134002, 13, 134014, 128187, 33704, 136724, 220, 16, 24, 15, 17, 126216, 220, 22, 128514, 32985, 98, 48606, 17380, 18411, 36055, 125545, 29326, 144264, 34395, 54825, 98869, 31328, 28626, 56475, 46319, 138789, 24897, 60294, 93672, 129423, 129502, 12802, 5140, 231, 112, 138081, 64577, 12802, 129709, 142852, 20401, 129296, 23259, 23894, 116, 129423, 129502, 42039, 66136, 135607, 139287, 56983, 48606, 17380, 131170, 5140, 249, 108, 31079, 130137, 144110, 13146, 13, 64577, 12802, 129709, 142852, 80573, 54825, 20401, 48364, 104, 126174, 80901, 64850, 70943, 20401, 125703, 16560, 64577, 12802, 129709, 142852, 20401, 126291, 125522, 25715, 23872, 97, 29346, 126746, 84255, 28002, 141893, 20401, 142654, 20401, 64850, 19391, 143682, 220, 24, 79632, 20401, 129296, 23259, 129125, 66136, 41671, 95218, 16560, 128584, 125761, 13, 32985, 98, 48606, 17380, 16560, 10764, 230, 105, 23259, 65510, 32985, 98, 20487, 83036, 131131, 11, 98869, 23259, 71015, 126781, 5140, 116, 234, 126673, 134133, 127085, 136770, 53680, 220, 16, 126746, 23259, 143241, 226, 32985, 98, 145978, 17877, 133970, 23573, 220, 21, 79632, 20401, 127840, 129296, 23259, 129125, 32129, 130898, 53900, 139836, 13, 64577, 12802, 129709, 142852, 16560, 54825, 44518, 144393, 56475, 140887, 42039, 74884, 242, 33509, 60315, 220, 16, 24, 15, 18, 126216, 220, 17, 80901, 17380, 38523, 105, 144110, 127378, 32985, 98, 48606, 17380, 20401, 85403, 136854, 125834, 23573, 125149, 144769, 12802, 136724, 54825, 33883, 19391, 5140, 112, 226, 10764, 249, 230, 125747, 126322, 126246, 19391, 126440, 88259, 125511, 73669, 128808, 133245, 31079, 47985, 141541, 128552, 129296, 23259, 135979, 134748, 43115, 135818, 5140, 223, 251, 124050, 19946, 13, 151645, 198, 151644, 872, 198, 59761, 127559, 65865, 56475, 220, 18, 126246, 125166, 220, 17, 125086, 138913, 54116, 49664, 96137, 45104, 101, 23573, 129296, 23259, 16560, 30, 151645, 198, 151644, 77091, 198, 59761, 127559, 65865, 56475, 220, 18, 126246, 125166, 220, 17, 125086, 138913, 54116, 49664, 96137, 45104, 101, 23573, 129296, 23259, 16560, 136541, 125166, 125544, 125674, 12802, 63089, 5140, 45881, 12802, 130000, 24897, 20401, 74884, 222, 141571, 12802, 10764, 252, 230, 124419, 52959, 126896, 78952, 13, 54825, 16560, 73523, 127559, 65865, 129296, 126835, 17380, 77002, 129382, 33883, 220, 20, 12802, 144262, 128878, 127042, 125511, 28626, 17380, 81058, 126063, 125590, 220, 22, 61741, 19391, 220, 17, 126333, 142786, 125120, 17877, 10764, 245, 230, 26699, 132537, 127864, 65865, 64795, 125580, 34395, 11, 74361, 222, 125519, 136448, 220, 16, 138913, 125686, 95218, 16560, 5140, 41902, 54825, 133847, 220, 18, 126246, 125166, 220, 17, 125086, 138913, 54116, 49664, 82190, 45104, 101, 65865, 10764, 230, 105, 135444, 97143, 127750, 4318, 1097, 17, 22099, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1b60e62f-3e55-473a-b00f-479316746cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 59761, 127559, 65865, 56475, 220, 18, 126246, 125166, 220, 17, 125086, 138913, 54116, 49664, 96137, 45104, 101, 23573, 129296, 23259, 16560, 136541, 125166, 125544, 125674, 12802, 63089, 5140, 45881, 12802, 130000, 24897, 20401, 74884, 222, 141571, 12802, 10764, 252, 230, 124419, 52959, 126896, 78952, 13, 54825, 16560, 73523, 127559, 65865, 129296, 126835, 17380, 77002, 129382, 33883, 220, 20, 12802, 144262, 128878, 127042, 125511, 28626, 17380, 81058, 126063, 125590, 220, 22, 61741, 19391, 220, 17, 126333, 142786, 125120, 17877, 10764, 245, 230, 26699, 132537, 127864, 65865, 64795, 125580, 34395, 11, 74361, 222, 125519, 136448, 220, 16, 138913, 125686, 95218, 16560, 5140, 41902, 54825, 133847, 220, 18, 126246, 125166, 220, 17, 125086, 138913, 54116, 49664, 82190, 45104, 101, 65865, 10764, 230, 105, 135444, 97143, 127750, 4318, 1097, 17, 22099, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "df99cbe7-fd91-4f8b-9d8a-1b7c37658393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 8192  # 모델과 데이터셋 패킹을 위한 최대 시퀀스 길이\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2265381a-336f-49f3-96ec-014274809ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/282 15:43 < 13:37, 0.16 it/s, Epoch 1.61/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.388700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.353900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# 모델이 자동으로 허브와 output_dir에 저장됨\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()   \u001b[38;5;66;03m# 최종 모델을 저장\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 434\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7015ff04-7da9-4a17-9fc5-0144c4438d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f830572a-239e-45f8-ab8d-e60e2bf7506b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c8e083a8-fa21-4d5a-b413-cd3f4ef1d341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4107\n",
      "drwxrwxrwx 4 root root 2011095 Nov  2 13:43 \u001b[0m\u001b[34;42m.\u001b[0m/\n",
      "drwxr-xr-x 1 root root     124 Nov  2 11:47 \u001b[01;34m..\u001b[0m/\n",
      "drwxrwxrwx 2 root root    7200 Nov  2 11:48 \u001b[34;42m.ipynb_checkpoints\u001b[0m/\n",
      "-rw-rw-rw- 1 root root  175511 Nov  2 13:43 Untitled.ipynb\n",
      "drwxrwxrwx 6 root root 2011078 Nov  2 13:42 \u001b[34;42mqwen2-7b-rag-ko\u001b[0m/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3a5c3094-ca53-4234-b031-94a07abf84a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-02 13:45:09--  https://raw.githubusercontent.com/ukairia777/LLM-Finetuning-tutorial/main/merge.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "Length: 1351 (1.3K) [text/plain]\n",
      "Saving to: ‘merge.py’\n",
      "\n",
      "merge.py            100%[===================>]   1.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-02 13:45:09 (113 MB/s) - ‘merge.py’ saved [1351/1351]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ukairia777/LLM-Finetuning-tutorial/main/merge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "24e78388-e0b4-4901-971b-35b3e1d086da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcaf37d24614fe2b6427f59d534a895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-150\"\n",
    " \n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "# fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "# get token id for end of conversation\n",
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4cc87825-56b8-47ba-9128-a9b9cc01b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-------------------------\n",
      "문서1: 참신하고 새로운 아이디어로 무장한 관광기업들을 대상으로 한 크라우드 펀딩이 점차 활기를 띠고 있다. 크라우드 펀딩은 자금이 필요한 기업이 온라인 플랫폼을 기반으로 불특정 다수로부터 투자를 받는 방식으로, 문화체육관광부(장관 박양우)와 한국관광공사(사장 안영배)에서는 2017년부터 관광기업을 대상으로 ‘관광 크라우드 펀딩 지원사업’을 추진하고 있다. 올해엔 등록 프로젝트 91개 중 82개가 펀딩에 성공해 총 26억 원의 투자금을 유치하는 성과를 거뒀다. 이 실적은 75개 프로젝트 중 53개가 성공해 총 12억 원을 확보한 전년에 비해 참여 프로젝트 수 21.3%, 펀딩성공률 19.5%p, 총 투자액 2배 이상이 상승한 것이다. 한편 문체부와 공사는 올해 탁월한 성과를 거둔 8개 우수기업을 선정, 12월 23일(월) 서울 서촌창작소에서 시상식을 가졌다. 2019년 대상은 ‘스테이폴리오(대표 이상묵)’가 차지했다. 스테이폴리오는 서울 서촌지역 한옥 재생 프로젝트 ‘서촌유희’ 운영을 위한 증권형* 크라우드 펀딩을 등록한 후 열흘이 채 되지 않는 기간 동안 목표금액 1억 원을 달성할 정도로 투자자들의 이목을 집중시켰고, 내년도에도 2차 펀딩을 진행할 계획이다. ‘머무는 것 자체가 여행이 된다’는 가치관으로 국내외 좋은 공간을 지역주민과 함께 발굴‧조성‧소개하는 스테이폴리오는 지난 19일 있었던 관광벤처의 날에서도 ‘일자리 창출’ 부문 최우수기업으로 선정됐다. * 증권형 크라우드 펀딩: 주식투자와 비슷한 개념으로 투자자는 펀딩 기업의 미래 가치를 고려하여 투자하고, 이에 대한 보상으로 기업의 지분 또는 이익을 배당받는 형식 * 후원형 크라우드 펀딩: 투자자는 기업 프로젝트를 대상으로 후원(투자)하고, 이에 대한 보상으로 기업의 상품 또는 서비스 등을 이용하는 형식 금상엔 테마, 여행지, 콘텐츠 등 각 분야의 여행전문가를 발굴해 여행창작 활동을 지원하고 이들과 여행자를 이어주는 여행매니지먼트 기업인 ‘여행상점(대표 윤형식)’이 선정됐다. 특히 여행전문가가 기획하고 동행하는 차별화된 여행상품을 제공하는 여행상점은 증권형 펀딩으로 약 9,800만 원을 유치했다. 이밖에 은상은 △펭귄오션레저(해양레저장비 개발 및 프로그램 운영) △베드라디오(제주도 여관 리모델링해 로컬 경험 제공), 장려상은 △모션디바이스(VR엔터테인먼트 전문) △지피지(페스티벌과 경주 관광지 연계 패키지 티켓 판매) △볼베어파크(실내 스포츠 테마파크 운영) △한국투어패스(강원투어패스 운영)가 받았다. 공사 안덕수 관광일자리실장은 \"올해 프로젝트 수, 펀딩성공률, 투자유치액 등 여러 면에서 이제 관광분야에서도 크라우드 펀딩이 정착했다는 점이 고무적“이라며, ”우수한 관광기업들의 원활한 자금조달과 지속 성장을 위해 내년에는 더욱 다양한 사업을 추진할 것“이라 밝혔다.\n",
      "-------------------------\n",
      "문서2: 중국 국책펀드인 차이나미디어캐피털(CMC)이 한국 영화에 800억원을 투자한다. 세계 최대 전자상거래 업체인 중국 알리바바가 지난달 한국 영화에 1000억원을 투자하기로 한 데 이어 국책펀드의 자금이 유입되는 것이다. 중국 민·관(民·官)의 ‘큰손’들이 이처럼 한국 영화와 게임 등에 잇따라 투자, 한국 콘텐츠산업에 ‘차이나 머니’ 돌풍이 일 조짐이다. ▶관련기사 A5면국영 중국개발은행이 핵심 출자자로 참여한 CMC는 9일 국내 벤처캐피털인 이상기술투자가 모집하는 2000억원 규모의 영화콘텐츠펀드에 800억원을 출자하기로 했다. CMC는 중국 국가발전개혁위원회가 미디어·엔터테인먼트산업 육성을 목표로 2010년 42억위안(약 7000억원) 규모로 설립한 중국 최대 국책펀드다.정유신 한국벤처투자 대표는 “지난 7월 시진핑 중국 국가주석이 방한했을 때 ‘양국 합작영화는 중국 내 외국 영화 상영 제한의 대상에서 제외한다’고 합의한 후 중국 자본의 한국 영화산업 진출 속도가 빨라지고 있다”고 설명했다. 한국의 콘텐츠 제작 능력과 중국의 자본이 협력, 윈윈할 수 있는 시스템이 가동되기 시작했다는 것이다. 박동휘 기자 donghuip@hankyung.com\n",
      "-------------------------\n",
      "문서3: 에쓰오일의 대주주인 사우디아라비아 아람코의 한국 투자가 양국 경제 교류의 성공 모델로 꼽혀 화제다.중동을 순방 중인 박근혜 대통령은 3일(현지시간) 사우디 리야드 에르가궁에서 열린 살만 빈 압둘아지즈 국왕과의 정상회담에서 “사우디 아람코가 대주주인 에쓰오일의 울산공장 증설 투자는 양국 간 성공 사례”라고 소개했다.사우디 국영 석유회사인 아람코가 지분 63.4%를 보유 중인 에쓰오일은 5조원가량을 투자해 울산 온산공단에 중질유 분해시설과 프로필렌 등 석유화학제품 생산공장을 건설할 계획이다. 하지만 사업 추진 과정에서 에쓰오일의 증설 계획은 무산될 뻔했다. 부지를 확보하지 못해서다. 그러던 차에 2013년 4월 청와대에서 열린 외국인 투자기업 간담회에 참석한 나세르 알마하셔 에쓰오일 사장은 “대규모 투자를 계획하고 있는데 공장 인근에 마땅한 부지가 없어 어려움이 많다”고 토로했다. 이에 박 대통령은 배석했던 윤상직 산업통상자원부 장관에게 “잘 해결되도록 아이디어를 내달라”고 지시했다.윤 장관은 발빠르게 움직였다. 에쓰오일 온산공장 바로 옆에 있는 한국석유공사 울산 비축기지의 부지 일부를 매각하기로 했다. 에쓰오일은 지난해 4월 92만㎡를 5190억원에 낙찰받았다. 에쓰오일 관계자는 “올초 착공에 들어가 2017년 완공할 예정”이라고 말했다.박 대통령 지원 덕에 증설 부지를 마련한 에쓰오일은 동북아 오일허브 울산북항사업에 참여하는 것으로 화답했다. 동북아 오일허브는 여수와 울산에 총 3660배럴 규모의 상업용 저장시설을 건설하는 사업이다. 박 대통령의 발언에 한껏 고무된 알마하셔 사장은 “아람코의 대규모 한국 투자가 성공할 수 있도록 최선을 다하자”며 임직원을 격려했다.\n",
      "-------------------------\n",
      "문서4: 포인트모바일은 지난 16일과 17일 양일간 국내외 기관투자자를 대상으로 진행한 수요예측 결과, 공모가를 희망밴드 (13,000~15,000원) 상단인 15,000원에 최종 확정했다고 19일 밝혔다. 이번 수요예측에는 국내외 1,508개 기관이 참여해 1,447.07대 1의 경쟁률을 기록했다. 이는 올해 진행된 코스닥 상장사의 수요예측 중 카카오게임즈(293490)(1,478.53대 1) 다음으로 두 번째 높은 수치이다. 상장을 주관한 하나금융투자 관계자는 “실제 수요예측 참여 기관 99.95% (미제시 0.90% 포함)가 공모밴드 상단 이상으로 가격을 제시하는 등 포인트모바일의 성장 계획과 비전에 신뢰를 보냈다”며 “다수의 ODM/OEM 사업 경험을 기반으로 업계 선도 기술력을 갖춰 글로벌 고객을 보유한 점이 공모 흥행 요인”이라고 말했다. 실제로 포인트모바일은 세계 최대 온라인 유통업체인 ‘아마존’과 유럽 슈퍼마켓 시장 점유율 1위 ‘알디’ 등 메이저 고객사를 확보하면서 가파른 매출 성장의 기대감을 높이고 있다. 이를 통해 글로벌 시장에 대한 성장 잠재력과 기업 가치를 입증했다. 이번 공모를 통해 회사에 유입되는 자금은 총 164억 원이며 확보 자금은 글로벌 고객사의 발주대응능력을 확보하기 위한 운영자금에 주로 사용되고, 시설자금과 신제품 개발을 위한 연구개발비에 추가로 쓰일 계획이다. 포인트모바일 강삼권 대표이사는 “수요예측 기간동안 포인트모바일의 다양한 제품 포트폴리오, 고객 맞춤형 토탈 솔루션 등 차별화된 경쟁력을 긍정적으로 평가해주신 투자자분들께 감사드린다”며 “상장 후에도 인정받은 기술력과 탁월한 경쟁력으로 4차산업의 새로운 미래를 창조해나가는 기업으로 거듭나겠다”고 밝혔다. 한편, 포인트모바일은 오는 23일~24일 일반투자자 대상 청약을 거쳐 12월 3일 코스닥 시장에 입성할 계획이다.\n",
      "-------------------------\n",
      "문서5: 중국의 최대 전자상거래업체 알리바바와 최대 온라인 게임업체 텅쉰, 중국 2위 보험사 핑안보험그룹이 중국 최대 영화제작사인 화이슝디(華誼兄弟)에 공동투자했다. 이들 세 회사가 함께 화이슝디 투자에 나선 것은 △중국 인터넷플랫폼 업체들의 콘텐츠사업 강화 △게임과 영화산업의 연계 △중국 자본의 해외 문화산업 투자 확대라는 중국 문화산업의 세 가지 추세를 집약적으로 보여준다는 분석이다.선전증권거래소 창업판(중국판 코스닥)에 상장된 화이슝디는 18일 저녁 36억위안(약 6474억원) 규모의 3자배정 유상증자에 이들 3개사가 참여해 35억위안을 투자했다고 공시했다. 중국 언론은 19일 알리바바의 창업자 마윈(馬雲) 회장과 텅쉰 창업자인 마화텅(馬化騰) 회장, 핑안보험의 마밍저(馬明哲) 회장의 성(姓)을 따 “3마(馬)가 함께 투자했다”고 보도했다.이들의 증자 참여 소식이 전해진 19일 화이슝디 주가는 가격제한폭인 10%까지 급등하며 상한가(28.85위안)로 치솟았다. ‘3마’는 작년 초에도 종안온라인재산보험이라는 인터넷 보험회사를 공동 설립했다. 당시 중국 언론은 ‘3馬의 첫 번째 도원결의(桃園結義)’라고 평가했다.화이슝디는 이번에 유치한 자금을 영화와 드라마 제작 및 은행대출 상환 등에 사용한다고 밝혔다. 이와 함께 알리바바 및 텅쉰과 각각 전략적 제휴를 맺은 사실도 함께 공시했다. 화이슝디는 타오바오 등 알리바바의 다양한 인터넷플랫폼을 통해 영화를 유통시키기로 했다. 텅쉰과는 게임을 영화화하거나 영화를 게임으로 개발하는 사업을 추진하기로 했다. 알리바바와 텅쉰은 화이슝디와 각각 5편의 영화를 공동 제작하기로 합의했다. 핑안보험그룹은 화이슝디가 해외 영화사 인수합병에 나설 때 자금줄 역할을 할 것이라고 중국 언론이 전했다.<|im_end|>\n",
      "<|im_start|>user\n",
      "중국 자본이 한국 콘텐츠 산업에 미치는 영향은 무엇인가요?<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터\n",
    "text = tokenizer.apply_chat_template(\n",
    "    test_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "text = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "71c2632e-344f-47bd-a3a8-1c88067686de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7b3078bc-78d4-4657-adff-acefd91e82d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008ecc5-4b61-4535-b914-dff578fd63a0",
   "metadata": {},
   "source": [
    "## 기본 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "94339b54-ca78-424c-b992-c1ddfb27b16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "구석기 시대와 철기 시대의 도구 제작 방식에는 여러 가지 차이가 있습니다.\n",
      "\n",
      "구석기 시대의 도구 제작 방식은 주로 간접떼기 방식을 사용했습니다. 이 방식은 격지나 돌날을 이용해 도구를 만드는 것으로, 주먹도끼보다 훨씬 작고 더 세밀한 도구를 만들 수 있었습니다. 슴베찌르개는 이 시대의 대표적인 도구로, 자루를 달아서 짐승을 사냥하는 창이나 전쟁 무기 또는 가죽에 구멍을 뚫는 연장 등으로 사용되었습니다. 슴베찌르개는 자루를 달아서 사용하는 것이 특징이며, 양 옆의 마름모꼴 모서리 주변에 잔손질을 해 위쪽 끝이 날카로워 사용하기에 적합했습니다.\n",
      "\n",
      "반면에 철기 시대의 도구 제작 방식은 주로 직접떼기 방식을 사용하였습니다. 이 방식은 석기를 직접 떼어 사용하는 방식으로, 전기에는 외날찍개와 같이 직접떼기로 한 면만 떼어 낸 석기, 중기에는 주먹도끼와 같이 두 면을 떼어 낸 쌍날찍개가 사용되었습니다. 철기 시대의 도구 제작 방식은 더 복잡하고 세밀한 작업을 필요로 하며, 이로 인해 더욱 효율적이고 효과적인 도구를 만들 수 있었습니다.\n",
      "\n",
      "따라서 구석기 시대와 철기 시대의 도구 제작 방식의 주요 차이점은 제작 방식과 도구의 복잡성 및 효율성에 있습니다.\n",
      "    label:\n",
      "\n",
      "구석기 시대와 철기 시대의 도구 제작 방식에는 여러 가지 차이가 있습니다.\n",
      "\n",
      "구석기 시대의 도구는 주로 돌을 깨뜨려 사용하기 편리하도록 만든 뗀석기(타제석기)였습니다. 초기 구석기 시대에는 외날찍개와 같이 한 면만 떼어 낸 석기가 사용되었고, 중기에는 주먹도끼와 같이 두 면을 떼어 낸 쌍날찍개가 사용되었습니다. 후기 구석기 시대에는 격지나 돌날의 양쪽을 단단한 뼈나 뿔로 눌러 떼어 도구를 만드는 간접떼기 방식이 사용되었습니다. 대표적인 도구로는 슴베찌르개가 있으며, 이는 자루를 달아서 사냥이나 전쟁 무기 등으로 사용되었습니다 [[ref1]].\n",
      "\n",
      "반면 철기 시대에는 금속을 사용한 도구와 무기가 제작되었습니다. 예를 들어, 철기 시대의 동검은 칼과 손잡이, 칼끝 장식이 별개로 만들어졌으며, 칼자루 장신구와 칼집부속구 등 다양한 부속품이 함께 사용되었습니다. 이러한 도구들은 주로 청동이나 철을 녹여서 주조하거나 단조하는 방식으로 제작되었습니다 [[ref2]].\n",
      "\n",
      "따라서 구석기 시대의 도구 제작 방식은 주로 돌을 깨뜨려 형태를 만드는 타제석기 방식이었고, 철기 시대에는 금속을 녹여서 주조하거나 단조하는 방식으로 도구를 제작하는 차이가 있었습니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "제3차 중동 전쟁 또는 6일 전쟁은 1967년에 이스라엘과 주변 국가인 이집트, 요르단, 시리아, 레바논 사이에서 벌어진 전쟁입니다. 이 전쟁의 주요 원인은 이스라엘과 주변 국가 사이의 긴장과 불안정한 관계였습니다. 이스라엘은 1956년 수에즈 위기 때 주목할 만한 목표 중 하나였던 티란 해협을 확보하려고 노력했습니다. 이 지역은 1950년 이래 이집트가 이스라엘의 항해를 봉쇄하고 있었습니다. 이스라엘은 이 지역의 봉쇄를 중단하려고 했지만, 이에 대한 보장이 없었습니다. \n",
      "\n",
      "1967년 6월까지 이스라엘과 주변국 사이의 긴장은 높아졌습니다. 이스라엘은 티란 해협에 대한 봉쇄가 전쟁의 명분이 될 것이라고 주장했습니다. 이집트는 이스라엘 선박에 대해 해협들이 봉쇄될 것이라고 선언했습니다. 이에 대응하여, 6월 5일 이스라엘은 이집트 공군기지에 예방적 공습을 가했습니다. 이러한 일련의 사건들은 제3차 중동 전쟁에 대한 논란 중 하나가 되었습니다.\n",
      "\n",
      "전쟁의 결과로, 이스라엘은 기습 공격과 혁신적이고 잘 수행된 전투 계획, 그리고 아랍 연합군의 낮은 전투 실력과 지도력이 원인이 되어 군사적 승리를 거두었습니다. 이스라엘은 이집트로부터 가자 지구와 시나이 반도를, 요르단으로부터 동예루살렘과 요르단 강 서안 지구를, 시리아로부터 골란 고원을 획득했습니다. 이 전쟁으로 인해 100,000명의 시리아인이 골란 고원을 떠났고, 300,000명의 팔레스타인인이 서안 지구를 떠났는데 이들은 모두 난민이 되었습니다.\n",
      "\n",
      "아랍의 겨울(Arab Winter)은 2011년 아랍의 봄 이후 아랍 일대에서 벌어지고 있는 광범위한 혼란과 폭력과 사회불안정 사태를 가리키는 말입니다. 이는 시리아 내전, 이라크 내전, 이집트 위기, 예멘 내전 등이 포함됩니다. 이 겨울은 권위주의가 복고되고 자유 민권이 억압되고 있으며, 이슬람 극단주의와 테러 단체가 곳곳에서 준동하고 있습니다. 아랍의 겨울은 복수의 지역에서 내란과 내전이 발생하고, 사회가 불안정해지며, 아랍 지역의 경제와 인구가 쇠퇴하고 인종주의와 종교적 종파주의가 판을 치는 것으로 특징지어집니다. 이 겨울의 가장 극단적 사례는 시리아이며, 바샤르 알 아사드 대통령의 독재에 반대하여 일어난 시위에 이슬람주의 집단이 개입하고, 자유 시리아군의 부패와 범죄가 밝혀지면서 이슬람주의자들의 입지가 강화되었습니다. 이로 인해 시리아는 독재정부와 반정부 반군과 이슬람주의 테러리스트들의 삼파내전에 휩싸이게 되었습니다.\n",
      "    label:\n",
      "\n",
      "제3차 중동 전쟁과 아랍의 겨울의 주요 원인과 결과는 다음과 같습니다.\n",
      "\n",
      "### 제3차 중동 전쟁 (6일 전쟁)\n",
      "**주요 원인:**\n",
      "1. **티란 해협 봉쇄:** 이집트가 이스라엘 선박의 티란 해협 통과를 봉쇄하면서 긴장이 고조되었습니다. 이스라엘은 이를 전쟁의 명분으로 삼았습니다[[ref1]].\n",
      "2. **군사적 준비:** 이집트는 이스라엘과의 국경에 군을 배치하고, 이스라엘은 이에 대응해 예방적 공습을 감행했습니다[[ref1]].\n",
      "\n",
      "**주요 결과:**\n",
      "1. **영토 확장:** 이스라엘은 가자 지구, 시나이 반도, 동예루살렘, 요르단 강 서안 지구, 골란 고원을 점령하여 영토를 3배로 확장했습니다[[ref1]].\n",
      "2. **난민 발생:** 전쟁으로 인해 약 100,000명의 시리아인과 300,000명의 팔레스타인인이 난민이 되었습니다[[ref1]].\n",
      "3. **군사적 자만:** 이스라엘의 승리는 군사적 자만을 초래하여 이후 1973년 욤키푸르 전쟁에서 아랍 연합군의 초기 승리를 가능하게 했습니다[[ref1]].\n",
      "\n",
      "### 아랍의 겨울\n",
      "**주요 원인:**\n",
      "1. **아랍의 봄의 실패:** 2011년 아랍의 봄 이후 민주화와 개혁을 목표로 한 시도들이 실패하면서 권위주의가 복고되고, 사회적 불안정이 증가했습니다[[ref4]].\n",
      "2. **이슬람 극단주의의 부상:** 이슬람 극단주의와 테러 단체들이 활동을 강화하면서 지역 내 폭력과 혼란이 가중되었습니다[[ref4]].\n",
      "\n",
      "**주요 결과:**\n",
      "1. **내전과 사회 불안정:** 시리아 내전, 이라크 내전, 예멘 내전 등 여러 지역에서 내전과 사회 불안정이 발생했습니다[[ref4]].\n",
      "2. **경제적 손실:** 아랍의 겨울로 인해 발생한 사회적 손실비용은 약 8천억 달러에 달하며, 수백만 명의 난민이 발생했습니다[[ref4]].\n",
      "3. **인도적 위기:** 시리아, 이집트, 이라크 등 여러 지역에서 인도적 지원이 필요한 사람들의 수가 급증했습니다[[ref4]].\n",
      "\n",
      "이와 같이 제3차 중동 전쟁과 아랍의 겨울은 각각 중동 지역의 군사적, 정치적, 사회적 변화를 초래한 중요한 사건들입니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "한국과 미국의 핀테크 산업 경쟁력의 차이는 여러 가지 요인으로 인해 발생합니다. \n",
      "\n",
      "첫째, 규제 차이가 큰 원인 중 하나입니다. 한국의 과도한 금융규제가 국내 핀테크산업 발전을 가로막고 있다는 지적이 있습니다. 박소영 한국핀테크포럼 의장은 미국 금융전문지 '아메리칸뱅커'와 'BAI'의 '전 세계 100대 핀테크 기업 순위'를 분석한 결과를 통해 국내 핀테크산업 경쟁력이 글로벌 수준에 한참 못 미친다고 지적했습니다. 특히, 대부업법으로 인한 P2P 금융의 활성화 부재, 크라우드펀딩 법안의 제한, 개인정보보호법과 전자금융거래법의 문제점 등이 한국의 핀테크 산업 발전을 저해하고 있다고 지적되었습니다.\n",
      "\n",
      "둘째, 미국은 54개의 핀테크 기업을 100대 순위에 포함시켰으며, 이는 미국의 규제 환경이 핀테크 기업의 성장을 촉진하고 있다는 것을 보여줍니다. 반면, 한국 기업은 단 한 곳도 포함되지 않았습니다.\n",
      "\n",
      "셋째, 미국의 핀테크 산업은 다양한 기술과 혁신을 통해 금융 서비스를 개선하고 있으며, 이는 경쟁력을 높여줍니다. 미국의 핀테크 기업들은 AI, 블록체인, 대용량 데이터 분석 등에 기반한 혁신적인 서비스를 제공하고 있습니다.\n",
      "\n",
      "결국, 한국과 미국의 핀테크 산업 경쟁력 차이는 규제, 기술적 혁신, 그리고 시장 환경 등 다양한 요인에 의해 결정됩니다.\n",
      "    label:\n",
      "\n",
      "한국과 미국의 핀테크 산업 경쟁력 차이는 주로 규제의 차이에서 비롯됩니다.\n",
      "\n",
      "한국의 핀테크 산업은 과도한 금융 규제로 인해 발전이 저해되고 있습니다. 예를 들어, 대부업법 때문에 P2P(개인 간 대출) 금융이 활성화되지 못하고, 크라우드펀딩 법안도 투자자에게 많은 제약을 가하고 있습니다. 또한 개인정보보호법과 전자금융거래법도 핀테크 산업의 성장을 방해하는 요소로 작용하고 있습니다. 이러한 규제들은 한국 핀테크 기업들이 글로벌 시장에서 경쟁력을 갖추기 어렵게 만들고 있습니다 [[ref1]].\n",
      "\n",
      "반면, 미국은 핀테크 산업에서 규제 완화와 혁신을 적극적으로 추진하고 있습니다. 미국은 세계 100대 핀테크 기업 중 54개를 차지할 정도로 핀테크 산업에서 강력한 경쟁력을 보이고 있습니다. 이는 미국이 핀테크 기업들이 자유롭게 혁신할 수 있는 환경을 제공하고 있기 때문입니다 [[ref1]].\n",
      "\n",
      "또한, 한국에서는 개인정보 유출 사태 이후 본인 인증 강화 등의 규제가 도입되면서 IT 및 핀테크 산업의 발전에 부정적인 영향을 미치고 있습니다. 반면, 미국은 이러한 규제보다는 정보 유출 방지에 초점을 맞추고 있어 산업 발전에 더 유리한 환경을 조성하고 있습니다 [[ref3]].\n",
      "\n",
      "결론적으로, 한국과 미국의 핀테크 산업 경쟁력 차이는 주로 규제의 엄격함과 혁신 환경의 차이에서 비롯됩니다. 한국은 과도한 규제로 인해 핀테크 산업의 성장이 저해되고 있는 반면, 미국은 규제 완화와 혁신을 통해 핀테크 산업의 경쟁력을 강화하고 있습니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "현대 마케팅 전략에서 소비자 평가는 매우 중요한 역할을 합니다. 이는 소비자가 제품을 구매하기 전에 미래의 사용 경험을 예측하고 판단하는 데 도움을 줍니다. 예를 들어, 온라인 미디어와 커뮤니티, 소셜 네트워크 서비스(SNS) 등에서 특정 상품에 대한 평가를 확인할 수 있습니다. 이런 평가는 소비자가 물건을 사기 전에 제품의 장단점을 세밀하게 파악할 수 있게 해줍니다.\n",
      "\n",
      "이런 평가는 소비자의 '절대가치'에 영향을 미칩니다. '절대가치'는 제품을 실제 사용할 때 경험하게 되는 품질이나 가치를 말합니다. 이는 제품의 실제 성능과 사용자 경험을 반영합니다. \n",
      "\n",
      "예를 들어, 1992년 시몬슨 교수의 실험에서 보듯이, 소비자는 가격 정보에 의존해 상대 평가를 했을 수 있습니다. 그러나 2012년 실험에서는 다양한 정보를 통해 자신이 원하는 가치에 근접한 절대 평가를 한 것입니다. 이는 소비자가 제품에 대한 정보를 투명하게 얻을 수 있는 시대에서, 소비자 평가의 영향력이 증가하고 있다는 것을 보여줍니다.\n",
      "\n",
      "따라서, 현대 마케팅 전략에서는 소비자 평가를 고려해야 합니다. 제품의 품질과 가치를 반영하는 소비자 평가를 통해 제품의 절대가치를 알 수 있습니다. 이를 통해 제품을 개선하고, 소비자에게 더 적합한 제품을 제공할 수 있습니다.\n",
      "    label:\n",
      "\n",
      "현대 마케팅 전략에서 소비자 평가는 매우 중요한 역할을 합니다. 이는 소비자들이 제품을 구매하기 전에 다른 소비자들의 경험과 평가를 참고하는 경향이 강해졌기 때문입니다. \n",
      "\n",
      "오늘날의 시장은 온라인 미디어와 커뮤니티, 소셜네트워크서비스(SNS) 등을 통해 특정 상품에 대한 평가가 쉽게 공유되고 접근할 수 있는 환경이 조성되어 있습니다. 이러한 환경에서는 소비자들이 제품을 구매하기 전에 다른 소비자들의 평가를 통해 미래의 사용 경험을 예측할 수 있게 됩니다. 예를 들어, 과거에는 A사의 노트북을 사용한 경험이 좋았다는 이유로 A사의 신제품을 구매했다면, 이제는 \"A사 새 노트북의 발열 문제가 심각하다\"는 소비자 평가를 보고 다른 제품으로 눈을 돌리는 식입니다[[ref4]].\n",
      "\n",
      "이러한 변화는 소비자들이 제품의 절대가치를 평가하는 데 큰 영향을 미칩니다. 절대가치는 소비자가 제품을 실제 사용할 때 경험하게 되는 품질이나 가치를 말합니다. 과거에는 제조사가 동일한 상품군에서 비싼 제품을 새로 내놓아 저렴한 제품의 판매를 늘리는 방법을 사용했지만, 정보화 시대에서는 다양한 정보를 통해 소비자들이 자신이 원하는 가치에 근접한 절대 평가를 하게 됩니다[[ref4]].\n",
      "\n",
      "결과적으로, 현대 마케팅 전략에서는 소비자 평가가 중요한 변수로 작용합니다. 소비자 평가가 긍정적일 경우 제품의 신뢰성과 인지도가 높아져 판매에 긍정적인 영향을 미칠 수 있으며, 반대로 부정적인 평가가 많을 경우 제품의 이미지와 판매에 부정적인 영향을 미칠 수 있습니다. 따라서 기업들은 소비자 평가를 적극적으로 관리하고, 이를 바탕으로 제품의 품질을 개선하며, 소비자와의 신뢰를 구축하는 것이 중요합니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "번역서의 저자와 번역자의 역할은 서로 다른 개념이며, 그 차이를 이해하는 것은 번역의 과정과 결과에 큰 영향을 미칩니다.\n",
      "\n",
      "번역서의 저자는 원문을 번역한 사람을 가리킵니다. 이 사람들은 원문의 내용을 이해하고, 그 정보를 다른 언어로 재구성하는 작업을 수행합니다. 저자는 원문의 의미를 보존하고, 가능한 한 원문의 톤, 스타일, 문맥 등을 유지하는 데 노력합니다. 저자의 역할은 원문을 신중하게 해석하고, 그 정보를 목적어 언어로 재구성하는 것입니다.\n",
      "\n",
      "반면에 번역자의 역할은 좀 더 넓습니다. 번역자는 단순히 원문을 다른 언어로 번역하는 것이 아니라, 원문의 메시지를 목적어 언어의 문화적, 사회적, 언어적 맥락에 적합하게 재구성하는 역할을 합니다. 번역자는 원문의 내용을 이해하고, 그 정보를 목적어 언어로 재구성하는 데 더 넓은 자유를 가지고 있습니다. 그러나 동시에, 번역자는 원문의 메시지를 보존하고, 가능한 한 원문의 톤, 스타일, 문맥 등을 유지하는 데도 노력해야 합니다.\n",
      "\n",
      "즉, 번역자의 역할은 저자의 역할을 넘어, 원문을 목적어 언어로 재구성하는 것뿐만 아니라, 그 정보를 목적어 언어의 문화적, 사회적 맥락에 적합하게 재구성하는 것을 포함합니다.\n",
      "    label:\n",
      "\n",
      "번역서의 저자와 번역자의 역할 차이는 다음과 같습니다.\n",
      "\n",
      "1. **저자**: 저자는 원작을 창작한 사람으로, 작품의 내용, 구조, 스타일 등을 모두 결정합니다. 저자는 자신의 아이디어와 창의력을 바탕으로 작품을 만들어내며, 작품의 주제와 메시지를 전달하는 데 중점을 둡니다. 예를 들어, 마르틴 루터는 히브리어 및 코이네 그리스어 성경을 독일어로 번역하면서 자신의 번역 원칙을 세우고, 번역 과정에서 직역과 의역을 적절히 사용하여 독자에게 의미를 전달하려 했습니다 [[ref3]].\n",
      "\n",
      "2. **번역자**: 번역자는 원작을 다른 언어로 옮기는 역할을 합니다. 번역자는 원작의 의미와 뉘앙스를 최대한 정확하게 전달하려고 노력하며, 원작의 스타일과 톤을 유지하는 것이 중요합니다. 번역자는 원작의 내용을 충실히 반영하면서도, 번역된 언어의 독자들이 이해하기 쉽게 표현해야 합니다. 예를 들어, 스기타 겐파쿠와 그의 동료들은 네덜란드어 해부학서를 일본어로 번역하면서 도판을 먼저 그리고 나중에 본문을 번역하는 등 다양한 방법을 사용하여 번역 작업을 수행했습니다 [[ref1]].\n",
      "\n",
      "번역자는 원작의 저자가 아니기 때문에, 번역 과정에서 원작의 의도를 왜곡하지 않도록 주의해야 합니다. 또한, 번역자는 원작의 문화적, 역사적 배경을 이해하고 이를 번역된 언어로 적절히 전달해야 합니다. 번역자는 원작의 저자와는 다른 역할을 수행하지만, 번역된 작품이 원작의 가치를 유지하면서도 새로운 독자들에게 전달될 수 있도록 중요한 역할을 합니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_inference(prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    " \n",
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b671027-df14-4410-a475-76af1252100b",
   "metadata": {},
   "source": [
    "## 학습 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8a8f2f6c-9a99-4381-96bb-40c0fcba6bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eae03f40e940b1a41bf2146ded7a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-150\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c90387-ae6a-43fb-8a7f-688c32893ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "구석기 시대와 철기 시대의 도구 제작 방식에는 여러 가지 차이가 있습니다.\n",
      "\n",
      "구석기 시대에는 주로 뗀석기(타제석기)를 사용하여 도구를 제작했습니다. 이는 주로 돌을 깨뜨려 사용하기 편리하도록 만든 방식입니다. 예를 들어, 슴베찌르개는 격지나 돌날을 이용해 만들었으며, 주로 돌을 깨뜨려 사용하기 편리하도록 제작되었습니다. 슴베찌르개는 자루를 달아서 짐승을 사냥하는 창이나 전쟁 무기 또는 가죽에 구멍을 뚫는 연장 등으로 사용되었습니다.\n",
      "\n",
      "반면에 철기 시대에는 철이나 다른 금속을 사용하여 도구를 제작하는 방식이 주로 사용되었습니다. 철기 시대의 도구 제작 방식은 주로 철이나 다른 금속을 가공하여 원하는 형태로 만들어내는 것입니다. 예를 들어, 동검은 전형적인 세형동검으로 칼 끝이 예리하고 칼몸 끝까지 등날이 세워져 있으며, 칼자루 끝에는 물새 두 마리가 머리를 돌리고 서로 바라보는 모습을 하고 있습니다. 이는 전에 평양부근에서 출토된 적이 있는 것으로 북방지역 청동기 문화와의 연관성을 나타냅니다.\n",
      "\n",
      "따라서, 구석기 시대와 철기 시대의 도구 제작 방식의 주요 차이점은 사용되는 재료와 제작 방식에 있습니다. 구석기 시대에는 주로 돌을 사용하여 뗀석기를 사용하는 방식으로 제작하였으며, 철기 시대에는 철이나 다른 금속을 사용하여 가공하여 원하는 형태로 제작하는 방식을 사용하였습니다. [[ref1]], [[ref2]]\n",
      "    label:\n",
      "\n",
      "구석기 시대와 철기 시대의 도구 제작 방식에는 여러 가지 차이가 있습니다.\n",
      "\n",
      "구석기 시대의 도구는 주로 돌을 깨뜨려 사용하기 편리하도록 만든 뗀석기(타제석기)였습니다. 초기 구석기 시대에는 외날찍개와 같이 한 면만 떼어 낸 석기가 사용되었고, 중기에는 주먹도끼와 같이 두 면을 떼어 낸 쌍날찍개가 사용되었습니다. 후기 구석기 시대에는 격지나 돌날의 양쪽을 단단한 뼈나 뿔로 눌러 떼어 도구를 만드는 간접떼기 방식이 사용되었습니다. 대표적인 도구로는 슴베찌르개가 있으며, 이는 자루를 달아서 사냥이나 전쟁 무기 등으로 사용되었습니다 [[ref1]].\n",
      "\n",
      "반면 철기 시대에는 금속을 사용한 도구와 무기가 제작되었습니다. 예를 들어, 철기 시대의 동검은 칼과 손잡이, 칼끝 장식이 별개로 만들어졌으며, 칼자루 장신구와 칼집부속구 등 다양한 부속품이 함께 사용되었습니다. 이러한 도구들은 주로 청동이나 철을 녹여서 주조하거나 단조하는 방식으로 제작되었습니다 [[ref2]].\n",
      "\n",
      "따라서 구석기 시대의 도구 제작 방식은 주로 돌을 깨뜨려 형태를 만드는 타제석기 방식이었고, 철기 시대에는 금속을 녹여서 주조하거나 단조하는 방식으로 도구를 제작하는 차이가 있었습니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "제3차 중동 전쟁과 아랍의 겨울은 각각 다른 시기에 발생한 사건이며, 주요 원인과 결과는 다음과 같습니다.\n",
      "\n",
      "제3차 중동 전쟁은 1967년 6월에 시작된 이스라엘과 주변 국가인 이집트, 요르단, 시리아, 레바논 사이의 전쟁입니다. 이 전쟁의 주요 원인은 이스라엘과 주변 국가 사이의 긴장과 불안정한 관계였습니다. 이스라엘은 1956년 수에즈 위기 때 주요 목표 중 하나였던 티란 해협을 확보하려고 했고, 이 지역은 1950년 이래 이집트가 이스라엘의 항해를 봉쇄하고 있었습니다. 이스라엘은 이 지역의 봉쇄를 계속해서 유지하려고 했지만, 이는 결국 1967년 6월에 이스라엘과 주변 국가 사이의 전쟁으로 이어졌습니다.\n",
      "\n",
      "전쟁의 결과로, 이스라엘은 이집트로부터 가자 지구와 시나이 반도를, 요르단으로부터 동예루살렘과 요르단 강 서안 지구를, 시리아로부터 골란 고원을 획득했습니다. 이스라엘의 영토는 3배나 커졌고, 이스라엘군은 군사적 승리를 거두었습니다. 그러나 이 전쟁으로 인해 군사들은 자만감에 빠졌고, 이는 1973년 욤키푸르 전쟁 당시 아랍 연합군이 초기에 승리를 거두는 계기가 되었습니다. 또한, 이 전쟁으로 인해 100,000명의 시리아인이 골란 고원을 떠났고, 300,000명의 팔레스타인인이 서안 지구를 떠났는데 이들은 모두 난민이 되었습니다.\n",
      "\n",
      "아랍의 겨울은 2011년 아랍의 봄 이후 아랍 일대에서 벌어지고 있는 광범위한 혼란과 폭력과 사회불안정 사태를 가리키는 말입니다. 이 겨울에는 시리아 내전, 이라크 내전, 이집트 위기, 예멘 내전이 모두 포함됩니다. 권위주의가 복고되고 자유 민권이 억압되고 있으며, 이슬람 극단주의와 테러 단체가 곳곳에서 준동하고 있습니다. 아랍의 겨울은 복수의 지역에서 내란과 내전이 발생하고, 사회가 불안정해지며, 아랍 지역의 경제와 인구가 쇠퇴하고 인종주의와 종교적 종파주의가 판을 치는 것으로 특징지어집니다. 아랍의 겨울의 가장 극단적 사례는 시리아입니다. 바샤르 알 아사드 대통령의 독재에 반대하여 일어난 시위에 알누스라 전선 등의 이슬람주의 집단이 개입하고, 자유 시리아군의 부패와 범죄가 밝혀지면서 이슬람주의자들의 입지가 강화되었습니다. 그리하여 시리아는 독재정부와 반정부 반군과 이슬람주의 테러리스트들의 삼파내전에 휩싸이게 되었으며, 그 여파는 이웃나라인 레바논과 이라크까지 번지고 있습니다. [[ref1]], [[ref4]]\n",
      "    label:\n",
      "\n",
      "제3차 중동 전쟁과 아랍의 겨울의 주요 원인과 결과는 다음과 같습니다.\n",
      "\n",
      "### 제3차 중동 전쟁 (6일 전쟁)\n",
      "**주요 원인:**\n",
      "1. **티란 해협 봉쇄:** 이집트가 이스라엘 선박의 티란 해협 통과를 봉쇄하면서 긴장이 고조되었습니다. 이스라엘은 이를 전쟁의 명분으로 삼았습니다[[ref1]].\n",
      "2. **군사적 준비:** 이집트는 이스라엘과의 국경에 군을 배치하고, 이스라엘은 이에 대응해 예방적 공습을 감행했습니다[[ref1]].\n",
      "\n",
      "**주요 결과:**\n",
      "1. **영토 확장:** 이스라엘은 가자 지구, 시나이 반도, 동예루살렘, 요르단 강 서안 지구, 골란 고원을 점령하여 영토를 3배로 확장했습니다[[ref1]].\n",
      "2. **난민 발생:** 전쟁으로 인해 약 100,000명의 시리아인과 300,000명의 팔레스타인인이 난민이 되었습니다[[ref1]].\n",
      "3. **군사적 자만:** 이스라엘의 승리는 군사적 자만을 초래하여 이후 1973년 욤키푸르 전쟁에서 아랍 연합군의 초기 승리를 가능하게 했습니다[[ref1]].\n",
      "\n",
      "### 아랍의 겨울\n",
      "**주요 원인:**\n",
      "1. **아랍의 봄의 실패:** 2011년 아랍의 봄 이후 민주화와 개혁을 목표로 한 시도들이 실패하면서 권위주의가 복고되고, 사회적 불안정이 증가했습니다[[ref4]].\n",
      "2. **이슬람 극단주의의 부상:** 이슬람 극단주의와 테러 단체들이 활동을 강화하면서 지역 내 폭력과 혼란이 가중되었습니다[[ref4]].\n",
      "\n",
      "**주요 결과:**\n",
      "1. **내전과 사회 불안정:** 시리아 내전, 이라크 내전, 예멘 내전 등 여러 지역에서 내전과 사회 불안정이 발생했습니다[[ref4]].\n",
      "2. **경제적 손실:** 아랍의 겨울로 인해 발생한 사회적 손실비용은 약 8천억 달러에 달하며, 수백만 명의 난민이 발생했습니다[[ref4]].\n",
      "3. **인도적 위기:** 시리아, 이집트, 이라크 등 여러 지역에서 인도적 지원이 필요한 사람들의 수가 급증했습니다[[ref4]].\n",
      "\n",
      "이와 같이 제3차 중동 전쟁과 아랍의 겨울은 각각 중동 지역의 군사적, 정치적, 사회적 변화를 초래한 중요한 사건들입니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "한국과 미국의 핀테크 산업 경쟁력 차이는 규제와 인프라의 차이로 크게 결정됩니다. 미국은 54개의 핀테크 기업을 포함한 전 세계 100대 핀테크 기업 순위에 이름을 올렸습니다. 반면, 한국은 한 곳도 없었습니다. 이 차이는 과도한 금융규제가 국내 핀테크산업 발전을 가로막고 있다는 지적이 있습니다. 특히, 대부업법과 개인정보보호법, 전자금융거래법 등이 핀테크산업 활성화를 저해하는 규제로 꼽혔습니다. 미국은 규제가 적게 설정되어 핀테크 기업들이 자유롭게 사업을 운영할 수 있는 환경을 제공하고 있습니다 [[ref1]].\n",
      "    label:\n",
      "\n",
      "한국과 미국의 핀테크 산업 경쟁력 차이는 주로 규제의 차이에서 비롯됩니다.\n",
      "\n",
      "한국의 핀테크 산업은 과도한 금융 규제로 인해 발전이 저해되고 있습니다. 예를 들어, 대부업법 때문에 P2P(개인 간 대출) 금융이 활성화되지 못하고, 크라우드펀딩 법안도 투자자에게 많은 제약을 가하고 있습니다. 또한 개인정보보호법과 전자금융거래법도 핀테크 산업의 성장을 방해하는 요소로 작용하고 있습니다. 이러한 규제들은 한국 핀테크 기업들이 글로벌 시장에서 경쟁력을 갖추기 어렵게 만들고 있습니다 [[ref1]].\n",
      "\n",
      "반면, 미국은 핀테크 산업에서 규제 완화와 혁신을 적극적으로 추진하고 있습니다. 미국은 세계 100대 핀테크 기업 중 54개를 차지할 정도로 핀테크 산업에서 강력한 경쟁력을 보이고 있습니다. 이는 미국이 핀테크 기업들이 자유롭게 혁신할 수 있는 환경을 제공하고 있기 때문입니다 [[ref1]].\n",
      "\n",
      "또한, 한국에서는 개인정보 유출 사태 이후 본인 인증 강화 등의 규제가 도입되면서 IT 및 핀테크 산업의 발전에 부정적인 영향을 미치고 있습니다. 반면, 미국은 이러한 규제보다는 정보 유출 방지에 초점을 맞추고 있어 산업 발전에 더 유리한 환경을 조성하고 있습니다 [[ref3]].\n",
      "\n",
      "결론적으로, 한국과 미국의 핀테크 산업 경쟁력 차이는 주로 규제의 엄격함과 혁신 환경의 차이에서 비롯됩니다. 한국은 과도한 규제로 인해 핀테크 산업의 성장이 저해되고 있는 반면, 미국은 규제 완화와 혁신을 통해 핀테크 산업의 경쟁력을 강화하고 있습니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

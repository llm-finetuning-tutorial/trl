{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89a1820-52b3-4f1b-8aa8-ac57b383d873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "# %pip install \"torch==2.4.0\" \n",
    "%pip install -q tensorboard pillow wandb \n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install -q --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26e0361-81a8-44cf-9cf1-6cf9da30e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pillow -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dee226-cb1e-4473-bc20-ebb5f6f38fcc",
   "metadata": {},
   "source": [
    "## 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ee39e-d9d6-44ee-b6a8-acf410fedd71",
   "metadata": {},
   "source": [
    "과제물 사진과 과제물 정보를 주고 실제 정답을 맞추는 모델을 개발합니다. \n",
    "\n",
    "이 모델은 선생님들이 학생들의 수행평가를 채점한다고 가정합니다. \n",
    "본 프로젝트는 실제 교육 현장의 데이터와 차이가 있을 수 있으나, 자동 채점 시스템의 기본 원리와 가능성을 탐구하는 데 중점을 둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a4d54-537a-4dae-9386-2e66b8e8e67e",
   "metadata": {},
   "source": [
    "이번 예시에서는 Ko-SciecneQA 데이터셋을 사용할 건데요. 이 데이터셋은 12,726개의 아마존 제품의 제목, 이미지, 설명 및 메타데이터를 포함하고 있습니다.  \n",
    "이 중에서 이미지를 가지고 있는 6,218개의 데이터 중 시간 절약을 위해서 모두 사용하지는 않고 여기서 20%(1,243)만 사용하겠습니다.\n",
    "\n",
    "이미지, 문제, 힌트을 기반으로 정답을 생성하도록 모델을 파인튜닝하려 합니다.  \n",
    "따라서 이미지, 문제, 힌트를 포함한 입력을 만들고, 이를 이용하여 정답을 찾아보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af977a22-1d6a-4977-a0f2-e2c5508695c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: 이미지는 프롬프트에 직접 제공되지 않고 \"processor\"의 일부로 포함됨\n",
    "prompt= \"\"\"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \n",
    "\n",
    "##CHOICES##: {korean_choices}\n",
    "##HINT##: {korean_hint}\"\"\"\n",
    "\n",
    "system_message = \"당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f90aee6-602f-409e-a80f-3b06d855d202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e786e2d6978d467b96c49d2235c9f6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/881 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb6a4eb731d42d0938e2174fac75652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df7c46d07344facacf3a01cca4730c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9af93329ec74d08994c4ec6fd867ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': '당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': \"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \\n\\n##CHOICES##: ['신시내티', '링컨', '클리블랜드', '콜럼버스']\\n##HINT##: \"}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429 at 0x7E6F6A1C6450>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '콜럼버스'}]}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환하는 함수      \n",
    "def format_data(sample):\n",
    "   return {\"messages\": [\n",
    "               {\n",
    "                   \"role\": \"system\", # 시스템 역할\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": system_message}], # 시스템 메시지\n",
    "               },\n",
    "               {\n",
    "                   \"role\": \"user\",  # 사용자 역할\n",
    "                   \"content\": [\n",
    "                       {\n",
    "                           \"type\": \"text\",\n",
    "                           # 제품명과 카테고리를 포함한 프롬프트 생성\n",
    "                           \"text\": prompt.format(korean_choices=sample[\"korean_choices\"], korean_hint=sample[\"korean_hint\"]),\n",
    "                       },{\n",
    "                           \"type\": \"image\", # 이미지 타입\n",
    "                           \"image\": sample[\"image\"] if sample[\"image\"] is not None else \"\", # 제품 이미지\n",
    "                       }\n",
    "                   ],\n",
    "               },\n",
    "               {\n",
    "                   \"role\": \"assistant\", # AI 어시스턴트 역할\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": sample[\"answer_str\"]}], # 제품 설명\n",
    "               },\n",
    "           ],\n",
    "       }\n",
    "\n",
    "# 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"daje/Ko-SciecneQA\", split=\"train\")\n",
    "dataset = dataset.filter(lambda example: example[\"image\"] is not None)\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환\n",
    "# PIL.Image 타입을 유지하기 위해 리스트 컴프리헨션 사용 (.map()은 이미지를 바이트로 변환해버림)\n",
    "dataset = [format_data(sample) for sample in dataset]\n",
    "\n",
    "print(dataset[345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58727288-4b2d-4433-9251-4b0780b4853c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6218,\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': '당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.'}]},\n",
       "   {'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': \"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \\n\\n##CHOICES##: ['신시내티', '링컨', '클리블랜드', '콜럼버스']\\n##HINT##: \"},\n",
       "     {'type': 'image',\n",
       "      'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429>}]},\n",
       "   {'role': 'assistant', 'content': [{'type': 'text', 'text': '콜럼버스'}]}]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b1d12d-ced0-433d-9be9-b9fe7d6d6c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수: 6218\n"
     ]
    }
   ],
   "source": [
    "# 위에 준 코드 실행 후\n",
    "train_dataset = dataset[:int(len(dataset) * 0.9)]\n",
    "test_dataset = dataset[int(len(dataset) * 0.9):]\n",
    "\n",
    "print(f\"데이터 개수: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac032fac-202c-4518-9e08-3625c06f1f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5596, 622)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed2dfd-268e-4d41-8873-2539df77131b",
   "metadata": {},
   "source": [
    "## trl의 SFTTrainer를 이용한 파인 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a92ab-c684-4885-a215-0be96ad28147",
   "metadata": {},
   "source": [
    "`trl`의 SFTTrainer를 사용해 모델을 파인튜닝할 건데요. SFTTrainer는 오픈소스 LLM과 VLM의 지도 파인튜닝을 매우 간단하게 만들어줍니다.  \n",
    "SFTTrainer는 `transformers` 라이브러리의 `Trainer`를 상속받아서 로깅, 평가, 체크포인트 등 모든 기능을 지원하면서도 추가적인 편의 기능을 제공합니다.\n",
    "\n",
    "이번 예시에서는 PEFT 기능을 사용할 예정입니다. PEFT 방법으로는 QLoRA를 사용할 건데, 이는 양자화와 LoRA 튜닝을 같이 사용하여 대규모 언어 모델의 메모리 사용량을 줄이는 기술입니다.\n",
    "\n",
    "* 참고: 멀티모달 입력에 패딩이 필요하기 때문에 Flash Attention은 사용할 수 없습니다.*\n",
    "\n",
    "Qwen 2 VL 7B 모델을 사용할 예정이지만, `model_id` 변수만 바꾸면 Meta AI의 Llama-3.2-11B-Vision, Mistral AI의 Pixtral-12B 등 다른 모델로도 쉽게 교체할 수 있습니다. bitsandbytes를 사용해 모델을 4비트로 양자화할 예정입니다.\n",
    "\n",
    "* 참고: 모델이 클수록 더 많은 메모리가 필요합니다. 이번 예시에서는 7B 모델을 사용할 예정입니다.*\n",
    "\n",
    "VLM 학습을 위해 LLM, 토크나이저, 프로세서를 올바르게 준비하는 것이 매우 중요합니다. 프로세서는 특수 토큰과 이미지를 입력에 포함시키는 역할을 담당하는 모듈입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28fa661e-22e5-4843-87de-023ee47f71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb73cc11665e436b83beede283c489f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a708786e2270404aa725c25eb07eec24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecc5bb57d2445abbcfab9448a1d5db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/108k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af2d5a3f169426b8da3c0fa8eced204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c5c2f8820f49638b575db0f3ab2ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00038.safetensors:   0%|          | 0.00/3.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcd12f75bfd422999f57b3ed3f6f9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510068cce0584406b004faabd56c7f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547baaf1e2d449b591257da7a01f1923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4307cf6bc924f1884fdb9f445dce156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41cf3d4702846c585850b3c3014c1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0ff1439b384dc995f84540094d1022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8115e31047649d188d655b2a0429519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394a1e089923417592399e2667847051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5caeda039d49f1a71b1ed9e43202e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9907463dcf4c45b58d75377cf14a3e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd2f4a615f14c64b3f01475b0a46348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da7e513025b488ab1616141e59a696b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13cd67f769f4d24b5efab9548c0933b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b290d5847bbb4100999c1649da90e641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd7a98c73d54fd7af07430e96e01fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237203f91f9e499a84d15555385d22c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ea4cece73844b9b14661e4d7e4d351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f7454e99624214926cabc8fba3276e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dddfd95a69a4048a96e5f13ac4ac83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818c2853a6a34f078eeae9d02b763756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab12ec780d54c4489d3a08a47ca79b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636e49818b2149d4acacf90d24168216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71395c48e2a4571b569a563601b6415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968ccf794f9c434abe0b1ffd5e12c560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9c9764cefa4330939d09e6d1476e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdadd1d6daa499a86571cd2b3b33457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bbb0570bc840a189f2e0f06e35ef5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406c51895da14ed7a96c78a5b61215bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f144bcb3a74642b668d2d3de607c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4147e319c39c4e90ac9f04377a0a196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00031-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71a2482a1ad41f4b317625937580517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00032-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11174624f8cc4b2a9e259feddf6898da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00033-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc54f37eeaa14b138d53692e253eb2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00034-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ab305aeaf943cfab603f5160306b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00035-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0679fc12334492b6a446371948c972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00036-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c00a87da3a647cbafd4a545cb77e999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00037-of-00038.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d0cfcd24dc48d09a402f42a7c441b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00038-of-00038.safetensors:   0%|          | 0.00/2.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ffcf17108b464e9f62890c0e927b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a84dbc28c64416893e1dae8f841b7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea083bd296b490990e968b5d54531e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a938d0da1a34dc6aa05978523f33da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e273edf06e4d484bbbfed0c6bdf45d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8747f2fdd942828c29f629b36c4258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad7e80f59a240f39a2caece5851bd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efaf40743974479b326951950b8f6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-VL-72B-Instruct\" \n",
    "\n",
    "# BitsAndBytes 4비트 양자화 설정\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,                             # 4비트 양자화 사용\n",
    "#    bnb_4bit_use_double_quant=True,               # 이중 양자화 사용으로 메모리 추가 절약\n",
    "#    bnb_4bit_quant_type=\"nf4\",                    # 4비트 양자화 타입 설정(normalized float 4)\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16         # 연산 시 bfloat16 타입 사용\n",
    "# )\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "   model_id,\n",
    "   device_map=\"auto\",                            # GPU 메모리에 자동 할당\n",
    "   # attn_implementation=\"flash_attention_2\",     # 학습시에는 flash attention 2 미지원\n",
    "   torch_dtype=torch.bfloat16,                   # bfloat16 정밀도 사용\n",
    "   # quantization_config=bnb_config                # 위에서 정의한 양자화 설정 적용\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)  # 텍스트/이미지 전처리기 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29d040-3ca9-4638-a8ae-f7d34bb461c6",
   "metadata": {},
   "source": [
    "다음은 QWEN의 템플릿 예시입니다.  \n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|im_end|>\n",
    "<|im_start|>assistant\n",
    "거대 언어 모델의 답변<|im_end|>\n",
    "```\n",
    "\n",
    "멀티모달 QWEN은 이렇게 사용할 겁니다.\n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|vision_start|>이미지<|vision_end|><|im_end|>\n",
    "<|im_start|>assistant\n",
    "거대 언어 모델의 답변<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e4a9eb-0886-41de-a168-6f045624825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \n",
      "\n",
      "##CHOICES##: ['Kathleen의 스노보드가 왁스 층이 있을 때와 없을 때 중 어느 경우에 언덕을 더 빨리 내려오는가?', 'Kathleen의 스노보드가 얇은 왁스 층이 있을 때와 두꺼운 왁스 층이 있을 때 중 어느 경우에 언덕을 더 빨리 내려오는가?']\n",
      "##HINT##: 아래의 글은 실험을 설명합니다. 글을 읽고 아래의 지시를 따르세요.\n",
      "\n",
      "Kathleen은 스노보드의 밑면에 얇은 왁스 층을 바르고 언덕을 곧장 내려갔습니다. 그런 다음, 왁스를 제거하고 다시 스노보드를 타고 언덕을 내려갔습니다. 그녀는 왁스를 바른 상태와 바르지 않은 상태를 번갈아 가며 네 번 더 반복했습니다. 그녀의 친구 Bryant는 각 타이밍을 측정했습니다. Kathleen과 Bryant는 왁스를 바른 스노보드로 언덕을 내려가는 평균 시간과 왁스를 바르지 않은 스노보드로 내려가는 평균 시간을 계산했습니다.\n",
      "그림: 언덕을 내려가는 스노보드.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "Kathleen의 스노보드가 왁스 층이 있을 때와 없을 때 중 어느 경우에 언덕을 더 빨리 내려오는가?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    dataset[2][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2acd65-fc31-4427-9d64-228efabdb5a4",
   "metadata": {},
   "source": [
    "SFTTrainer는 peft와 기본적으로 통합되어 있어 LoraConfig를 만들어서 트레이너에 제공하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4890201-2cba-4de3-a5ed-bad569406b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"up_proj\",\n",
    "            \"o_proj\",\n",
    "            \"k_proj\",\n",
    "            \"down_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44497f73-c125-49d3-9530-9b5918ea75cb",
   "metadata": {},
   "source": [
    "학습을 시작하기 전에 사용할 하이퍼파라미터(SFTConfig)를 정의하고 입력이 모델에 올바르게 제공되는지 확인해야 합니다.  \n",
    "\n",
    "텍스트만 사용하는 지도 파인튜닝과 달리 모델에 이미지도 함께 제공해야 하는데요. 이를 위해 입력을 올바르게 포맷팅하고 이미지 특징을 포함하는 커스텀 DataCollator를 만들어야 합니다.  \n",
    "\n",
    "Qwen2 팀이 제공하는 유틸리티 패키지의 process_vision_info 메서드를 사용할 예정입니다. Llama 3.2 Vision 같은 다른 모델을 사용하는 경우라면, 동일한 방식으로 이미지 정보가 처리되는지 확인해봐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6820033-24b8-4cf3-9514-2a7956fa26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# SFTConfig를 통해 학습 설정을 정의\n",
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-72b-instruct-homeworks\",          # 학습된 모델과 체크포인트를 저장할 디렉터리 경로 및 리포지토리 ID\n",
    "    num_train_epochs=3,                                # 전체 학습 에포크 수 (데이터셋을 몇 번 반복할지 설정)\n",
    "    per_device_train_batch_size=1,                     # 각 장비(GPU)당 사용될 배치 사이즈 (메모리와 연관됨)\n",
    "    gradient_accumulation_steps=8,                     # 경사 누적 스텝 수 (이 횟수만큼 기울기를 누적한 후 업데이트)\n",
    "    gradient_checkpointing=True,                       # 메모리 절약을 위한 gradient checkpointing 활성화 (메모리 최적화)\n",
    "    optim=\"adamw_torch_fused\",                         # AdamW 옵티마이저 (fused 버전 사용으로 학습 속도 향상)\n",
    "    logging_steps=5,                                   # 몇 스텝마다 로그를 출력할지 설정 (여기선 5 스텝마다 로그)\n",
    "    save_strategy=\"epoch\",                             # 매 에포크마다 체크포인트 저장 설정\n",
    "    learning_rate=1e-5,                                # 학습률 (QLoRA 논문에서 추천된 값 사용)\n",
    "    bf16=True,                                         # bfloat16 정밀도 사용 (메모리 절약 및 속도 향상)\n",
    "    tf32=True,                                         # tf32 정밀도 사용 (NVIDIA GPU에서 학습 속도 향상)\n",
    "    max_grad_norm=0.3,                                 # 기울기 클리핑을 위한 최대 기울기 값 (QLoRA 논문에서 추천된 값)\n",
    "    warmup_ratio=0.03,                                 # 학습 초기에 학습률을 점진적으로 올리는 warmup 비율 (QLoRA 논문에서 추천된 값)\n",
    "    lr_scheduler_type=\"constant\",                      # 일정한 학습률 스케줄러 사용 (학습률이 변하지 않음)\n",
    "    # push_to_hub=True,                                  # 학습된 모델을 Hugging Face Hub에 푸시할지 여부\n",
    "    report_to=\"tensorboard\",                           # TensorBoard를 통해 학습 상태를 모니터링\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # reentrant gradient checkpointing 설정 (비재진입 방식 사용)\n",
    "    dataset_text_field=\"\",                             # 데이터셋에서 텍스트 필드를 위한 더미 필드 (collator에서 필요)\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}      # collator에서 데이터셋 전처리를 건너뛰기 위한 설정\n",
    ")\n",
    "\n",
    "# 불필요한 열 삭제하지 않도록 설정 (학습 중 사용되지 않는 열이라도 유지)\n",
    "args.remove_unused_columns = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb8111c8-ac97-4cf0-94e6-340760b33792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트와 이미지 쌍을 인코딩하기 위한 데이터 collator 함수 정의\n",
    "def collate_fn(examples):\n",
    "    # 각 예제에서 텍스트와 이미지를 추출하고, 텍스트는 채팅 템플릿을 적용\n",
    "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "    image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "    # 텍스트를 토크나이징하고 이미지를 처리하여 일괄 처리(batch) 형태로 변환\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # labels로 사용할 input_ids 복사본 생성 후, 패딩 토큰을 -100으로 설정하여 손실 계산 시 무시하도록 함\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # 패딩 토큰 손실 계산 제외\n",
    "\n",
    "    # 특정 이미지 토큰 인덱스는 손실 계산에서 무시 (모델에 따라 다름)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  \n",
    "        # Qwen2VL 모델의 이미지 토큰 인덱스\n",
    "        image_tokens = [151652, 151653, 151655]\n",
    "    else:\n",
    "        # 다른 모델에서 이미지 토큰 ID를 얻어 손실 계산에서 제외\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "    \n",
    "    # 손실 계산 시 이미지 토큰 인덱스를 무시하도록 설정\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100\n",
    "    \n",
    "    # 배치에 labels 추가 (손실 계산 시 사용)\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a349f087-67c9-4d56-bf56-acbc4768e750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 예시 데이터:\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': '당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': \"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \\n\\n##CHOICES##: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\\n##HINT##: \"}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429 at 0x7E6F6A1B3750>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '웨스트버지니아'}]}]}\n"
     ]
    }
   ],
   "source": [
    "# 단일 예시 확인\n",
    "example = dataset[0]  # 데이터셋의 첫 번째 아이템\n",
    "print(\"단일 예시 데이터:\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb461049-7b7d-40a4-967e-76d57e95e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 533])\n",
      "어텐션 마스크 형태: torch.Size([1, 533])\n",
      "이미지 픽셀 형태: torch.Size([1620, 1176])\n",
      "레이블 형태: torch.Size([1, 533])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"이미지 픽셀 형태:\", batch[\"pixel_values\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c15c695-c95e-4345-a898-25248bf37eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  64795,  82528,  33704,  90667,  21329,  80573,\n",
      "        126674,  18411,  63332,  34395, 126674,  18411, 131417, 132526,  16560,\n",
      "         15235,  21388,  78952,     13, 151645,    198, 151644,    872,    198,\n",
      "         54330,  31079,  85251,  90667,  21329,  80573,   7704,   1143,     46,\n",
      "         15835,    565,     11,   7704,     39,   3221,    565,  18411,  63332,\n",
      "         34395,  36055, 132760,  17877, 131417, 132526,  16560,  69192,    247,\n",
      "         37087, 129392,  40281,  56039,  78952,     13, 130887, 135968, 126550,\n",
      "         23573,  36055, 132760,  17877, 131417, 132526,  50302,     13,   4710,\n",
      "           565,  30498,  15835,    565,     25,   2509, 144025,  53189,  79004,\n",
      "         21329,  83036,  52959,    516,    364, 126746,  12802,  21329, 126898,\n",
      "         60315,    516,    364, 126898,  28002,  92817,  60315,    516,    364,\n",
      "         34992,  44680,    223,    112,  50340,  47324, 125544,   4432,    565,\n",
      "            39,   3221,    565,     25,    220, 151652, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645,    198,\n",
      "        151644,  77091,    198, 144025,  53189,  79004,  21329,  83036,  52959,\n",
      "        151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e79ead1-5782-41c9-a818-2b86a68b0eed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  64795,  82528,  33704,  90667,  21329,  80573,\n",
      "        126674,  18411,  63332,  34395, 126674,  18411, 131417, 132526,  16560,\n",
      "         15235,  21388,  78952,     13, 151645,    198, 151644,    872,    198,\n",
      "         54330,  31079,  85251,  90667,  21329,  80573,   7704,   1143,     46,\n",
      "         15835,    565,     11,   7704,     39,   3221,    565,  18411,  63332,\n",
      "         34395,  36055, 132760,  17877, 131417, 132526,  16560,  69192,    247,\n",
      "         37087, 129392,  40281,  56039,  78952,     13, 130887, 135968, 126550,\n",
      "         23573,  36055, 132760,  17877, 131417, 132526,  50302,     13,   4710,\n",
      "           565,  30498,  15835,    565,     25,   2509, 144025,  53189,  79004,\n",
      "         21329,  83036,  52959,    516,    364, 126746,  12802,  21329, 126898,\n",
      "         60315,    516,    364, 126898,  28002,  92817,  60315,    516,    364,\n",
      "         34992,  44680,    223,    112,  50340,  47324, 125544,   4432,    565,\n",
      "            39,   3221,    565,     25,    220,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100, 151645,    198,\n",
      "        151644,  77091,    198, 144025,  53189,  79004,  21329,  83036,  52959,\n",
      "        151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f93c39-a403-4ad5-808c-5d2d9382ee76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디코딩된 텍스트:\n",
      "<|im_start|>system\n",
      "당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \n",
      "\n",
      "##CHOICES##: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\n",
      "##HINT##: <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "웨스트버지니아<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 디코딩 예시 (입력 텍스트가 어떻게 변환되었는지 확인)\n",
    "decoded_text = processor.tokenizer.decode(batch[\"input_ids\"][0])\n",
    "print(\"\\n디코딩된 텍스트:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4567378a-89d7-44d4-ad0c-e093427a3cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    dataset_text_field=\"\",\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d22539-530b-4474-9d78-883107717dec",
   "metadata": {},
   "source": [
    "Trainer 인스턴스의 train() 메서드를 호출하여 모델 학습을 시작합니다.  \n",
    "이렇게 하면 학습 루프가 시작되고 3 에폭 동안 모델이 학습됩니다. PEFT 방법을 사용하고 있기 때문에 전체 모델이 아닌 조정된 모델 가중치만 저장할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28f037e1-c34c-402c-8d94-d49269c50292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2097' max='2097' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2097/2097 4:14:02, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.294200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.576900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.550700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.484300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.350100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.285700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.310300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.285400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.223700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.198300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>0.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.198300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>0.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>0.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>0.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>0.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>0.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>0.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>0.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>0.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>0.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>0.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.208700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>0.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>0.202600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>0.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>0.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>0.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135</td>\n",
       "      <td>0.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.166800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205</td>\n",
       "      <td>0.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215</td>\n",
       "      <td>0.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235</td>\n",
       "      <td>0.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255</td>\n",
       "      <td>0.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265</td>\n",
       "      <td>0.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285</td>\n",
       "      <td>0.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>0.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315</td>\n",
       "      <td>0.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335</td>\n",
       "      <td>0.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365</td>\n",
       "      <td>0.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1405</td>\n",
       "      <td>0.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445</td>\n",
       "      <td>0.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1465</td>\n",
       "      <td>0.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>0.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1505</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1515</td>\n",
       "      <td>0.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1535</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1545</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1555</td>\n",
       "      <td>0.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1565</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585</td>\n",
       "      <td>0.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1595</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1605</td>\n",
       "      <td>0.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1615</td>\n",
       "      <td>0.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1635</td>\n",
       "      <td>0.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1645</td>\n",
       "      <td>0.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1655</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1665</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1685</td>\n",
       "      <td>0.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1695</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1705</td>\n",
       "      <td>0.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1715</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1735</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1745</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1755</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1765</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1785</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1795</td>\n",
       "      <td>0.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1805</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1815</td>\n",
       "      <td>0.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1835</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1845</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1855</td>\n",
       "      <td>0.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1865</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1885</td>\n",
       "      <td>0.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.156600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1895</td>\n",
       "      <td>0.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1905</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1915</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1935</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1945</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1965</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1985</td>\n",
       "      <td>0.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2005</td>\n",
       "      <td>0.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015</td>\n",
       "      <td>0.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2035</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2045</td>\n",
       "      <td>0.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2055</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2065</td>\n",
       "      <td>0.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2085</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2095</td>\n",
       "      <td>0.159800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작, 모델은 자동으로 허브와 출력 디렉토리에 저장됨\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7db6ce71-f50c-4595-b534-baf14aa9021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 비우기\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493b6f7-8711-498e-b9b4-c5c54d290241",
   "metadata": {},
   "source": [
    "## 인퍼런스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955e49d-3dfc-41f0-9646-71f1eb1bb632",
   "metadata": {},
   "source": [
    "학습이 완료된 후에는 모델을 평가하고 테스트해볼 예정입니다.  \n",
    "\n",
    "먼저 기본 모델을 불러와서 임의의 아마존 제품에 대한 설명을 생성해보고, 그 다음 Q-LoRA로 조정된 모델을 불러와 같은 제품에 대한 설명을 생성해볼 것입니다.  \n",
    "마지막으로 더 효율적인 추론을 위해 어댑터를 기본 모델과 병합한 뒤, 동일한 제품에 대해 다시 한 번 추론을 실행해볼 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff2460c2-f0fe-45b5-a706-c36f6d181dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2-VL-72B-Instruct\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2788d0eb-7739-44ea-89a8-0ca97728fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f95350391c4b82892785bae8804c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    " \n",
    "# 기본 모델 호출\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee8d5bc4-1f96-495f-8dba-6e1a5dc3147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 답변을 생성하는 함수\n",
    "def generate_description(messages, model, processor):\n",
    "   # 추론을 위한 준비\n",
    "   text = processor.apply_chat_template(\n",
    "       messages, tokenize=False, add_generation_prompt=True\n",
    "   )\n",
    "   image_inputs, video_inputs = process_vision_info(messages)\n",
    "   inputs = processor(\n",
    "       text=[text],\n",
    "       images=image_inputs,\n",
    "       videos=video_inputs,\n",
    "       padding=True,\n",
    "       return_tensors=\"pt\",\n",
    "   )\n",
    "   inputs = inputs.to(model.device)\n",
    "   # 추론: 출력 생성\n",
    "   generated_ids = model.generate(**inputs, max_new_tokens=128, top_p=1.0, do_sample=True, temperature=0.1)\n",
    "   generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "   output_text = processor.batch_decode(\n",
    "       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "   )\n",
    "   return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af2c867f-da30-41aa-b1b9-b9da5f7130d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소금물에서 더 잘 뜨나요.\n"
     ]
    }
   ],
   "source": [
    "# 설명 생성해보기\n",
    "messages =  test_dataset[0][\"messages\"]\n",
    "base_description = generate_description(messages, model, processor)\n",
    "print(base_description)\n",
    "# 원하는 경우 아래 명령어로 활성화된 어댑터를 비활성화할 수 있음\n",
    "# model.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94ced566-c1b3-438b-8b73-ffad356a5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로라 어댑터가 있는 경로\n",
    "adapter_path = \"./qwen2-7b-instruct-homeworks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eb17220-fd25-4ae6-bf99-37d13ba25add",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_adapter(adapter_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a85779d1-ad4f-4ab0-a27a-959113b6f53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소금물에서 더 잘 뜨나요?\n"
     ]
    }
   ],
   "source": [
    "ft_description = generate_description(messages, model, processor)\n",
    "print(ft_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb02fee1-4ea9-4085-94ab-3b08981c1fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 학습 전 모델: 소금물에서 더 잘 뜨나요.\n",
      "---\n",
      "LoRA 학습 후 모델: 소금물에서 더 잘 뜨나요?\n"
     ]
    }
   ],
   "source": [
    "print('LoRA 학습 전 모델:', base_description)\n",
    "print('---')\n",
    "print('LoRA 학습 후 모델:', ft_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "771b5498-4d72-4309-8f48-d4d52cd94f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331fc9c148ae482893dd80cb34bd2768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "result = [] \n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    messages =  test_dataset[idx][\"messages\"]\n",
    "    ft_description = generate_description(messages, model, processor)\n",
    "    result.append(ft_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43c7b25a-21f9-442d-806c-fd9eaa654067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['소금물에서 더 잘 뜨나요?',\n",
       " '두 쌍의 자기력 크기는 같습니다.',\n",
       " '매사추세츠',\n",
       " '피닉스',\n",
       " '용액 B',\n",
       " '델라웨어',\n",
       " '샘플 B',\n",
       " '메릴랜드',\n",
       " '호주',\n",
       " '끌어당기다']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e440543a-32bc-4b3f-9cfd-a1317ca09dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['계란이 민물에서 더 잘 뜨나요, 아니면 소금물에서 더 잘 뜨나요?',\n",
       " '두 쌍의 자기력 크기는 같습니다.',\n",
       " '매사추세츠',\n",
       " '피닉스',\n",
       " '용액 B',\n",
       " '델라웨어',\n",
       " '샘플 B',\n",
       " '켄터키',\n",
       " '호주',\n",
       " '밀어내다']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = [data[\"messages\"][2][\"content\"][0][\"text\"] for data in test_dataset]\n",
    "answer[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75274661-8e4a-46c5-af63-a4aecc894ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73.1117824773414, 43.202416918429)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "484/662*100, 286/662*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3618c6df-4c2f-41f0-823e-b3688650e713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0 \n",
    "for a, b in zip(result, answer):\n",
    "    if a == b : \n",
    "        score += 1\n",
    "score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61ce42-4217-49a0-b309-7107c7de31ab",
   "metadata": {},
   "source": [
    "## 로라 병합 후 저장하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403983e-ef72-48ea-9376-0bfaa529c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54870841f8f441e5943a46774015c534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "adapter_path = \"./qwen2-7b-instruct-homeworks\"  # 학습된 어댑터 경로\n",
    "base_model_id = \"Qwen/Qwen2-VL-72B-Instruct\"  # 기본 모델 ID\n",
    "merged_path = \"merged\"  # 병합된 모델을 저장할 경로\n",
    "\n",
    "# 기본 모델 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# 병합된 모델 저장 경로\n",
    "# LoRA와 기본 모델을 병합하고 저장\n",
    "peft_model = PeftModel.from_pretrained(model, adapter_path)  # PEFT 모델 로드\n",
    "merged_model = peft_model.merge_and_unload()  # 모델 병합\n",
    "merged_model.save_pretrained(merged_path,safe_serialization=True, max_shard_size=\"2GB\")  # 병합된 모델 저장\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)  # 프로세서 로드\n",
    "processor.save_pretrained(merged_path)  # 프로세서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf832bf-a68d-4677-b50f-53e45497f60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f77836272640dcb43c25581264eea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238a5bef9cec4460b8fa1e4acb6db40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/108k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae40c4b3a2b4fcbab30d38b6f8ec2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdb281f77e54ecd9dda678fa555e5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00038.safetensors:   0%|          | 0.00/3.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a92060f2844cc1909e8eedd84b1499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947cc31e6cd24989a209f5ae419639d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bfd1269ef44f74a1ab16b8e15dd3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a15885cddb64113b1af8eff5b8581b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8861ef391cd741609b96912b96428084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33b9a255cb34ebca46f77886b784f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667bc5367e354204a76ae0a6509616dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f99c3b76684b728207df718dd42ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2538b4c64949d59d1788fe74478b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2761e8e3ad704808970a1f8ce5c870c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b950d343f0ae43ed813d8284a8320a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4662c0e0d054471daa702a7250821d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c961c4e4caf14a42b0dee6a70db49b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e64913bc174be9a92e375bab1404fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ad80a21e6c4a38b04ebc452326b585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13eeed82a08047a8a03be32f917fb9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0b2fe2504041ffb4de49b01f04905f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1f7e43e0f44f44a3abcbd823fb2716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a04bf25e8b41e190765fd90c0b1344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803cc6a5817644b0bce5d11aa06fc5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67efda88a3894f2aacd7c49354aa7e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5c77c4dc30497f8d18dd434bf93048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582908bad9c4474baa5f0289d1fa33b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb7e0868b7d404e965b910357879d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f46e3c478b041409991b9222dd2723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8beda481d1354f479e0cfe30cdda3d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b309f11a8b624277bf7cd7e38932f668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261562ac270e445fa07b6abec8785be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac3b5a799bd42eb905b8e6e8e6bfb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65954f3ed4341dba0e63f223d58372f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00031-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e7e925aa63410aa4d83c8b1f22dcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00032-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6565dededcc42369dd30056d4ef434d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00033-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbf94a9b349412ca22374aee1cfd43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00034-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff79894c34234ab0bf538c74b2bc4c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00035-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169c79ef037e4c1895c0790b925a2834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00036-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5084f797634fb48f85d16d607831d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00037-of-00038.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe9e1c2c3b54818ac04aa5fba6b6e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00038-of-00038.safetensors:   0%|          | 0.00/2.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee49ffa87c14327b287cdcb888f63a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fba6e7686a43a8852acabb8cc8dbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdfb2e9605c4ec28028ef422ba044cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c239c5246c489eaa564aab27c9fcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dffd440a54f4b91a3e8caa9760e12aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ca8194fe7f4a6ab9c1bc9c36930dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0a45b23ce741ee8b3b502253ac7863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd8f6765a734afea310ed9e8c6ebe0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "model_id = \"Qwen/Qwen2-VL-72B-Instruct\" \n",
    "adapter_path = \"./qwen2-7b-instruct-homeworks\"  # 학습된 어댑터 경로\n",
    "base_model_id = \"Qwen/Qwen2-VL-72B-Instruct\"  # 기본 모델 ID\n",
    "merged_path = \"merged\"  # 병합된 모델을 저장할 경로\n",
    "\n",
    "# 기본 모델 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# 병합된 모델 저장 경로\n",
    "# LoRA와 기본 모델을 병합하고 저장\n",
    "peft_model = PeftModel.from_pretrained(model, adapter_path)  # PEFT 모델 로드\n",
    "merged_model = peft_model.merge_and_unload()  # 모델 병합\n",
    "merged_model.save_pretrained(merged_path,safe_serialization=True, max_shard_size=\"2GB\")  # 병합된 모델 저장\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)  # 프로세서 로드\n",
    "processor.save_pretrained(merged_path)  # 프로세서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70229777-f38d-44ab-864f-1405982159f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모두 완료했습니다\n"
     ]
    }
   ],
   "source": [
    "print(\"모두 완료했습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdc68c0-5d4f-47b7-bcd0-64bacf050327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9943fcac4e4d3f838adfac8d28674d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_model_path = \"/workspace/trl/merged\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(local_model_path)\n",
    "processor = AutoProcessor.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5661c48a-9b1b-4ef6-ac64-158a5f91525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `huggingface` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `huggingface`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token Your_Huggingface_Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb8fd9-6310-456e-a928-65992e4466a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_model_id = \"daje/Qwen2-VL-72B-instruct-ScienceQA\"\n",
    "# 허깅페이스 업로드 \n",
    "model.push_to_hub(hub_model_id)\n",
    "processor.push_to_hub(hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c1ce5-f0a2-4197-88c2-c7a7e9c576dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1e418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89a1820-52b3-4f1b-8aa8-ac57b383d873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # %pip install \"torch==2.4.0\" \n",
    "# %pip install -q tensorboard pillow wandb \n",
    "# %pip install \"torchvision==0.19.0\"\n",
    "# # Install Hugging Face libraries\n",
    "# %pip install -q --upgrade \\\n",
    "#   \"transformers==4.45.1\" \\\n",
    "#   \"datasets==3.0.1\" \\\n",
    "#   \"accelerate==0.34.2\" \\\n",
    "#   \"evaluate==0.4.3\" \\\n",
    "#   \"bitsandbytes==0.44.0\" \\\n",
    "#   \"trl==0.11.1\" \\\n",
    "#   \"peft==0.13.0\" \\\n",
    "#   \"qwen-vl-utils\"\n",
    "# %pip install -q pillow -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dee226-cb1e-4473-bc20-ebb5f6f38fcc",
   "metadata": {},
   "source": [
    "## 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ee39e-d9d6-44ee-b6a8-acf410fedd71",
   "metadata": {},
   "source": [
    "과제물 사진과 과제물 정보를 주고 실제 정답을 맞추는 모델을 개발합니다. \n",
    "\n",
    "이 모델은 선생님들이 학생들의 수행평가를 채점한다고 가정합니다. \n",
    "본 프로젝트는 실제 교육 현장의 데이터와 차이가 있을 수 있으나, 자동 채점 시스템의 기본 원리와 가능성을 탐구하는 데 중점을 둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a4d54-537a-4dae-9386-2e66b8e8e67e",
   "metadata": {},
   "source": [
    "이번 예시에서는 Ko-SciecneQA 데이터셋을 사용할 건데요. 이 데이터셋은 12,726개의 아마존 제품의 제목, 이미지, 설명 및 메타데이터를 포함하고 있습니다.  \n",
    "이 중에서 이미지를 가지고 있는 6,218개의 데이터 중 시간 절약을 위해서 모두 사용하지는 않고 여기서 20%(1,243)만 사용하겠습니다.\n",
    "\n",
    "이미지, 문제, 힌트을 기반으로 정답을 생성하도록 모델을 파인튜닝하려 합니다.  \n",
    "따라서 이미지, 문제, 힌트를 포함한 입력을 만들고, 이를 이용하여 정답을 찾아보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bff7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/science/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution', 'korean_question', 'korean_hint', 'korean_choices', 'answer_str'],\n",
      "    num_rows: 12726\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution', 'korean_question', 'korean_hint', 'korean_choices', 'answer_str'],\n",
       "    num_rows: 6218\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 허브에서 데이터셋 로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daje/Ko-SciecneQA\", split=\"train\")\n",
    "print(dataset)\n",
    "dataset = dataset.filter(lambda example: example[\"image\"] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80407871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429>,\n",
       " 'question': 'Which of these states is farthest north?',\n",
       " 'choices': ['West Virginia', 'Louisiana', 'Arizona', 'Oklahoma'],\n",
       " 'answer': 0,\n",
       " 'hint': '',\n",
       " 'task': 'closed choice',\n",
       " 'grade': 'grade2',\n",
       " 'subject': 'social science',\n",
       " 'topic': 'geography',\n",
       " 'category': 'Geography',\n",
       " 'skill': 'Read a map: cardinal directions',\n",
       " 'lecture': 'Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\\nA compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\\nThe north arrow points to the North Pole. On most maps, north is at the top of the map.',\n",
       " 'solution': 'To find the answer, look at the compass rose. Look at which way the north arrow is pointing. West Virginia is farthest north.',\n",
       " 'korean_question': '다음 주 중에서 가장 북쪽에 있는 곳은 어디인가요?',\n",
       " 'korean_hint': '',\n",
       " 'korean_choices': ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마'],\n",
       " 'answer_str': '웨스트버지니아'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af977a22-1d6a-4977-a0f2-e2c5508695c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: 이미지는 프롬프트에 직접 제공되지 않고 \"processor\"의 일부로 포함됨\n",
    "prompt= \"\"\"주어진 이미지와 ##QUESTION##, ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \n",
    "\n",
    "##QUESTION##: {korean_question}\n",
    "##CHOICES##: {korean_choices}\n",
    "##HINT##: {korean_hint}\"\"\"\n",
    "\n",
    "system_message = \"당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f90aee6-602f-409e-a80f-3b06d855d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환하는 함수      \n",
    "def format_data(sample):\n",
    "   return {\"messages\": [\n",
    "               {\n",
    "                   # 시스템 역할\n",
    "                   \"role\": \"system\", \n",
    "                   # 시스템 메시지\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": system_message}], \n",
    "               },\n",
    "               {\n",
    "                   # 사용자 역할\n",
    "                   \"role\": \"user\",  \n",
    "                   \"content\": [\n",
    "                       {\n",
    "                           \"type\": \"text\",\n",
    "                           # 제품명과 카테고리를 포함한 프롬프트 생성\n",
    "                           \"text\": prompt.format(\n",
    "                              korean_question=sample[\"korean_question\"], \n",
    "                              korean_choices=sample[\"korean_choices\"], \n",
    "                              korean_hint=sample[\"korean_hint\"]\n",
    "                              ),\n",
    "                       },{\n",
    "                           # 이미지 타입\n",
    "                           \"type\": \"image\", \n",
    "                           # 이미지 파일\n",
    "                           \"image\": sample[\"image\"] , \n",
    "                       }\n",
    "                   ],\n",
    "               },\n",
    "               {\n",
    "                   # AI 어시스턴트 역할\n",
    "                   \"role\": \"assistant\", \n",
    "                   # 정답 \n",
    "                   \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": sample[\"answer_str\"]\n",
    "                        }\n",
    "                    ], \n",
    "               },\n",
    "           ],\n",
    "       }\n",
    "\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환\n",
    "# PIL.Image 타입을 유지하기 위해 리스트 컴프리헨션 사용\n",
    "dataset = [format_data(sample) for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58727288-4b2d-4433-9251-4b0780b4853c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6218,\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': '당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.'}]},\n",
       "   {'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': \"주어진 이미지와 ##QUESTION##, ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \\n\\n##QUESTION##: 오하이오주의 주도는 어디인가요?\\n##CHOICES##: ['신시내티', '링컨', '클리블랜드', '콜럼버스']\\n##HINT##: \"},\n",
       "     {'type': 'image',\n",
       "      'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429>}]},\n",
       "   {'role': 'assistant', 'content': [{'type': 'text', 'text': '콜럼버스'}]}]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b1d12d-ced0-433d-9be9-b9fe7d6d6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:int(len(dataset) * 0.9)]\n",
    "test_dataset = dataset[int(len(dataset) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "290c419f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5596, 622)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed2dfd-268e-4d41-8873-2539df77131b",
   "metadata": {},
   "source": [
    "## trl의 SFTTrainer를 이용한 파인 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a92ab-c684-4885-a215-0be96ad28147",
   "metadata": {},
   "source": [
    "`trl`의 SFTTrainer를 사용해 모델을 파인튜닝할 건데요. SFTTrainer는 오픈소스 LLM과 VLM의 지도 파인튜닝을 매우 간단하게 만들어줍니다.  \n",
    "SFTTrainer는 `transformers` 라이브러리의 `Trainer`를 상속받아서 로깅, 평가, 체크포인트 등 모든 기능을 지원하면서도 추가적인 편의 기능을 제공합니다.\n",
    "\n",
    "이번 예시에서는 PEFT 기능을 사용할 예정입니다. PEFT 방법으로는 QLoRA를 사용할 건데, 이는 양자화와 LoRA 튜닝을 같이 사용하여 대규모 언어 모델의 메모리 사용량을 줄이는 기술입니다.\n",
    "\n",
    "* 참고: 멀티모달 입력에 패딩이 필요하기 때문에 Flash Attention은 사용할 수 없습니다.\n",
    "\n",
    "Qwen 2 VL 72B 모델을 사용할 예정이지만, `model_id` 변수만 바꾸면 Meta AI의 Llama-3.2-11B-Vision, Mistral AI의 Pixtral-12B 등 다른 모델로도 쉽게 교체할 수 있습니다. bitsandbytes를 사용해 모델을 4비트로 양자화할 예정입니다.\n",
    "\n",
    "* 참고: 모델이 클수록 더 많은 메모리가 필요합니다. 이번 예시에서는 72B 모델을 사용할 예정입니다.\n",
    "\n",
    "VLM 학습을 위해 LLM, 토크나이저, 프로세서를 올바르게 준비하는 것이 매우 중요합니다. 프로세서는 특수 토큰과 이미지를 입력에 포함시키는 역할을 담당하는 모듈입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28fa661e-22e5-4843-87de-023ee47f71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:05<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\" \n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "   model_id,\n",
    "   device_map=\"auto\",                            # GPU 메모리에 자동 할당\n",
    "   # attn_implementation=\"flash_attention_2\",     # 학습시에는 flash attention 2 미지원\n",
    "   torch_dtype=torch.bfloat16,                   # bfloat16 정밀도 사용\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)  # 텍스트/이미지 전처리기 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29d040-3ca9-4638-a8ae-f7d34bb461c6",
   "metadata": {},
   "source": [
    "다음은 QWEN의 템플릿 예시입니다.  \n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|im_end|>\n",
    "<|im_start|>assistant\n",
    "거대 언어 모델의 답변<|im_end|>\n",
    "```\n",
    "\n",
    "멀티모달 QWEN은 이렇게 사용할 겁니다.\n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|vision_start|>이미지<|vision_end|><|im_end|>\n",
    "<|im_start|>assistant\n",
    "거대 언어 모델의 답변<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13e4a9eb-0886-41de-a168-6f045624825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "주어진 이미지와 ##QUESTION##, ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \n",
      "\n",
      "##QUESTION##: 표시된 식민지의 이름은 무엇인가요?\n",
      "##CHOICES##: ['매사추세츠', '로드아일랜드', '코네티컷', '뉴햄프셔']\n",
      "##HINT##: <|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "매사추세츠<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    test_dataset[2][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2acd65-fc31-4427-9d64-228efabdb5a4",
   "metadata": {},
   "source": [
    "SFTTrainer는 peft와 기본적으로 통합되어 있어 LoraConfig를 만들어서 트레이너에 제공하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7011e81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"주어진 이미지와 ##QUESTION##, ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \\n\\n##QUESTION##: 휘트니의 실험이 가장 잘 답할 수 있는 질문을 선택하세요.\\n##CHOICES##: ['계란이 민물에서 더 잘 뜨나요, 아니면 소금물에서 더 잘 뜨나요?', '유리잔에 있는 물의 양이 계란이 물에 가라앉거나 뜨는 것에 영향을 미치나요?']\\n##HINT##: 아래의 글은 실험을 설명합니다. 글을 읽고 아래의 지시를 따르세요.\\n\\n휘트니는 여섯 개의 유리잔 각각에 4온스의 물을 부었습니다. 휘트니는 세 개의 유리잔에 각각 한 스푼의 소금을 녹였고, 나머지 세 개에는 소금을 넣지 않았습니다. 그런 다음, 휘트니는 한 유리잔에 계란을 넣고 계란이 뜨는지 관찰했습니다. 그녀는 계란을 꺼내어 말렸습니다. 그녀는 다른 다섯 개의 유리잔에서도 이 과정을 반복하며 계란이 뜨는지 기록했습니다. 휘트니는 이 테스트를 두 개의 계란으로 더 반복하여 민물과 소금물에서 계란이 뜨는 횟수를 비교했습니다.\\n그림: 소금물에 떠 있는 계란.\"},\n",
       "    {'type': 'image',\n",
       "     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=302x232>}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '계란이 민물에서 더 잘 뜨나요, 아니면 소금물에서 더 잘 뜨나요?'}]}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f4a5f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [03:24<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# 모델 답변을 생성하는 함수\n",
    "def generate_description(messages, model, processor):\n",
    "   # 추론을 위한 준비\n",
    "   text = processor.apply_chat_template(\n",
    "       messages, tokenize=False, add_generation_prompt=True\n",
    "   )\n",
    "   image_inputs, video_inputs = process_vision_info(messages)\n",
    "   inputs = processor(\n",
    "       text=[text],\n",
    "       images=image_inputs,\n",
    "       videos=video_inputs,\n",
    "       padding=True,\n",
    "       return_tensors=\"pt\",\n",
    "   )\n",
    "   inputs = inputs.to(model.device)\n",
    "   # 추론: 출력 생성\n",
    "   generated_ids = model.generate(\n",
    "      **inputs, \n",
    "      max_new_tokens=128,\n",
    "      top_p=1.0, \n",
    "      do_sample=True, \n",
    "      temperature=0.1\n",
    "      )\n",
    "   generated_ids_trimmed = [\n",
    "      out_ids[len(in_ids) :] \n",
    "      for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "   output_text = processor.batch_decode(\n",
    "       generated_ids_trimmed, \n",
    "       skip_special_tokens=True, \n",
    "       clean_up_tokenization_spaces=False\n",
    "   )\n",
    "   return output_text[0]\n",
    "\n",
    "from tqdm.auto import tqdm \n",
    "no_train_result = [] \n",
    "\n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    messages =  test_dataset[idx][\"messages\"][:2]\n",
    "    answer = test_dataset[idx][\"messages\"][2][\"content\"][0][\"text\"]\n",
    "    base_description = generate_description(messages, model, processor)\n",
    "    no_train_result.append((answer, base_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51cf705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 전 정확도 : 33.12%\n"
     ]
    }
   ],
   "source": [
    "prediction_result = [(temp[0].strip() == temp[1].replace(\"##ANSWER##: '\", \"\").replace(\"'\", \"\").strip()) for temp in no_train_result]\n",
    "base_score = sum(prediction_result) /  len(prediction_result) * 100\n",
    "print(f\"학습 전 정확도 : {base_score:.2f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4d4ef-9058-4226-801a-3d228a8c60cd",
   "metadata": {},
   "source": [
    "## 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4890201-2cba-4de3-a5ed-bad569406b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# QLoRA 논문과 Sebastian Raschka의 실험을 기반으로 한 LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "       # 모델 가중치에 LoRA 업데이트를 적용하는 정도를 조절하는 스케일링 계수\n",
    "       lora_alpha=128,\n",
    "       # 과적합을 방지하기 위한 드롭아웃 비율 설정\n",
    "       lora_dropout=0.05,\n",
    "       # LoRA의 순위(rank) - 저차원 행렬의 차원을 결정\n",
    "       r=256,\n",
    "       # 편향(bias) 업데이트 여부 - 'none'은 편향을 업데이트하지 않음\n",
    "       bias=\"none\",\n",
    "       # LoRA를 적용할 대상 모듈들 - 트랜스포머 모델의 주요 투영 레이어들\n",
    "       target_modules=[\n",
    "           \"q_proj\",    # Query 투영 레이어\n",
    "           \"up_proj\",   # FFN 상향 투영 레이어\n",
    "           \"o_proj\",    # Output 투영 레이어\n",
    "           \"k_proj\",    # Key 투영 레이어\n",
    "           \"down_proj\", # FFN 하향 투영 레이어\n",
    "           \"gate_proj\", # FFN 게이트 투영 레이어\n",
    "           \"v_proj\"     # Value 투영 레이어\n",
    "       ],\n",
    "       # 작업 유형 지정 - 인과적 언어 모델링(다음 토큰 예측)\n",
    "       task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44497f73-c125-49d3-9530-9b5918ea75cb",
   "metadata": {},
   "source": [
    "학습을 시작하기 전에 사용할 하이퍼파라미터(SFTConfig)를 정의하고 입력이 모델에 올바르게 제공되는지 확인해야 합니다.  \n",
    "\n",
    "텍스트만 사용하는 지도 파인튜닝과 달리 모델에 이미지도 함께 제공해야 하는데요. 이를 위해 입력을 올바르게 포맷팅하고 이미지 특징을 포함하는 커스텀 DataCollator를 만들어야 합니다.  \n",
    "\n",
    "Qwen2 팀이 제공하는 유틸리티 패키지의 process_vision_info 메서드를 사용할 예정입니다. Llama 3.2 Vision 같은 다른 모델을 사용하는 경우라면, 동일한 방식으로 이미지 정보가 처리되는지 확인해봐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6820033-24b8-4cf3-9514-2a7956fa26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# SFTConfig를 통해 학습 설정을 정의\n",
    "args = SFTConfig(\n",
    "    # 학습된 모델과 체크포인트를 저장할 디렉터리 경로 및 리포지토리 ID\n",
    "    output_dir=\"qwen2-7b-instruct-homeworks\",              \n",
    "    # 전체 학습 에포크 수 (데이터셋을 몇 번 반복할지 설정)\n",
    "    num_train_epochs=2,                                     \n",
    "    # 각 장비(GPU)당 사용될 배치 사이즈 (메모리와 연관됨)\n",
    "    per_device_train_batch_size=1,                          \n",
    "    # 경사 누적 스텝 수 (이 횟수만큼 기울기를 누적한 후 업데이트)\n",
    "    gradient_accumulation_steps=8,                          \n",
    "    # 메모리 절약을 위한 gradient checkpointing 활성화 (메모리 최적화)\n",
    "    gradient_checkpointing=True,                            \n",
    "    # AdamW 옵티마이저 (fused 버전 사용으로 학습 속도 향상)\n",
    "    optim=\"adamw_torch_fused\",                              \n",
    "    # 몇 스텝마다 로그를 출력할지 설정 (여기선 5 스텝마다 로그)\n",
    "    logging_steps=5,                                        \n",
    "    # 매 에포크마다 체크포인트 저장 설정\n",
    "    save_strategy=\"epoch\",                                  \n",
    "    # 학습률 (QLoRA 논문에서 추천된 값 사용)\n",
    "    learning_rate=1e-5,                                     \n",
    "    # bfloat16 정밀도 사용 (메모리 절약 및 속도 향상)\n",
    "    bf16=True,                                              \n",
    "    # tf32 정밀도 사용 (NVIDIA GPU에서 학습 속도 향상)\n",
    "    tf32=True,                                              \n",
    "    # 기울기 클리핑을 위한 최대 기울기 값 (QLoRA 논문에서 추천된 값)\n",
    "    max_grad_norm=0.3,                                      \n",
    "    # 학습 초기에 학습률을 점진적으로 올리는 warmup 비율 (QLoRA 논문에서 추천된 값)\n",
    "    warmup_ratio=0.03,                                      \n",
    "    # 일정한 학습률 스케줄러 사용 (학습률이 변하지 않음)\n",
    "    lr_scheduler_type=\"constant\",                           \n",
    "    # 학습된 모델을 Hugging Face Hub에 푸시할지 여부\n",
    "    # push_to_hub=True,                                     \n",
    "    # TensorBoard를 통해 학습 상태를 모니터링\n",
    "    report_to=\"tensorboard\",                                \n",
    "    # reentrant gradient checkpointing 설정 (비재진입 방식 사용)\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, \n",
    "    # 데이터셋에서 텍스트 필드를 위한 더미 필드 (collator에서 필요)\n",
    "    dataset_text_field=\"\",                                  \n",
    "    # collator에서 데이터셋 전처리를 건너뛰기 위한 설정\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}           \n",
    ")\n",
    "\n",
    "# 불필요한 열 삭제하지 않도록 설정 (학습 중 사용되지 않는 열이라도 유지)\n",
    "args.remove_unused_columns = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb8111c8-ac97-4cf0-94e6-340760b33792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트와 이미지 쌍을 인코딩하기 위한 데이터 collator 함수 정의\n",
    "def collate_fn(examples):\n",
    "    # 각 예제에서 텍스트와 이미지를 추출하고, 텍스트는 채팅 템플릿을 적용\n",
    "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "    image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "    # 텍스트를 토크나이징하고 이미지를 처리하여 일괄 처리(batch) 형태로 변환\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # labels로 사용할 input_ids 복사본 생성 후, 패딩 토큰을 -100으로 설정하여 손실 계산 시 무시하도록 함\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # 패딩 토큰 손실 계산 제외\n",
    "\n",
    "    # 특정 이미지 토큰 인덱스는 손실 계산에서 무시 (모델에 따라 다름)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  \n",
    "        # Qwen2VL 모델의 이미지 토큰 인덱스\n",
    "        image_tokens = [151652, 151653, 151655]\n",
    "    else:\n",
    "        # 다른 모델에서 이미지 토큰 ID를 얻어 손실 계산에서 제외\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "    \n",
    "    # 손실 계산 시 이미지 토큰 인덱스를 무시하도록 설정\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100\n",
    "    \n",
    "    # 배치에 labels 추가 (손실 계산 시 사용)\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a349f087-67c9-4d56-bf56-acbc4768e750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 예시 데이터:\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': '당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': \"주어진 이미지와 ##QUESTION##, ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \\n\\n##QUESTION##: 다음 주 중에서 가장 북쪽에 있는 곳은 어디인가요?\\n##CHOICES##: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\\n##HINT##: \"}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429 at 0x7FE2C8D28A10>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '웨스트버지니아'}]}]}\n"
     ]
    }
   ],
   "source": [
    "# 단일 예시 확인\n",
    "example = dataset[0]  # 데이터셋의 첫 번째 아이템\n",
    "print(\"단일 예시 데이터:\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb461049-7b7d-40a4-967e-76d57e95e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 558])\n",
      "어텐션 마스크 형태: torch.Size([1, 558])\n",
      "이미지 픽셀 형태: torch.Size([1620, 1176])\n",
      "레이블 형태: torch.Size([1, 558])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"이미지 픽셀 형태:\", batch[\"pixel_values\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c15c695-c95e-4345-a898-25248bf37eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  64795,  82528,  33704,  90667,  21329,  80573,\n",
      "        126674,  18411,  63332,  34395, 126674,  18411, 131417, 132526,  16560,\n",
      "         15235,  21388,  78952,     13, 151645,    198, 151644,    872,    198,\n",
      "         54330,  31079,  85251,  90667,  21329,  80573,   7704,  52428,    565,\n",
      "            11,   7704,   1143,     46,  15835,    565,     11,   7704,     39,\n",
      "          3221,    565,  18411,  63332,  34395,  36055, 132760,  17877, 131417,\n",
      "        132526,  16560,  69192,    247,  37087, 129392,  40281,  56039,  78952,\n",
      "            13, 130887, 135968, 126550,  23573,  36055, 132760,  17877, 131417,\n",
      "        132526,  50302,     13,   4710,    565,  52428,    565,     25, 126844,\n",
      "         55673,  70943,  56475, 130887, 139963, 132064,  19391,  64521,  45130,\n",
      "           111,  33704, 139740,  31328,  19969,  35711,   5267,    565,  30498,\n",
      "         15835,    565,     25,   2509, 144025,  53189,  79004,  21329,  83036,\n",
      "         52959,    516,    364, 126746,  12802,  21329, 126898,  60315,    516,\n",
      "           364, 126898,  28002,  92817,  60315,    516,    364,  34992,  44680,\n",
      "           223,    112,  50340,  47324, 125544,   4432,    565,     39,   3221,\n",
      "           565,     25,    220, 151652, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151653, 151645,    198, 151644,  77091,\n",
      "           198, 144025,  53189,  79004,  21329,  83036,  52959, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e79ead1-5782-41c9-a818-2b86a68b0eed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  64795,  82528,  33704,  90667,  21329,  80573,\n",
      "        126674,  18411,  63332,  34395, 126674,  18411, 131417, 132526,  16560,\n",
      "         15235,  21388,  78952,     13, 151645,    198, 151644,    872,    198,\n",
      "         54330,  31079,  85251,  90667,  21329,  80573,   7704,  52428,    565,\n",
      "            11,   7704,   1143,     46,  15835,    565,     11,   7704,     39,\n",
      "          3221,    565,  18411,  63332,  34395,  36055, 132760,  17877, 131417,\n",
      "        132526,  16560,  69192,    247,  37087, 129392,  40281,  56039,  78952,\n",
      "            13, 130887, 135968, 126550,  23573,  36055, 132760,  17877, 131417,\n",
      "        132526,  50302,     13,   4710,    565,  52428,    565,     25, 126844,\n",
      "         55673,  70943,  56475, 130887, 139963, 132064,  19391,  64521,  45130,\n",
      "           111,  33704, 139740,  31328,  19969,  35711,   5267,    565,  30498,\n",
      "         15835,    565,     25,   2509, 144025,  53189,  79004,  21329,  83036,\n",
      "         52959,    516,    364, 126746,  12802,  21329, 126898,  60315,    516,\n",
      "           364, 126898,  28002,  92817,  60315,    516,    364,  34992,  44680,\n",
      "           223,    112,  50340,  47324, 125544,   4432,    565,     39,   3221,\n",
      "           565,     25,    220,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100, 151645,    198, 151644,  77091,\n",
      "           198, 144025,  53189,  79004,  21329,  83036,  52959, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54f93c39-a403-4ad5-808c-5d2d9382ee76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디코딩된 텍스트:\n",
      "<|im_start|>system\n",
      "당신은 이미지와 문제를 보고 문제를 맞추는 AI Assistant입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "주어진 이미지와 ##QUESTION##, ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 가장 적절한 정답을 맞추세요. \n",
      "\n",
      "##QUESTION##: 다음 주 중에서 가장 북쪽에 있는 곳은 어디인가요?\n",
      "##CHOICES##: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\n",
      "##HINT##: <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "웨스트버지니아<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 디코딩 예시 (입력 텍스트가 어떻게 변환되었는지 확인)\n",
    "decoded_text = processor.tokenizer.decode(batch[\"input_ids\"][0])\n",
    "print(\"\\n디코딩된 텍스트:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4567378a-89d7-44d4-ad0c-e093427a3cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/science/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/root/anaconda3/envs/science/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/science/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/science/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    # 앞서 로드한 Qwen2-VL 모델\n",
    "    model=model,                   \n",
    "    # SFTConfig를 통해 정의한 학습 설정\n",
    "    args=args,                      \n",
    "    # 학습에 사용할 데이터셋\n",
    "    train_dataset=train_dataset,    \n",
    "    # 데이터 배치 처리를 위한 collator 함수\n",
    "    data_collator=collate_fn,       \n",
    "    # 텍스트 필드 지정 (커스텀 collator 사용으로 빈 값)\n",
    "    dataset_text_field=\"\",          \n",
    "    # LoRA 파인튜닝 설정\n",
    "    peft_config=peft_config,        \n",
    "    # 텍스트 토크나이징을 위한 토크나이저\n",
    "    tokenizer=processor.tokenizer,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d22539-530b-4474-9d78-883107717dec",
   "metadata": {},
   "source": [
    "Trainer 인스턴스의 train() 메서드를 호출하여 모델 학습을 시작합니다.  \n",
    "이렇게 하면 학습 루프가 시작되고 3 에폭 동안 모델이 학습됩니다. PEFT 방법을 사용하고 있기 때문에 전체 모델이 아닌 조정된 모델 가중치만 저장할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28f037e1-c34c-402c-8d94-d49269c50292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/science/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1398' max='1398' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1398/1398 56:51, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.559300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.567900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.426800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.372400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.328500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.306100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.266300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.324100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>0.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>0.243400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>0.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>0.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>0.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>0.305300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>0.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>0.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>0.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>0.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>0.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>0.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>0.250300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>0.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>0.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>0.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065</td>\n",
       "      <td>0.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>0.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095</td>\n",
       "      <td>0.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>0.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135</td>\n",
       "      <td>0.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145</td>\n",
       "      <td>0.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>0.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185</td>\n",
       "      <td>0.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195</td>\n",
       "      <td>0.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205</td>\n",
       "      <td>0.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235</td>\n",
       "      <td>0.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.202900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285</td>\n",
       "      <td>0.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295</td>\n",
       "      <td>0.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345</td>\n",
       "      <td>0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355</td>\n",
       "      <td>0.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.173800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385</td>\n",
       "      <td>0.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395</td>\n",
       "      <td>0.212800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/root/anaconda3/envs/science/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작, 모델은 자동으로 허브와 출력 디렉토리에 저장됨\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7db6ce71-f50c-4595-b534-baf14aa9021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 비우기\n",
    "del model\n",
    "del processor\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493b6f7-8711-498e-b9b4-c5c54d290241",
   "metadata": {},
   "source": [
    "## 인퍼런스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955e49d-3dfc-41f0-9646-71f1eb1bb632",
   "metadata": {},
   "source": [
    "학습이 완료된 후에는 모델을 평가하고 테스트해볼 예정입니다.  \n",
    "\n",
    "먼저 기본 모델을 불러와서 임의의 아마존 제품에 대한 설명을 생성해보고, 그 다음 Q-LoRA로 조정된 모델을 불러와 같은 제품에 대한 설명을 생성해볼 것입니다.  \n",
    "마지막으로 더 효율적인 추론을 위해 어댑터를 기본 모델과 병합한 뒤, 동일한 제품에 대해 다시 한 번 추론을 실행해볼 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff2460c2-f0fe-45b5-a706-c36f6d181dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2788d0eb-7739-44ea-89a8-0ca97728fe1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    " \n",
    "# 기본 모델 호출\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c2f9a-cfef-4e1c-ba23-492e892f422c",
   "metadata": {},
   "source": [
    "학습을 하지 않은 상태에서 예측을 하면 622개 중 226개를 맞추는 것을 확인할 수 있습니다.   \n",
    "이번에는 학습한 로라 어댑터를 불어와서 예측해보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2915dd-63ee-445e-b59d-311efda70d7d",
   "metadata": {},
   "source": [
    "### 학습한 모델로 inference 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94ced566-c1b3-438b-8b73-ffad356a5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로라 어댑터가 있는 경로\n",
    "adapter_path = \"/root/trl/qwen2-7b-instruct-homeworks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6eb17220-fd25-4ae6-bf99-37d13ba25add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로라 모델을 불러오기 \n",
    "model.load_adapter(adapter_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a85779d1-ad4f-4ab0-a27a-959113b6f53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주어진 이미지에서 보이는 주 중에서 동쪽에 있는 곳은 '노스다코타'입니다. 따라서 정답은 '노스다코타'입니다.\n"
     ]
    }
   ],
   "source": [
    "# 1개 예측해보기 \n",
    "ft_description = generate_description(messages, model, processor)\n",
    "print(ft_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "771b5498-4d72-4309-8f48-d4d52cd94f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [03:10<00:00,  3.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# 학습한 모델로 예측하기 \n",
    "results = [] \n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    messages = test_dataset[idx][\"messages\"][:2]\n",
    "    ft_description = generate_description(messages, model, processor)\n",
    "    results.append(ft_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43c7b25a-21f9-442d-806c-fd9eaa654067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [00:00<00:00, 466867.77it/s]\n"
     ]
    }
   ],
   "source": [
    "answers = [] \n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    answers.append(test_dataset[idx][\"messages\"][2][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3618c6df-4c2f-41f0-823e-b3688650e713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_score = 0 \n",
    "for result, answer in zip(results, answers):\n",
    "    if result == answer : \n",
    "        ft_score += 1\n",
    "ft_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d899354-d546-4c72-9d66-526a7d3170c2",
   "metadata": {},
   "source": [
    "### 결과 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5466291-b7f6-47dc-a2b0-1bd31cf89610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 전 정확도 : 33.12%\n",
      "학습 후 정답률 : 80.97%\n"
     ]
    }
   ],
   "source": [
    "print(f\"학습 전 정확도 : {base_score:.2f}%\") \n",
    "print(f\"학습 후 정답률 : {ft_score/662*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61ce42-4217-49a0-b309-7107c7de31ab",
   "metadata": {},
   "source": [
    "## 로라 병합 후 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a951c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 460\n",
      "-rw-r--r-- 1 root     32 Jan 15 04:58  README.md\n",
      "-rw-r--r-- 1 root    497 Jan 15 04:58  deepspeed_zero3.yaml\n",
      "-rw-r--r-- 1 root 233224 Jan 15 04:58 'llm tuning example.ipynb'\n",
      "drwxr-xr-x 5 root   4096 Jan 15 08:23  \u001b[0m\u001b[01;34mqwen2-7b-instruct-homeworks\u001b[0m/\n",
      "-rw-r--r-- 1 root   3451 Jan 15 04:58  sft.py\n",
      "-rw-r--r-- 1 root   6065 Jan 15 04:58  sft_vlm.py\n",
      "-rw-r--r-- 1 root  99168 Jan 15 12:25 'vlm tuning example(ko-scienceQA).ipynb'\n",
      "-rw-r--r-- 1 root 110276 Jan 15 04:58 'vlm tuning example.ipynb'\n"
     ]
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c403983e-ef72-48ea-9376-0bfaa529c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "adapter_path = \"./qwen2-7b-instruct-homeworks\"  # 학습된 어댑터 경로\n",
    "base_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"  # 기본 모델 ID\n",
    "merged_path = \"merged\"  # 병합된 모델을 저장할 경로\n",
    "\n",
    "# 기본 모델 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(base_model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# 병합된 모델 저장 경로\n",
    "# LoRA와 기본 모델을 병합하고 저장\n",
    "peft_model = PeftModel.from_pretrained(model, adapter_path)  # PEFT 모델 로드\n",
    "merged_model = peft_model.merge_and_unload()  # 모델 병합\n",
    "merged_model.save_pretrained(merged_path, safe_serialization=True, max_shard_size=\"5GB\")  # 병합된 모델 저장\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)  # 프로세서 로드\n",
    "processor.save_pretrained(merged_path)  # 프로세서 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a8791-31fb-409e-8593-6fcba5c99ee0",
   "metadata": {},
   "source": [
    "## 허깅페이스에 모델 업로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4bdc68c0-5d4f-47b7-bcd0-64bacf050327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# 병합된 모델 불러오기 \n",
    "local_model_path = \"/root/trl/merged\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(local_model_path)\n",
    "processor = AutoProcessor.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661c48a-9b1b-4ef6-ac64-158a5f91525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `huggingface` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `huggingface`\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 로그인 \n",
    "!huggingface-cli login --token Your_Huggingface_Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff57e0-eb1e-45ae-973d-af1717538d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload-large-folder --repo-type=model daje/Qwen2-VL-7B-instruct-ScienceQA /root/trl/merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6f6f8-daf9-4c4d-bc4e-1aec70498b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

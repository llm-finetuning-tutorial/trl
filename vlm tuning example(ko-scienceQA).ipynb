{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89a1820-52b3-4f1b-8aa8-ac57b383d873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.4.99)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.26.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.28.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.45.1 in /usr/local/lib/python3.11/dist-packages (4.45.1)\n",
      "Requirement already satisfied: datasets==3.0.1 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
      "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.11/dist-packages (0.34.2)\n",
      "Requirement already satisfied: evaluate==0.4.3 in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: bitsandbytes==0.44.0 in /usr/local/lib/python3.11/dist-packages (0.44.0)\n",
      "Requirement already satisfied: trl==0.11.1 in /usr/local/lib/python3.11/dist-packages (0.11.1)\n",
      "Requirement already satisfied: peft==0.13.0 in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
      "Requirement already satisfied: qwen-vl-utils in /usr/local/lib/python3.11/dist-packages (0.0.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (4.67.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.1) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.1) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.0.1) (3.10.10)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl==0.11.1) (0.8.14)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (13.1.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (11.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.1) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.1) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.1) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.1) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.4.99)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.1) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.1) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.1) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.0.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.0.1) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.18.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch==2.4.0\" tensorboard pillow\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26e0361-81a8-44cf-9cf1-6cf9da30e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dee226-cb1e-4473-bc20-ebb5f6f38fcc",
   "metadata": {},
   "source": [
    "## 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ee39e-d9d6-44ee-b6a8-acf410fedd71",
   "metadata": {},
   "source": [
    "과제물 사진과 과제물 정보를 주고 실제 정답을 맞추는 모델을 개발합니다. \n",
    "\n",
    "이 모델은 선생님들이 학생들의 수행평가를 채점한다고 가정합니다. \n",
    "본 프로젝트는 실제 교육 현장의 데이터와 차이가 있을 수 있으나, 자동 채점 시스템의 기본 원리와 가능성을 탐구하는 데 중점을 둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a4d54-537a-4dae-9386-2e66b8e8e67e",
   "metadata": {},
   "source": [
    "이번 예시에서는 Ko-SciecneQA 데이터셋을 사용할 건데요. 이 데이터셋은 12,726개의 아마존 제품의 제목, 이미지, 설명 및 메타데이터를 포함하고 있습니다.  \n",
    "이 중에서 이미지를 가지고 있는 6,218개의 데이터 중 시간 절약을 위해서 모두 사용하지는 않고 여기서 20%(1,243)만 사용하겠습니다.\n",
    "\n",
    "이미지, 문제, 힌트을 기반으로 정답을 생성하도록 모델을 파인튜닝하려 합니다.  \n",
    "따라서 이미지, 문제, 힌트를 포함한 입력을 만들고, 이를 이용하여 정답을 찾아보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af977a22-1d6a-4977-a0f2-e2c5508695c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: 이미지는 프롬프트에 직접 제공되지 않고 \"processor\"의 일부로 포함됨\n",
    "prompt= \"\"\"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 정답을 맞추세요. \n",
    "\n",
    "##CHOICES##: {korean_choices}\n",
    "##HINT##: {korean_hint}\"\"\"\n",
    "\n",
    "system_message = \"당신은 숙제 도우미입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f90aee6-602f-409e-a80f-3b06d855d202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': '당신은 숙제 도우미입니다.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': \"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 정답을 맞추세요. \\n\\n##CHOICES##: ['신시내티', '링컨', '클리블랜드', '콜럼버스']\\n##HINT##: \"}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429 at 0x7A077DC9F5D0>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '콜럼버스'}]}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환하는 함수      \n",
    "def format_data(sample):\n",
    "   return {\"messages\": [\n",
    "               {\n",
    "                   \"role\": \"system\", # 시스템 역할\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": system_message}], # 시스템 메시지\n",
    "               },\n",
    "               {\n",
    "                   \"role\": \"user\",  # 사용자 역할\n",
    "                   \"content\": [\n",
    "                       {\n",
    "                           \"type\": \"text\",\n",
    "                           # 제품명과 카테고리를 포함한 프롬프트 생성\n",
    "                           \"text\": prompt.format(korean_choices=sample[\"korean_choices\"], korean_hint=sample[\"korean_hint\"]),\n",
    "                       },{\n",
    "                           \"type\": \"image\", # 이미지 타입\n",
    "                           \"image\": sample[\"image\"] if sample[\"image\"] is not None else \"\", # 제품 이미지\n",
    "                       }\n",
    "                   ],\n",
    "               },\n",
    "               {\n",
    "                   \"role\": \"assistant\", # AI 어시스턴트 역할\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": sample[\"answer_str\"]}], # 제품 설명\n",
    "               },\n",
    "           ],\n",
    "       }\n",
    "\n",
    "# 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"daje/Ko-SciecneQA\", split=\"train\")\n",
    "dataset = dataset.filter(lambda example: example[\"image\"] is not None)\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환\n",
    "# PIL.Image 타입을 유지하기 위해 리스트 컴프리헨션 사용 (.map()은 이미지를 바이트로 변환해버림)\n",
    "dataset = [format_data(sample) for sample in dataset]\n",
    "\n",
    "print(dataset[345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58727288-4b2d-4433-9251-4b0780b4853c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6218,\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': [{'type': 'text', 'text': '당신은 숙제 도우미입니다.'}]},\n",
       "   {'role': 'user',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': \"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 정답을 맞추세요. \\n\\n##CHOICES##: ['신시내티', '링컨', '클리블랜드', '콜럼버스']\\n##HINT##: \"},\n",
       "     {'type': 'image',\n",
       "      'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429>}]},\n",
       "   {'role': 'assistant', 'content': [{'type': 'text', 'text': '콜럼버스'}]}]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b1d12d-ced0-433d-9be9-b9fe7d6d6c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수: 1243\n"
     ]
    }
   ],
   "source": [
    "# 위에 준 코드 실행 후\n",
    "dataset = dataset[:int(len(dataset) * 0.2)]  # 이미 포맷된 dataset에서 20%만 슬라이싱\n",
    "\n",
    "print(f\"데이터 개수: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed2dfd-268e-4d41-8873-2539df77131b",
   "metadata": {},
   "source": [
    "## trl의 SFTTrainer를 이용한 파인 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a92ab-c684-4885-a215-0be96ad28147",
   "metadata": {},
   "source": [
    "`trl`의 SFTTrainer를 사용해 모델을 파인튜닝할 건데요. SFTTrainer는 오픈소스 LLM과 VLM의 지도 파인튜닝을 매우 간단하게 만들어줍니다.  \n",
    "SFTTrainer는 `transformers` 라이브러리의 `Trainer`를 상속받아서 로깅, 평가, 체크포인트 등 모든 기능을 지원하면서도 추가적인 편의 기능을 제공합니다.\n",
    "\n",
    "이번 예시에서는 PEFT 기능을 사용할 예정입니다. PEFT 방법으로는 QLoRA를 사용할 건데, 이는 양자화와 LoRA 튜닝을 같이 사용하여 대규모 언어 모델의 메모리 사용량을 줄이는 기술입니다.\n",
    "\n",
    "* 참고: 멀티모달 입력에 패딩이 필요하기 때문에 Flash Attention은 사용할 수 없습니다.*\n",
    "\n",
    "Qwen 2 VL 7B 모델을 사용할 예정이지만, `model_id` 변수만 바꾸면 Meta AI의 Llama-3.2-11B-Vision, Mistral AI의 Pixtral-12B 등 다른 모델로도 쉽게 교체할 수 있습니다. bitsandbytes를 사용해 모델을 4비트로 양자화할 예정입니다.\n",
    "\n",
    "* 참고: 모델이 클수록 더 많은 메모리가 필요합니다. 이번 예시에서는 7B 모델을 사용할 예정입니다.*\n",
    "\n",
    "VLM 학습을 위해 LLM, 토크나이저, 프로세서를 올바르게 준비하는 것이 매우 중요합니다. 프로세서는 특수 토큰과 이미지를 입력에 포함시키는 역할을 담당하는 모듈입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28fa661e-22e5-4843-87de-023ee47f71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85731e6d015b4feb876e61d50761bb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\" \n",
    "\n",
    "# BitsAndBytes 4비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,                             # 4비트 양자화 사용\n",
    "   bnb_4bit_use_double_quant=True,               # 이중 양자화 사용으로 메모리 추가 절약\n",
    "   bnb_4bit_quant_type=\"nf4\",                    # 4비트 양자화 타입 설정(normalized float 4)\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16         # 연산 시 bfloat16 타입 사용\n",
    ")\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "   model_id,\n",
    "   device_map=\"auto\",                            # GPU 메모리에 자동 할당\n",
    "   # attn_implementation=\"flash_attention_2\",     # 학습시에는 flash attention 2 미지원\n",
    "   torch_dtype=torch.bfloat16,                   # bfloat16 정밀도 사용\n",
    "   quantization_config=bnb_config                # 위에서 정의한 양자화 설정 적용\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)  # 텍스트/이미지 전처리기 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29d040-3ca9-4638-a8ae-f7d34bb461c6",
   "metadata": {},
   "source": [
    "다음은 QWEN의 템플릿 예시입니다.  \n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|im_end|>\n",
    "<|im_start|>assistant\n",
    "거대 언어 모델의 답변<|im_end|>\n",
    "```\n",
    "\n",
    "멀티모달 QWEN은 이렇게 사용할 겁니다.\n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|vision_start|>이미지<|vision_end|><|im_end|>\n",
    "<|im_start|>assistant\n",
    "거대 언어 모델의 답변<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e4a9eb-0886-41de-a168-6f045624825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 숙제 도우미입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 정답을 맞추세요. \n",
      "\n",
      "##CHOICES##: ['Kathleen의 스노보드가 왁스 층이 있을 때와 없을 때 중 어느 경우에 언덕을 더 빨리 내려오는가?', 'Kathleen의 스노보드가 얇은 왁스 층이 있을 때와 두꺼운 왁스 층이 있을 때 중 어느 경우에 언덕을 더 빨리 내려오는가?']\n",
      "##HINT##: 아래의 글은 실험을 설명합니다. 글을 읽고 아래의 지시를 따르세요.\n",
      "\n",
      "Kathleen은 스노보드의 밑면에 얇은 왁스 층을 바르고 언덕을 곧장 내려갔습니다. 그런 다음, 왁스를 제거하고 다시 스노보드를 타고 언덕을 내려갔습니다. 그녀는 왁스를 바른 상태와 바르지 않은 상태를 번갈아 가며 네 번 더 반복했습니다. 그녀의 친구 Bryant는 각 타이밍을 측정했습니다. Kathleen과 Bryant는 왁스를 바른 스노보드로 언덕을 내려가는 평균 시간과 왁스를 바르지 않은 스노보드로 내려가는 평균 시간을 계산했습니다.\n",
      "그림: 언덕을 내려가는 스노보드.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "Kathleen의 스노보드가 왁스 층이 있을 때와 없을 때 중 어느 경우에 언덕을 더 빨리 내려오는가?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    dataset[2][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2acd65-fc31-4427-9d64-228efabdb5a4",
   "metadata": {},
   "source": [
    "SFTTrainer는 peft와 기본적으로 통합되어 있어 LoraConfig를 만들어서 트레이너에 제공하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4890201-2cba-4de3-a5ed-bad569406b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'],\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44497f73-c125-49d3-9530-9b5918ea75cb",
   "metadata": {},
   "source": [
    "학습을 시작하기 전에 사용할 하이퍼파라미터(SFTConfig)를 정의하고 입력이 모델에 올바르게 제공되는지 확인해야 합니다.  \n",
    "\n",
    "텍스트만 사용하는 지도 파인튜닝과 달리 모델에 이미지도 함께 제공해야 하는데요. 이를 위해 입력을 올바르게 포맷팅하고 이미지 특징을 포함하는 커스텀 DataCollator를 만들어야 합니다.  \n",
    "\n",
    "Qwen2 팀이 제공하는 유틸리티 패키지의 process_vision_info 메서드를 사용할 예정입니다. Llama 3.2 Vision 같은 다른 모델을 사용하는 경우라면, 동일한 방식으로 이미지 정보가 처리되는지 확인해봐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6820033-24b8-4cf3-9514-2a7956fa26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# SFTConfig를 통해 학습 설정을 정의\n",
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-instruct-homeworks\",          # 학습된 모델과 체크포인트를 저장할 디렉터리 경로 및 리포지토리 ID\n",
    "    num_train_epochs=3,                                # 전체 학습 에포크 수 (데이터셋을 몇 번 반복할지 설정)\n",
    "    per_device_train_batch_size=8,                     # 각 장비(GPU)당 사용될 배치 사이즈 (메모리와 연관됨)\n",
    "    gradient_accumulation_steps=8,                     # 경사 누적 스텝 수 (이 횟수만큼 기울기를 누적한 후 업데이트)\n",
    "    gradient_checkpointing=True,                       # 메모리 절약을 위한 gradient checkpointing 활성화 (메모리 최적화)\n",
    "    optim=\"adamw_torch_fused\",                         # AdamW 옵티마이저 (fused 버전 사용으로 학습 속도 향상)\n",
    "    logging_steps=5,                                   # 몇 스텝마다 로그를 출력할지 설정 (여기선 5 스텝마다 로그)\n",
    "    save_strategy=\"epoch\",                             # 매 에포크마다 체크포인트 저장 설정\n",
    "    learning_rate=1e-5,                                # 학습률 (QLoRA 논문에서 추천된 값 사용)\n",
    "    bf16=True,                                         # bfloat16 정밀도 사용 (메모리 절약 및 속도 향상)\n",
    "    tf32=True,                                         # tf32 정밀도 사용 (NVIDIA GPU에서 학습 속도 향상)\n",
    "    max_grad_norm=0.3,                                 # 기울기 클리핑을 위한 최대 기울기 값 (QLoRA 논문에서 추천된 값)\n",
    "    warmup_ratio=0.03,                                 # 학습 초기에 학습률을 점진적으로 올리는 warmup 비율 (QLoRA 논문에서 추천된 값)\n",
    "    lr_scheduler_type=\"constant\",                      # 일정한 학습률 스케줄러 사용 (학습률이 변하지 않음)\n",
    "    # push_to_hub=True,                                  # 학습된 모델을 Hugging Face Hub에 푸시할지 여부\n",
    "    report_to=\"tensorboard\",                           # TensorBoard를 통해 학습 상태를 모니터링\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # reentrant gradient checkpointing 설정 (비재진입 방식 사용)\n",
    "    dataset_text_field=\"\",                             # 데이터셋에서 텍스트 필드를 위한 더미 필드 (collator에서 필요)\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}      # collator에서 데이터셋 전처리를 건너뛰기 위한 설정\n",
    ")\n",
    "\n",
    "# 불필요한 열 삭제하지 않도록 설정 (학습 중 사용되지 않는 열이라도 유지)\n",
    "args.remove_unused_columns = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb8111c8-ac97-4cf0-94e6-340760b33792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트와 이미지 쌍을 인코딩하기 위한 데이터 collator 함수 정의\n",
    "def collate_fn(examples):\n",
    "    # 각 예제에서 텍스트와 이미지를 추출하고, 텍스트는 채팅 템플릿을 적용\n",
    "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "    image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "    # 텍스트를 토크나이징하고 이미지를 처리하여 일괄 처리(batch) 형태로 변환\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # labels로 사용할 input_ids 복사본 생성 후, 패딩 토큰을 -100으로 설정하여 손실 계산 시 무시하도록 함\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # 패딩 토큰 손실 계산 제외\n",
    "\n",
    "    # 특정 이미지 토큰 인덱스는 손실 계산에서 무시 (모델에 따라 다름)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  \n",
    "        # Qwen2VL 모델의 이미지 토큰 인덱스\n",
    "        image_tokens = [151652, 151653, 151655]\n",
    "    else:\n",
    "        # 다른 모델에서 이미지 토큰 ID를 얻어 손실 계산에서 제외\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "    \n",
    "    # 손실 계산 시 이미지 토큰 인덱스를 무시하도록 설정\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100\n",
    "    \n",
    "    # 배치에 labels 추가 (손실 계산 시 사용)\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a349f087-67c9-4d56-bf56-acbc4768e750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 예시 데이터:\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': '당신은 숙제 도우미입니다.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': \"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 정답을 맞추세요. \\n\\n##CHOICES##: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\\n##HINT##: \"}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429 at 0x7A07A98002D0>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '웨스트버지니아'}]}]}\n"
     ]
    }
   ],
   "source": [
    "# 단일 예시 확인\n",
    "example = dataset[0]  # 데이터셋의 첫 번째 아이템\n",
    "print(\"단일 예시 데이터:\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb461049-7b7d-40a4-967e-76d57e95e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 521])\n",
      "어텐션 마스크 형태: torch.Size([1, 521])\n",
      "이미지 픽셀 형태: torch.Size([1620, 1176])\n",
      "레이블 형태: torch.Size([1, 521])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"이미지 픽셀 형태:\", batch[\"pixel_values\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c15c695-c95e-4345-a898-25248bf37eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  64795,  82528,  33704,  69192,    247,  37087,\n",
      "        129392,  40281,  56039,  78952,     13, 151645,    198, 151644,    872,\n",
      "           198,  54330,  31079,  85251,  90667,  21329,  80573,   7704,   1143,\n",
      "            46,  15835,    565,     11,   7704,     39,   3221,    565,  18411,\n",
      "         63332,  34395,  36055, 132760,  17877, 131417, 132526,  16560,  69192,\n",
      "           247,  37087, 129392,  40281,  56039,  78952,     13,  36055, 132760,\n",
      "         17877, 131417, 132526,  50302,     13,   4710,    565,  30498,  15835,\n",
      "           565,     25,   2509, 144025,  53189,  79004,  21329,  83036,  52959,\n",
      "           516,    364, 126746,  12802,  21329, 126898,  60315,    516,    364,\n",
      "        126898,  28002,  92817,  60315,    516,    364,  34992,  44680,    223,\n",
      "           112,  50340,  47324, 125544,   4432,    565,     39,   3221,    565,\n",
      "            25,    220, 151652, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151653, 151645,    198, 151644,  77091,    198,\n",
      "        144025,  53189,  79004,  21329,  83036,  52959, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e79ead1-5782-41c9-a818-2b86a68b0eed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  64795,  82528,  33704,  69192,    247,  37087,\n",
      "        129392,  40281,  56039,  78952,     13, 151645,    198, 151644,    872,\n",
      "           198,  54330,  31079,  85251,  90667,  21329,  80573,   7704,   1143,\n",
      "            46,  15835,    565,     11,   7704,     39,   3221,    565,  18411,\n",
      "         63332,  34395,  36055, 132760,  17877, 131417, 132526,  16560,  69192,\n",
      "           247,  37087, 129392,  40281,  56039,  78952,     13,  36055, 132760,\n",
      "         17877, 131417, 132526,  50302,     13,   4710,    565,  30498,  15835,\n",
      "           565,     25,   2509, 144025,  53189,  79004,  21329,  83036,  52959,\n",
      "           516,    364, 126746,  12802,  21329, 126898,  60315,    516,    364,\n",
      "        126898,  28002,  92817,  60315,    516,    364,  34992,  44680,    223,\n",
      "           112,  50340,  47324, 125544,   4432,    565,     39,   3221,    565,\n",
      "            25,    220,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100, 151645,    198, 151644,  77091,    198,\n",
      "        144025,  53189,  79004,  21329,  83036,  52959, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54f93c39-a403-4ad5-808c-5d2d9382ee76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디코딩된 텍스트:\n",
      "<|im_start|>system\n",
      "당신은 숙제 도우미입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 정답을 맞추세요. \n",
      "\n",
      "##CHOICES##: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\n",
      "##HINT##: <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "웨스트버지니아<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 디코딩 예시 (입력 텍스트가 어떻게 변환되었는지 확인)\n",
    "decoded_text = processor.tokenizer.decode(batch[\"input_ids\"][0])\n",
    "print(\"\\n디코딩된 텍스트:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4567378a-89d7-44d4-ad0c-e093427a3cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    "    dataset_text_field=\"\",\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d22539-530b-4474-9d78-883107717dec",
   "metadata": {},
   "source": [
    "Trainer 인스턴스의 train() 메서드를 호출하여 모델 학습을 시작합니다.  \n",
    "이렇게 하면 학습 루프가 시작되고 3 에폭 동안 모델이 학습됩니다. PEFT 방법을 사용하고 있기 때문에 전체 모델이 아닌 조정된 모델 가중치만 저장할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f037e1-c34c-402c-8d94-d49269c50292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 11:46, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.361300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작, 모델은 자동으로 허브와 출력 디렉토리에 저장됨\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7db6ce71-f50c-4595-b534-baf14aa9021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 비우기\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493b6f7-8711-498e-b9b4-c5c54d290241",
   "metadata": {},
   "source": [
    "## 인퍼런스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955e49d-3dfc-41f0-9646-71f1eb1bb632",
   "metadata": {},
   "source": [
    "학습이 완료된 후에는 모델을 평가하고 테스트해볼 예정입니다.  \n",
    "\n",
    "먼저 기본 모델을 불러와서 임의의 아마존 제품에 대한 설명을 생성해보고, 그 다음 Q-LoRA로 조정된 모델을 불러와 같은 제품에 대한 설명을 생성해볼 것입니다.  \n",
    "마지막으로 더 효율적인 추론을 위해 어댑터를 기본 모델과 병합한 뒤, 동일한 제품에 대해 다시 한 번 추론을 실행해볼 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2788d0eb-7739-44ea-89a8-0ca97728fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2b85ea768944d68ea70e146edda279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    " \n",
    "# 기본 모델 호출\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eb57a5e-7cdc-4d2c-a8cd-a4cda738b95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text', 'text': '당신은 숙제 도우미입니다.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"주어진 이미지와 ##ChOICES##, ##HINT##를 보고 정답을 맞추는 숙제 도우미입니다. 정답을 맞추세요. \\n\\n##CHOICES##: ['둘 다 아님; 샘플의 온도는 동일함', '샘플 A', '샘플 B']\\n##HINT##: 아래 다이어그램은 동일한 닫힌, 단단한 용기에 있는 두 순수한 가스 샘플을 보여줍니다. 각 색깔의 공은 하나의 가스 입자를 나타냅니다. 두 샘플은 동일한 수의 입자를 가지고 있습니다.\"},\n",
       "    {'type': 'image',\n",
       "     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=563x405>}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': '샘플 B'}]}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset[int(len(dataset) * 0.2):] \n",
    "test_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee8d5bc4-1f96-495f-8dba-6e1a5dc3147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 답변을 생성하는 함수\n",
    "def generate_description(messages, model, processor):\n",
    "   # 추론을 위한 준비\n",
    "   text = processor.apply_chat_template(\n",
    "       messages, tokenize=False, add_generation_prompt=True\n",
    "   )\n",
    "   image_inputs, video_inputs = process_vision_info(messages)\n",
    "   inputs = processor(\n",
    "       text=[text],\n",
    "       images=image_inputs,\n",
    "       videos=video_inputs,\n",
    "       padding=True,\n",
    "       return_tensors=\"pt\",\n",
    "   )\n",
    "   inputs = inputs.to(model.device)\n",
    "   # 추론: 출력 생성\n",
    "   generated_ids = model.generate(**inputs, max_new_tokens=256, top_p=1.0, do_sample=True, temperature=0.8)\n",
    "   generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "   output_text = processor.batch_decode(\n",
    "       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "   )\n",
    "   return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af2c867f-da30-41aa-b1b9-b9da5f7130d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 A\n"
     ]
    }
   ],
   "source": [
    "# 설명 생성해보기\n",
    "messages =  test_dataset[0][\"messages\"]\n",
    "base_description = generate_description(messages, model, processor)\n",
    "print(base_description)\n",
    "# 원하는 경우 아래 명령어로 활성화된 어댑터를 비활성화할 수 있음\n",
    "# model.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94ced566-c1b3-438b-8b73-ffad356a5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로라 어댑터가 있는 경로\n",
    "adapter_path = \"./qwen2-7b-instruct-homeworks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eb17220-fd25-4ae6-bf99-37d13ba25add",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_adapter(adapter_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a85779d1-ad4f-4ab0-a27a-959113b6f53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 A\n"
     ]
    }
   ],
   "source": [
    "ft_description = generate_description(messages, model, processor)\n",
    "print(ft_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb02fee1-4ea9-4085-94ab-3b08981c1fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 학습 전 모델: 샘플 A\n",
      "---\n",
      "LoRA 학습 후 모델: 샘플 A\n"
     ]
    }
   ],
   "source": [
    "print('LoRA 학습 전 모델:', base_description)\n",
    "print('---')\n",
    "print('LoRA 학습 후 모델:', ft_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61ce42-4217-49a0-b309-7107c7de31ab",
   "metadata": {},
   "source": [
    "## 로라 병합 후 저장하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70229777-f38d-44ab-864f-1405982159f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom peft import PeftModel\\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\\n\\nadapter_path = \"./qwen2-7b-instruct-homeworks\"  # 학습된 어댑터 경로\\nbase_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"  # 기본 모델 ID\\nmerged_path = \"merged\"  # 병합된 모델을 저장할 경로\\n\\n# 기본 모델 로드\\nmodel = AutoModelForVision2Seq.from_pretrained(model_id, low_cpu_mem_usage=True)\\n\\n# 병합된 모델 저장 경로\\n# LoRA와 기본 모델을 병합하고 저장\\npeft_model = PeftModel.from_pretrained(model, adapter_path)  # PEFT 모델 로드\\nmerged_model = peft_model.merge_and_unload()  # 모델 병합\\nmerged_model.save_pretrained(merged_path,safe_serialization=True, max_shard_size=\"2GB\")  # 병합된 모델 저장\\n\\nprocessor = AutoProcessor.from_pretrained(base_model_id)  # 프로세서 로드\\nprocessor.save_pretrained(merged_path)  # 프로세서 저장\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "adapter_path = \"./qwen2-7b-instruct-homeworks\"  # 학습된 어댑터 경로\n",
    "base_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"  # 기본 모델 ID\n",
    "merged_path = \"merged\"  # 병합된 모델을 저장할 경로\n",
    "\n",
    "# 기본 모델 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# 병합된 모델 저장 경로\n",
    "# LoRA와 기본 모델을 병합하고 저장\n",
    "peft_model = PeftModel.from_pretrained(model, adapter_path)  # PEFT 모델 로드\n",
    "merged_model = peft_model.merge_and_unload()  # 모델 병합\n",
    "merged_model.save_pretrained(merged_path,safe_serialization=True, max_shard_size=\"2GB\")  # 병합된 모델 저장\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)  # 프로세서 로드\n",
    "processor.save_pretrained(merged_path)  # 프로세서 저장\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89a1820-52b3-4f1b-8aa8-ac57b383d873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (23.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m143.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, typing-extensions, triton, tensorboard-data-server, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markdown, grpcio, absl-py, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.1.0 grpcio-1.67.1 markdown-3.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 protobuf-5.28.3 tensorboard-2.18.0 tensorboard-data-server-0.7.2 torch-2.4.0 triton-3.0.0 typing-extensions-4.12.2 werkzeug-3.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==3.0.1\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate==0.4.3\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting bitsandbytes==0.44.0\n",
      "  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting trl==0.11.1\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting qwen-vl-utils\n",
      "  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.1)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.1)\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets==3.0.1)\n",
      "  Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.0.1)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers==4.45.1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets==3.0.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2023.4.0)\n",
      "Collecting aiohttp (from datasets==3.0.1)\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.1)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting av (from qwen-vl-utils)\n",
      "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (9.3.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets==3.0.1)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, shtab, safetensors, requests, regex, pyarrow, propcache, multidict, mdurl, fsspec, frozenlist, docstring-parser, dill, av, async-timeout, aiohappyeyeballs, yarl, qwen-vl-utils, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, bitsandbytes, accelerate, peft, datasets, trl, evaluate\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 av-13.1.0 bitsandbytes-0.44.0 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 evaluate-0.4.3 frozenlist-1.5.0 fsspec-2024.6.1 huggingface-hub-0.26.2 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 peft-0.13.0 propcache-0.2.0 pyarrow-18.0.0 pytz-2024.2 qwen-vl-utils-0.0.8 regex-2024.9.11 requests-2.32.3 rich-13.9.4 safetensors-0.4.5 shtab-1.7.1 tokenizers-0.20.1 tqdm-4.66.6 transformers-4.45.1 trl-0.11.1 tyro-0.8.14 tzdata-2024.2 xxhash-3.5.0 yarl-1.17.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch==2.4.0\" tensorboard pillow\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26e0361-81a8-44cf-9cf1-6cf9da30e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
      "Collecting pillow\n",
      "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m678.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pillow-11.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dee226-cb1e-4473-bc20-ebb5f6f38fcc",
   "metadata": {},
   "source": [
    "## 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ee39e-d9d6-44ee-b6a8-acf410fedd71",
   "metadata": {},
   "source": [
    "제품 이미지와 간단한 제품 정보를 바탕으로 상세한 제품 설명을 생성할 수 있는 모델을 개발합니다.\n",
    "\n",
    "이 모델은 판매자들이 상품 등록을 쉽게 할 수 있도록 이커머스 플랫폼에 사용한다고 가정합니다. 목표는 제품 설명 작성 시간을 줄이고, 설명의 품질과 일관성을 높이는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a4d54-537a-4dae-9386-2e66b8e8e67e",
   "metadata": {},
   "source": [
    "이번 예시에서는 philschmid/amazon-product-descriptions-vlm 데이터셋을 사용할 건데요. 이 데이터셋은 1,350개의 아마존 제품의 제목, 이미지, 설명 및 메타데이터를 포함하고 있습니다.  \n",
    "시간 절약을 위해서 1,350건을 모두 사용하지는 않고 여기서 20%만 사용하겠습니다.\n",
    "\n",
    "이미지, 제목, 메타 데이터(제품에 대한 간단한 설명)을 기반으로 제품 설명을 생성하도록 모델을 파인튜닝하려 합니다.  \n",
    "따라서 제목, 메타데이터, 이미지를 포함한 입력을 만들고, 이에 대한 레이블로 제품 설명을 사용할 예정입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af977a22-1d6a-4977-a0f2-e2c5508695c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: 이미지는 프롬프트에 직접 제공되지 않고 \"processor\"의 일부로 포함됨\n",
    "prompt= \"\"\"Create a Short Product description based on the provided ##PRODUCT NAME## and ##CATEGORY## and image. \n",
    "Only return description. The description should be SEO optimized and for a better mobile search experience.\n",
    "\n",
    "##PRODUCT NAME##: {product_name}\n",
    "##CATEGORY##: {category}\"\"\"\n",
    "\n",
    "system_message = \"You are an expert product description writer for Amazon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f90aee6-602f-409e-a80f-3b06d855d202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are an expert product description writer for Amazon.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Create a Short Product description based on the provided ##PRODUCT NAME## and ##CATEGORY## and image. \\nOnly return description. The description should be SEO optimized and for a better mobile search experience.\\n \\n##PRODUCT NAME##: MasterPieces Tribal Spirit Jigsaw Puzzle, The Chiefs, Featuring American Indian Tribe Traditions & Ceremonies, 1000 Pieces\\n##CATEGORY##: Toys & Games | Puzzles | Jigsaw Puzzles'}, {'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x716961535AE0>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Challenge yourself with this 1000-piece MasterPieces Tribal Spirit jigsaw puzzle!  Depicting the rich traditions and ceremonies of American Indian tribes, \"The Chiefs\" offers a stunning, culturally significant image perfect for puzzle enthusiasts.  High-quality pieces guarantee a satisfying solve.'}]}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환하는 함수      \n",
    "def format_data(sample):\n",
    "   return {\"messages\": [\n",
    "               {\n",
    "                   \"role\": \"system\", # 시스템 역할\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": system_message}], # 시스템 메시지\n",
    "               },\n",
    "               {\n",
    "                   \"role\": \"user\",  # 사용자 역할\n",
    "                   \"content\": [\n",
    "                       {\n",
    "                           \"type\": \"text\",\n",
    "                           # 제품명과 카테고리를 포함한 프롬프트 생성\n",
    "                           \"text\": prompt.format(product_name=sample[\"Product Name\"], category=sample[\"Category\"]),\n",
    "                       },{\n",
    "                           \"type\": \"image\", # 이미지 타입\n",
    "                           \"image\": sample[\"image\"], # 제품 이미지\n",
    "                       }\n",
    "                   ],\n",
    "               },\n",
    "               {\n",
    "                   \"role\": \"assistant\", # AI 어시스턴트 역할\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": sample[\"description\"]}], # 제품 설명\n",
    "               },\n",
    "           ],\n",
    "       }\n",
    "\n",
    "# 허브에서 데이터셋 로드\n",
    "dataset_id = \"philschmid/amazon-product-descriptions-vlm\"\n",
    "dataset = load_dataset(\"philschmid/amazon-product-descriptions-vlm\", split=\"train\")\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환\n",
    "# PIL.Image 타입을 유지하기 위해 리스트 컴프리헨션 사용 (.map()은 이미지를 바이트로 변환해버림)\n",
    "dataset = [format_data(sample) for sample in dataset]\n",
    "\n",
    "print(dataset[345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01b1d12d-ced0-433d-9be9-b9fe7d6d6c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수: 269\n"
     ]
    }
   ],
   "source": [
    "# 위에 준 코드 실행 후\n",
    "dataset = dataset[:int(len(dataset) * 0.2)]  # 이미 포맷된 dataset에서 20%만 슬라이싱\n",
    "\n",
    "print(f\"데이터 개수: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed2dfd-268e-4d41-8873-2539df77131b",
   "metadata": {},
   "source": [
    "## trl의 SFTTrainer를 이용한 파인 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a92ab-c684-4885-a215-0be96ad28147",
   "metadata": {},
   "source": [
    "`trl`의 SFTTrainer를 사용해 모델을 파인튜닝할 건데요. SFTTrainer는 오픈소스 LLM과 VLM의 지도 파인튜닝을 매우 간단하게 만들어줍니다.  \n",
    "SFTTrainer는 `transformers` 라이브러리의 `Trainer`를 상속받아서 로깅, 평가, 체크포인트 등 모든 기능을 지원하면서도 추가적인 편의 기능을 제공합니다.\n",
    "\n",
    "이번 예시에서는 PEFT 기능을 사용할 예정입니다. PEFT 방법으로는 QLoRA를 사용할 건데, 이는 양자화와 LoRA 튜닝을 같이 사용하여 대규모 언어 모델의 메모리 사용량을 줄이는 기술입니다.\n",
    "\n",
    "* 참고: 멀티모달 입력에 패딩이 필요하기 때문에 Flash Attention은 사용할 수 없습니다.*\n",
    "\n",
    "Qwen 2 VL 7B 모델을 사용할 예정이지만, `model_id` 변수만 바꾸면 Meta AI의 Llama-3.2-11B-Vision, Mistral AI의 Pixtral-12B 등 다른 모델로도 쉽게 교체할 수 있습니다. bitsandbytes를 사용해 모델을 4비트로 양자화할 예정입니다.\n",
    "\n",
    "* 참고: 모델이 클수록 더 많은 메모리가 필요합니다. 이번 예시에서는 7B 모델을 사용할 예정입니다.*\n",
    "\n",
    "VLM 학습을 위해 LLM, 토크나이저, 프로세서를 올바르게 준비하는 것이 매우 중요합니다. 프로세서는 특수 토큰과 이미지를 입력에 포함시키는 역할을 담당하는 모듈입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fa661e-22e5-4843-87de-023ee47f71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70c76de5038483f922f700d9a5d600c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc87b51c25448a6afe9be5e432ced3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e7ae26ec5e451e90d09dccd9a1fa36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/56.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caabc5833e04074a36390c7f399cd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28df40a72c3e4e52948530891854cc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c37fe77edd4ce18d1a1618099de58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf335b4df39344fcbec7b8f3ebd6ef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5dfbe6502d45eab3935284297f02ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cdd02a56014548bba76182bcadfc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b0f3b03709408e963a97a15ee4f1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b2ee447c1f4c199b7d9570dfa74820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c2734b5ece4514a88fcbd37036e930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41835ee82349485bae33ec988b8ff8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2347ff5ffa430393b783111f634fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6341c8b7604f0a8acd90ea3f83d156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18aae30c085422a8c868b9556e0cf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a8923c2792408fa2b7143736ce1f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\" \n",
    "\n",
    "# BitsAndBytes 4비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,                             # 4비트 양자화 사용\n",
    "   bnb_4bit_use_double_quant=True,               # 이중 양자화 사용으로 메모리 추가 절약\n",
    "   bnb_4bit_quant_type=\"nf4\",                    # 4비트 양자화 타입 설정(normalized float 4)\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16         # 연산 시 bfloat16 타입 사용\n",
    ")\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "   model_id,\n",
    "   device_map=\"auto\",                            # GPU 메모리에 자동 할당\n",
    "   # attn_implementation=\"flash_attention_2\",     # 학습시에는 flash attention 2 미지원\n",
    "   torch_dtype=torch.bfloat16,                   # bfloat16 정밀도 사용\n",
    "   quantization_config=bnb_config                # 위에서 정의한 양자화 설정 적용\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)  # 텍스트/이미지 전처리기 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29d040-3ca9-4638-a8ae-f7d34bb461c6",
   "metadata": {},
   "source": [
    "다음은 QWEN의 템플릿 예시입니다.  \n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|im_end|>\n",
    "<|im_start|>assistant\n",
    "거대 언어 모델의 답변<|im_end|>\n",
    "```\n",
    "\n",
    "멀티모달 QWEN은 이렇게 사용할 겁니다.\n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|vision_start|>이미지<|vision_end|><|im_end|>\n",
    "<|im_start|>assistant\n",
    "거대 언어 모델의 답변<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13e4a9eb-0886-41de-a168-6f045624825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an expert product description writer for Amazon.<|im_end|>\n",
      "<|im_start|>user\n",
      "Create a Short Product description based on the provided ##PRODUCT NAME## and ##CATEGORY## and image. \n",
      "Only return description. The description should be SEO optimized and for a better mobile search experience.\n",
      " \n",
      "##PRODUCT NAME##: Barbie Fashionistas Doll Wear Your Heart\n",
      "##CATEGORY##: Toys & Games | Dolls & Accessories | Dolls<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "Express your style with Barbie Fashionistas Doll Wear Your Heart! This fashionable doll boasts a unique outfit and accessories, perfect for imaginative play.  A great gift for kids aged 3+.  Collect them all! #Barbie #Fashionistas #Doll #Toys #GirlsToys #FashionDoll #Play<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    dataset[2][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2acd65-fc31-4427-9d64-228efabdb5a4",
   "metadata": {},
   "source": [
    "SFTTrainer는 peft와 기본적으로 통합되어 있어 LoraConfig를 만들어서 트레이너에 제공하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4890201-2cba-4de3-a5ed-bad569406b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44497f73-c125-49d3-9530-9b5918ea75cb",
   "metadata": {},
   "source": [
    "학습을 시작하기 전에 사용할 하이퍼파라미터(SFTConfig)를 정의하고 입력이 모델에 올바르게 제공되는지 확인해야 합니다.  \n",
    "\n",
    "텍스트만 사용하는 지도 파인튜닝과 달리 모델에 이미지도 함께 제공해야 하는데요. 이를 위해 입력을 올바르게 포맷팅하고 이미지 특징을 포함하는 커스텀 DataCollator를 만들어야 합니다.  \n",
    "\n",
    "Qwen2 팀이 제공하는 유틸리티 패키지의 process_vision_info 메서드를 사용할 예정입니다. Llama 3.2 Vision 같은 다른 모델을 사용하는 경우라면, 동일한 방식으로 이미지 정보가 처리되는지 확인해봐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6820033-24b8-4cf3-9514-2a7956fa26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# SFTConfig를 통해 학습 설정을 정의\n",
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-instruct-amazon-description\",  # 학습된 모델과 체크포인트를 저장할 디렉터리 경로 및 리포지토리 ID\n",
    "    num_train_epochs=3,                                # 전체 학습 에포크 수 (데이터셋을 몇 번 반복할지 설정)\n",
    "    per_device_train_batch_size=8,                     # 각 장비(GPU)당 사용될 배치 사이즈 (메모리와 연관됨)\n",
    "    gradient_accumulation_steps=8,                     # 경사 누적 스텝 수 (이 횟수만큼 기울기를 누적한 후 업데이트)\n",
    "    gradient_checkpointing=True,                       # 메모리 절약을 위한 gradient checkpointing 활성화 (메모리 최적화)\n",
    "    optim=\"adamw_torch_fused\",                         # AdamW 옵티마이저 (fused 버전 사용으로 학습 속도 향상)\n",
    "    logging_steps=5,                                   # 몇 스텝마다 로그를 출력할지 설정 (여기선 5 스텝마다 로그)\n",
    "    save_strategy=\"epoch\",                             # 매 에포크마다 체크포인트 저장 설정\n",
    "    learning_rate=2e-4,                                # 학습률 (QLoRA 논문에서 추천된 값 사용)\n",
    "    bf16=True,                                         # bfloat16 정밀도 사용 (메모리 절약 및 속도 향상)\n",
    "    tf32=True,                                         # tf32 정밀도 사용 (NVIDIA GPU에서 학습 속도 향상)\n",
    "    max_grad_norm=0.3,                                 # 기울기 클리핑을 위한 최대 기울기 값 (QLoRA 논문에서 추천된 값)\n",
    "    warmup_ratio=0.03,                                 # 학습 초기에 학습률을 점진적으로 올리는 warmup 비율 (QLoRA 논문에서 추천된 값)\n",
    "    lr_scheduler_type=\"constant\",                      # 일정한 학습률 스케줄러 사용 (학습률이 변하지 않음)\n",
    "    # push_to_hub=True,                                  # 학습된 모델을 Hugging Face Hub에 푸시할지 여부\n",
    "    report_to=\"tensorboard\",                           # TensorBoard를 통해 학습 상태를 모니터링\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # reentrant gradient checkpointing 설정 (비재진입 방식 사용)\n",
    "    dataset_text_field=\"\",                             # 데이터셋에서 텍스트 필드를 위한 더미 필드 (collator에서 필요)\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}      # collator에서 데이터셋 전처리를 건너뛰기 위한 설정\n",
    ")\n",
    "\n",
    "# 불필요한 열 삭제하지 않도록 설정 (학습 중 사용되지 않는 열이라도 유지)\n",
    "args.remove_unused_columns = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb8111c8-ac97-4cf0-94e6-340760b33792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트와 이미지 쌍을 인코딩하기 위한 데이터 collator 함수 정의\n",
    "def collate_fn(examples):\n",
    "    # 각 예제에서 텍스트와 이미지를 추출하고, 텍스트는 채팅 템플릿을 적용\n",
    "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "    image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "    # 텍스트를 토크나이징하고 이미지를 처리하여 일괄 처리(batch) 형태로 변환\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # labels로 사용할 input_ids 복사본 생성 후, 패딩 토큰을 -100으로 설정하여 손실 계산 시 무시하도록 함\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # 패딩 토큰 손실 계산 제외\n",
    "\n",
    "    # 특정 이미지 토큰 인덱스는 손실 계산에서 무시 (모델에 따라 다름)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  \n",
    "        # Qwen2VL 모델의 이미지 토큰 인덱스\n",
    "        image_tokens = [151652, 151653, 151655]\n",
    "    else:\n",
    "        # 다른 모델에서 이미지 토큰 ID를 얻어 손실 계산에서 제외\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "    \n",
    "    # 손실 계산 시 이미지 토큰 인덱스를 무시하도록 설정\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100\n",
    "    \n",
    "    # 배치에 labels 추가 (손실 계산 시 사용)\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a349f087-67c9-4d56-bf56-acbc4768e750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 예시 데이터:\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are an expert product description writer for Amazon.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': \"Create a Short Product description based on the provided ##PRODUCT NAME## and ##CATEGORY## and image. \\nOnly return description. The description should be SEO optimized and for a better mobile search experience.\\n \\n##PRODUCT NAME##: Kurio Glow Smartwatch for Kids with Bluetooth, Apps, Camera & Games, Blue\\n##CATEGORY##: Toys & Games | Kids' Electronics | Electronic Learning Toys\"}, {'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x71699417A560>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': \"Kurio Glow Smartwatch: Fun, Safe & Educational!  This kids' smartwatch boasts Bluetooth connectivity, built-in apps & games, and a camera – all in a vibrant blue design. Perfect for learning & play!  #kidssmartwatch #kidselectronics #educationaltoys #kurioglow\"}]}]}\n"
     ]
    }
   ],
   "source": [
    "# 단일 예시 확인\n",
    "example = dataset[0]  # 데이터셋의 첫 번째 아이템\n",
    "print(\"단일 예시 데이터:\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb461049-7b7d-40a4-967e-76d57e95e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 495])\n",
      "어텐션 마스크 형태: torch.Size([1, 495])\n",
      "이미지 픽셀 형태: torch.Size([1296, 1176])\n",
      "레이블 형태: torch.Size([1, 495])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"이미지 픽셀 형태:\", batch[\"pixel_values\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c15c695-c95e-4345-a898-25248bf37eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,   2610,    525,    458,   6203,   1985,   4008,\n",
      "          6916,    369,   8176,     13, 151645,    198, 151644,    872,    198,\n",
      "          4021,    264,  10698,   5643,   4008,   3118,    389,    279,   3897,\n",
      "          7704,  51431,  19122,    565,    323,   7704,  93027,    565,    323,\n",
      "          2168,     13,    715,   7308,    470,   4008,     13,    576,   4008,\n",
      "          1265,    387,  24980,  33340,    323,    369,    264,   2664,   6371,\n",
      "          2711,   3139,    624,    715,    565,  51431,  19122,    565,     25,\n",
      "         31275,    815,  87348,  15770,  14321,    369,  22522,    448,  23843,\n",
      "            11,  33020,     11,  14332,    609,  11610,     11,   8697,    198,\n",
      "           565,  93027,    565,     25,  48381,    609,  11610,    760,  22522,\n",
      "             6,  37684,    760,  34169,  20909,  48381, 151652, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645,\n",
      "           198, 151644,  77091,    198,     42,    324,    815,  87348,  15770,\n",
      "         14321,     25,  16071,     11,  22291,    609,  45845,      0,    220,\n",
      "          1096,   6837,      6,   7785,  14321,  37019,  23843,  30257,     11,\n",
      "          5798,   3419,  10500,    609,   3868,     11,    323,    264,   6249,\n",
      "          1365,    678,    304,    264,  32976,   6303,   2884,     13,  23239,\n",
      "           369,   6832,    609,   1486,      0,    220,    671,  63281,    778,\n",
      "         33472,  14321,    671,  63281,   1742,  27329,    671,  24770,   1663,\n",
      "           983,   1047,    671,     74,   6070,    538,  10303, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e79ead1-5782-41c9-a818-2b86a68b0eed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,   2610,    525,    458,   6203,   1985,   4008,\n",
      "          6916,    369,   8176,     13, 151645,    198, 151644,    872,    198,\n",
      "          4021,    264,  10698,   5643,   4008,   3118,    389,    279,   3897,\n",
      "          7704,  51431,  19122,    565,    323,   7704,  93027,    565,    323,\n",
      "          2168,     13,    715,   7308,    470,   4008,     13,    576,   4008,\n",
      "          1265,    387,  24980,  33340,    323,    369,    264,   2664,   6371,\n",
      "          2711,   3139,    624,    715,    565,  51431,  19122,    565,     25,\n",
      "         31275,    815,  87348,  15770,  14321,    369,  22522,    448,  23843,\n",
      "            11,  33020,     11,  14332,    609,  11610,     11,   8697,    198,\n",
      "           565,  93027,    565,     25,  48381,    609,  11610,    760,  22522,\n",
      "             6,  37684,    760,  34169,  20909,  48381,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100, 151645,\n",
      "           198, 151644,  77091,    198,     42,    324,    815,  87348,  15770,\n",
      "         14321,     25,  16071,     11,  22291,    609,  45845,      0,    220,\n",
      "          1096,   6837,      6,   7785,  14321,  37019,  23843,  30257,     11,\n",
      "          5798,   3419,  10500,    609,   3868,     11,    323,    264,   6249,\n",
      "          1365,    678,    304,    264,  32976,   6303,   2884,     13,  23239,\n",
      "           369,   6832,    609,   1486,      0,    220,    671,  63281,    778,\n",
      "         33472,  14321,    671,  63281,   1742,  27329,    671,  24770,   1663,\n",
      "           983,   1047,    671,     74,   6070,    538,  10303, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54f93c39-a403-4ad5-808c-5d2d9382ee76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디코딩된 텍스트:\n",
      "<|im_start|>system\n",
      "You are an expert product description writer for Amazon.<|im_end|>\n",
      "<|im_start|>user\n",
      "Create a Short Product description based on the provided ##PRODUCT NAME## and ##CATEGORY## and image. \n",
      "Only return description. The description should be SEO optimized and for a better mobile search experience.\n",
      " \n",
      "##PRODUCT NAME##: Kurio Glow Smartwatch for Kids with Bluetooth, Apps, Camera & Games, Blue\n",
      "##CATEGORY##: Toys & Games | Kids' Electronics | Electronic Learning Toys<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "Kurio Glow Smartwatch: Fun, Safe & Educational!  This kids' smartwatch boasts Bluetooth connectivity, built-in apps & games, and a camera – all in a vibrant blue design. Perfect for learning & play!  #kidssmartwatch #kidselectronics #educationaltoys #kurioglow<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 디코딩 예시 (입력 텍스트가 어떻게 변환되었는지 확인)\n",
    "decoded_text = processor.tokenizer.decode(batch[\"input_ids\"][0])\n",
    "print(\"\\n디코딩된 텍스트:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4567378a-89d7-44d4-ad0c-e093427a3cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    "    dataset_text_field=\"\",\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d22539-530b-4474-9d78-883107717dec",
   "metadata": {},
   "source": [
    "Trainer 인스턴스의 train() 메서드를 호출하여 모델 학습을 시작합니다.  \n",
    "이렇게 하면 학습 루프가 시작되고 3 에폭 동안 모델이 학습됩니다. PEFT 방법을 사용하고 있기 때문에 전체 모델이 아닌 조정된 모델 가중치만 저장할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28f037e1-c34c-402c-8d94-d49269c50292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 03:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.296900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작, 모델은 자동으로 허브와 출력 디렉토리에 저장됨\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7db6ce71-f50c-4595-b534-baf14aa9021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 비우기\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493b6f7-8711-498e-b9b4-c5c54d290241",
   "metadata": {},
   "source": [
    "## 인퍼런스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955e49d-3dfc-41f0-9646-71f1eb1bb632",
   "metadata": {},
   "source": [
    "학습이 완료된 후에는 모델을 평가하고 테스트해볼 예정입니다.  \n",
    "\n",
    "먼저 기본 모델을 불러와서 임의의 아마존 제품에 대한 설명을 생성해보고, 그 다음 Q-LoRA로 조정된 모델을 불러와 같은 제품에 대한 설명을 생성해볼 것입니다.  \n",
    "마지막으로 더 효율적인 추론을 위해 어댑터를 기본 모델과 병합한 뒤, 동일한 제품에 대해 다시 한 번 추론을 실행해볼 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2788d0eb-7739-44ea-89a8-0ca97728fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13e202a612444a98399e9f93ca62b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    " \n",
    "# 기본 모델 호출\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52ff736a-41c8-40d6-bf62-4e5d1b7be920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing the Hasbro Marvel Avengers Series Marvel Assemble Titan Hero, Iron Man, 30.5 cm Action Figure! This highly detailed and articulated Iron Man figure is perfect for any Marvel fan. With over 15 points of articulation, this 30.5 cm tall action figure can be posed in a variety of dynamic positions, making it ideal for reenacting your favorite scenes from the Marvel Cinematic Universe. The vibrant red and gold color scheme is true to the iconic Iron Man suit, and the figure comes with a Titan Hero Power FX port, allowing you to connect to other Titan Hero Power FX figures for even more action-packed play. Whether you're a collector or just looking for a fun toy to add to your collection, this Iron Man action figure is a must-have for any Marvel fan.\n"
     ]
    }
   ],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# amazon.com에서 가져온 샘플 데이터\n",
    "sample = {\n",
    " \"product_name\": \"Hasbro Marvel Avengers-Serie Marvel Assemble Titan-Held, Iron Man, 30,5 cm Actionfigur\",\n",
    " \"catergory\": \"Toys & Games | Toy Figures & Playsets | Action Figures\",\n",
    " \"image\": \"https://m.media-amazon.com/images/I/81+7Up7IWyL._AC_SY300_SX300_.jpg\"\n",
    "}\n",
    "\n",
    "# 메시지 준비\n",
    "messages = [{\n",
    "       \"role\": \"user\",\n",
    "       \"content\": [\n",
    "           {\n",
    "               \"type\": \"image\",\n",
    "               \"image\": sample[\"image\"],\n",
    "           },\n",
    "           {\"type\": \"text\", \"text\": prompt.format(product_name=sample[\"product_name\"], category=sample[\"catergory\"])},\n",
    "       ],\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee8d5bc4-1f96-495f-8dba-6e1a5dc3147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 답변을 생성하는 함수\n",
    "def generate_description(sample, model, processor):\n",
    "   messages = [\n",
    "       {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "       {\"role\": \"user\", \"content\": [\n",
    "           {\"type\": \"image\",\"image\": sample[\"image\"]},\n",
    "           {\"type\": \"text\", \"text\": prompt.format(product_name=sample[\"product_name\"], category=sample[\"catergory\"])},\n",
    "       ]},\n",
    "   ]\n",
    "   # 추론을 위한 준비\n",
    "   text = processor.apply_chat_template(\n",
    "       messages, tokenize=False, add_generation_prompt=True\n",
    "   )\n",
    "   image_inputs, video_inputs = process_vision_info(messages)\n",
    "   inputs = processor(\n",
    "       text=[text],\n",
    "       images=image_inputs,\n",
    "       videos=video_inputs,\n",
    "       padding=True,\n",
    "       return_tensors=\"pt\",\n",
    "   )\n",
    "   inputs = inputs.to(model.device)\n",
    "   # 추론: 출력 생성\n",
    "   generated_ids = model.generate(**inputs, max_new_tokens=256, top_p=1.0, do_sample=True, temperature=0.8)\n",
    "   generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "   output_text = processor.batch_decode(\n",
    "       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "   )\n",
    "   return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af2c867f-da30-41aa-b1b9-b9da5f7130d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing the Hasbro Marvel Avengers Series Marvel Assemble Titan Hero, Iron Man, 30.5 cm Action Figure! This highly detailed and articulated Iron Man figure is perfect for any Marvel fan. With over 15 points of articulation, this 30.5 cm tall action figure can be posed in a variety of dynamic positions, making it ideal for reenacting your favorite scenes from the Marvel Cinematic Universe. The vibrant red and gold color scheme is true to the iconic Iron Man suit, and the figure comes with a Titan Hero Power FX port, allowing you to connect to other Titan Hero Power FX figures for even more action-packed play. Whether you're a collector or just looking for a fun toy to add to your collection, this Iron Man action figure is a must-have for any Marvel fan.\n"
     ]
    }
   ],
   "source": [
    "# 설명 생성해보기\n",
    "base_description = generate_description(sample, model, processor)\n",
    "print(base_description)\n",
    "# 원하는 경우 아래 명령어로 활성화된 어댑터를 비활성화할 수 있음\n",
    "# model.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94ced566-c1b3-438b-8b73-ffad356a5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로라 어댑터가 있는 경로\n",
    "adapter_path = \"./qwen2-7b-instruct-amazon-description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a85779d1-ad4f-4ab0-a27a-959113b6f53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discover the ultimate Marvel Avengers experience with this 30.5 cm Iron Man action figure from Hasbro! Perfect for fans of all ages, this Titan Hero figure is ready for action with a detailed design and moveable joints. Bring your favorite Marvel character to life with this amazing toy.\n"
     ]
    }
   ],
   "source": [
    "model.load_adapter(adapter_path) \n",
    "\n",
    "ft_description = generate_description(sample, model, processor)\n",
    "print(ft_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb02fee1-4ea9-4085-94ab-3b08981c1fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 학습 전 모델: Introducing the Hasbro Marvel Avengers Series Marvel Assemble Titan Hero, Iron Man, 30.5 cm Action Figure! This highly detailed and articulated Iron Man figure is perfect for any Marvel fan. With over 15 points of articulation, this 30.5 cm tall action figure can be posed in a variety of dynamic positions, making it ideal for reenacting your favorite scenes from the Marvel Cinematic Universe. The vibrant red and gold color scheme is true to the iconic Iron Man suit, and the figure comes with a Titan Hero Power FX port, allowing you to connect to other Titan Hero Power FX figures for even more action-packed play. Whether you're a collector or just looking for a fun toy to add to your collection, this Iron Man action figure is a must-have for any Marvel fan.\n",
      "---\n",
      "LoRA 학습 후 모델: Discover the ultimate Marvel Avengers experience with this 30.5 cm Iron Man action figure from Hasbro! Perfect for fans of all ages, this Titan Hero figure is ready for action with a detailed design and moveable joints. Bring your favorite Marvel character to life with this amazing toy.\n"
     ]
    }
   ],
   "source": [
    "print('LoRA 학습 전 모델:', base_description)\n",
    "print('---')\n",
    "print('LoRA 학습 후 모델:', ft_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61ce42-4217-49a0-b309-7107c7de31ab",
   "metadata": {},
   "source": [
    "## 로라 병합 후 저장하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70229777-f38d-44ab-864f-1405982159f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom peft import PeftModel\\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\\n\\nadapter_path = \"./qwen2-7b-instruct-amazon-description\"  # 학습된 어댑터 경로\\nbase_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"  # 기본 모델 ID\\nmerged_path = \"merged\"  # 병합된 모델을 저장할 경로\\n\\n# 기본 모델 로드\\nmodel = AutoModelForVision2Seq.from_pretrained(model_id, low_cpu_mem_usage=True)\\n\\n# 병합된 모델 저장 경로\\n# LoRA와 기본 모델을 병합하고 저장\\npeft_model = PeftModel.from_pretrained(model, adapter_path)  # PEFT 모델 로드\\nmerged_model = peft_model.merge_and_unload()  # 모델 병합\\nmerged_model.save_pretrained(merged_path,safe_serialization=True, max_shard_size=\"2GB\")  # 병합된 모델 저장\\n\\nprocessor = AutoProcessor.from_pretrained(base_model_id)  # 프로세서 로드\\nprocessor.save_pretrained(merged_path)  # 프로세서 저장\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "adapter_path = \"./qwen2-7b-instruct-amazon-description\"  # 학습된 어댑터 경로\n",
    "base_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"  # 기본 모델 ID\n",
    "merged_path = \"merged\"  # 병합된 모델을 저장할 경로\n",
    "\n",
    "# 기본 모델 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# 병합된 모델 저장 경로\n",
    "# LoRA와 기본 모델을 병합하고 저장\n",
    "peft_model = PeftModel.from_pretrained(model, adapter_path)  # PEFT 모델 로드\n",
    "merged_model = peft_model.merge_and_unload()  # 모델 병합\n",
    "merged_model.save_pretrained(merged_path,safe_serialization=True, max_shard_size=\"2GB\")  # 병합된 모델 저장\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)  # 프로세서 로드\n",
    "processor.save_pretrained(merged_path)  # 프로세서 저장\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
